<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Hamel&#39;s Blog - Hamel Husain</title>
<link>https://hamel.dev/blog/secret.html</link>
<atom:link href="https://hamel.dev/blog/secret.xml" rel="self" type="application/rss+xml"/>
<description>Notes on applied AI engineering, machine learning, and data science.</description>
<image>
<url>https://hamel.dev/quarto.png</url>
<title>Hamel&#39;s Blog - Hamel Husain</title>
<link>https://hamel.dev/blog/secret.html</link>
<height>81</height>
<width>144</width>
</image>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Tue, 18 Nov 2025 23:15:30 GMT</lastBuildDate>
<item>
  <title>LLM Evals: Everything You Need to Know</title>
  <dc:creator>Hamel Husain</dc:creator>
  <dc:creator>Shreya Shankar</dc:creator>
  <link>https://hamel.dev/blog/posts/evals-faq/</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>This document curates the most common questions Shreya and I received while <a href="https://bit.ly/evals-ai" target="_blank">teaching</a> 700+ engineers &amp; PMs AI Evals. <em>Warning: These are sharp opinions about what works in most cases. They are not universal truths. Use your judgment.</em></p>
<hr>
<div class="cta" style="text-align: center;">
<p><strong>üëâ <em>Want to learn more about AI Evals? Check out our <a href="https://bit.ly/evals-ai" target="_blank">AI Evals course</a></em></strong>. It‚Äôs a live cohort with hands on exercises and office hours. Here is a <a href="https://bit.ly/evals-ai" target="_blank">15% discount code</a> for readers. üëà</p>
</div>
<hr>
<section id="listen-to-the-audio-version-of-this-faq" class="level1">
<h1>Listen to the audio version of this FAQ</h1>
<p>If you prefer to listen to the audio version (narrated by AI), you can play it <a href="https://soundcloud.com/hamel-husain/llm-evals-faq">here</a>.</p>
<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/2138083206&amp;color=%23447099&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true">
</iframe>
<div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;">
<a href="https://soundcloud.com/hamel-husain" title="Hamel Husain" target="_blank" style="color: #cccccc; text-decoration: none;">Hamel Husain</a> ¬∑ <a href="https://soundcloud.com/hamel-husain/llm-evals-faq" title="LLM Evals FAQ" target="_blank" style="color: #cccccc; text-decoration: none;">LLM Evals FAQ</a>
</div>
</section>
<section id="getting-started-fundamentals" class="level1">
<h1>Getting Started &amp; Fundamentals</h1>
<section id="q-what-are-llm-evals" class="level2">
<h2 class="anchored" data-anchor-id="q-what-are-llm-evals">Q: What are LLM Evals?</h2>
<p>If you are completely new to product-specific LLM evals (not foundation model benchmarks), see these posts: <a href="../../../blog/posts/evals/index.html" target="_blank">part 1</a>, <a href="../../../blog/posts/llm-judge/index.html" target="_blank">part 2</a> and <a href="../../../blog/posts/field-guide/index.html" target="_blank">part 3</a>. Otherwise, keep reading.</p>
<div class="grid">
<div class="g-col-4">
<p><a href="https://hamel.dev/evals" target="_blank"><img src="https://hamel.dev/blog/posts/evals/images/diagram-cover.png" class="img-fluid"></a></p>
<p><a href="https://hamel.dev/evals" target="_blank"><strong>Your AI Product Needs Eval (Evaluation Systems)</strong></a></p>
<p><strong>Contents:</strong></p>
<ol type="1">
<li>Motivation</li>
<li>Iterating Quickly == Success<br>
</li>
<li>Case Study: Lucy, A Real Estate AI Assistant</li>
<li>The Types Of Evaluation
<ol type="a">
<li>Level 1: Unit Tests</li>
<li>Level 2: Human &amp; Model Eval</li>
<li>Level 3: A/B Testing</li>
<li>Evaluating RAG</li>
</ol></li>
<li>Eval Systems Unlock Superpowers For Free
<ol type="a">
<li>Fine-Tuning</li>
<li>Data Synthesis &amp; Curation</li>
<li>Debugging</li>
</ol></li>
</ol>
</div>
<div class="g-col-4">
<p><a href="https://hamel.dev/llm-judge/" target="_blank"><img src="https://hamel.dev/blog/posts/llm-judge/images/cover_img.png" class="img-fluid"></a></p>
<p><a href="https://hamel.dev/llm-judge/" target="_blank"><strong>Creating a LLM-as-a-Judge That Drives Business Results</strong></a></p>
<p><strong>Contents:</strong></p>
<ol type="1">
<li>The Problem: AI Teams Are Drowning in Data</li>
<li>Step 1: Find The Principal Domain Expert</li>
<li>Step 2: Create a Dataset</li>
<li>Step 3: Direct The Domain Expert to Make Pass/Fail Judgments with Critiques</li>
<li>Step 4: Fix Errors</li>
<li>Step 5: Build Your LLM as A Judge, Iteratively</li>
<li>Step 6: Perform Error Analysis</li>
<li>Step 7: Create More Specialized LLM Judges (if needed)</li>
<li>Recap of Critique Shadowing</li>
<li>Resources</li>
</ol>
</div>
<div class="g-col-4">
<p><a href="https://hamel.dev/field-guide" target="_blank"><img src="https://hamel.dev/blog/posts/field-guide/images/field_guide_2.png" class="img-fluid"></a></p>
<p><a href="https://hamel.dev/field-guide" target="_blank"><strong>A Field Guide to Rapidly Improving AI Products</strong></a></p>
<p><strong>Contents:</strong></p>
<ol type="1">
<li>How error analysis consistently reveals the highest-ROI improvements</li>
<li>Why a simple data viewer is your most important AI investment</li>
<li>How to empower domain experts (not just engineers) to improve your AI</li>
<li>Why synthetic data is more effective than you think</li>
<li>How to maintain trust in your evaluation system</li>
<li>Why your AI roadmap should count experiments, not features</li>
</ol>
</div>
</div>
<p><a href="../../../blog/posts/evals-faq/what-are-llm-evals.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-what-is-a-trace" class="level2">
<h2 class="anchored" data-anchor-id="q-what-is-a-trace">Q: What is a trace?</h2>
<p>A trace is the complete record of all actions, messages, tool calls, and data retrievals from a single initial user query through to the final response. It includes every step across all agents, tools, and system components in a session: multiple user messages, assistant responses, retrieved documents, and intermediate tool interactions.</p>
<p><strong>Note on terminology:</strong> Different observability vendors use varying definitions of traces and spans. <a href="https://mlops.systems/posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#llm-tracing-tools-naming-conventions-june-2025">Alex Strick van Linschoten‚Äôs analysis</a> highlights these differences (screenshot below):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/alex.jpeg" class="img-fluid figure-img"></p>
<figcaption>Vendor differences in trace definitions as of 2025-07-02</figcaption>
</figure>
</div>
<p><a href="../../../blog/posts/evals-faq/what-is-a-trace.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-whats-a-minimum-viable-evaluation-setup" class="level2">
<h2 class="anchored" data-anchor-id="q-whats-a-minimum-viable-evaluation-setup">Q: What‚Äôs a minimum viable evaluation setup?</h2>
<p>Start with error analysis, not infrastructure. Spend 30 minutes manually reviewing 20-50 LLM outputs whenever you make significant changes. Use one domain expert who understands your users as your quality decision maker (a ‚Äúbenevolent dictator‚Äù).</p>
<p>If possible, <strong>use notebooks</strong> to help you review traces and analyze data. In our opinion, this is the single most effective tool for evals because you can write arbitrary code, visualize data, and iterate quickly. You can even build your own custom annotation interface right inside notebooks, as shown in this <a href="https://youtu.be/aqKUwPKBkB0?si=5KDmMQnRzO_Ce9xH" target="_blank">video</a>.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/aqKUwPKBkB0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><a href="../../../blog/posts/evals-faq/whats-a-minimum-viable-evaluation-setup.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-much-of-my-development-budget-should-i-allocate-to-evals" class="level2">
<h2 class="anchored" data-anchor-id="q-how-much-of-my-development-budget-should-i-allocate-to-evals">Q: How much of my development budget should I allocate to evals?</h2>
<p>It‚Äôs important to recognize that evaluation is part of the development process rather than a distinct line item, similar to how debugging is part of software development.</p>
<p>You should always be doing <a href="https://www.youtube.com/watch?v=qH1dZ8JLLdU" target="_blank">error analysis</a>. When you discover issues through error analysis, many will be straightforward bugs you‚Äôll fix immediately. These fixes don‚Äôt require separate evaluation infrastructure as they‚Äôre just part of development.</p>
<p>The decision to build automated evaluators comes down to cost-benefit analysis. If you can catch an error with a simple assertion or regex check, the cost is minimal and probably worth it. But if you need to align an LLM-as-judge evaluator, consider whether the failure mode warrants that investment.</p>
<p>In the projects we‚Äôve worked on, <strong>we‚Äôve spent 60-80% of our development time on error analysis and evaluation</strong>. Expect most of your effort to go toward understanding failures (i.e.&nbsp;looking at data) rather than building automated checks.</p>
<p>Be <a href="https://ai-execs.com/2_intro.html#a-case-study-in-misleading-ai-advice" target="_blank">wary of optimizing for high eval pass rates</a>. If you‚Äôre passing 100% of your evals, you‚Äôre likely not challenging your system enough. A 70% pass rate might indicate a more meaningful evaluation that‚Äôs actually stress-testing your application. Focus on evals that help you catch real issues, not ones that make your metrics look good.</p>
<p><a href="../../../blog/posts/evals-faq/how-much-of-my-development-budget-should-i-allocate-to-evals.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-will-todays-evaluation-methods-still-be-relevant-in-5-10-years-given-how-fast-ai-is-changing" class="level2">
<h2 class="anchored" data-anchor-id="q-will-todays-evaluation-methods-still-be-relevant-in-5-10-years-given-how-fast-ai-is-changing">Q: Will today‚Äôs evaluation methods still be relevant in 5-10 years given how fast AI is changing?</h2>
<p>Yes. Even with perfect models, you still need to verify they‚Äôre solving the right problem. The need for systematic error analysis, domain-specific testing, and monitoring will still be important.</p>
<p>Today‚Äôs prompt engineering tricks might become obsolete, but you‚Äôll still need to understand failure modes. Additionally, a LLM cannot read your mind, and <a href="https://arxiv.org/abs/2404.12272" target="_blank">research shows</a> that people need to observe the LLM‚Äôs behavior in order to properly externalize their requirements.</p>
<p>For deeper perspective on this debate, see these two viewpoints: <a href="https://m.youtube.com/watch?si=qknrtQeITqJ7VsJH&amp;v=4dUFIRj-BWo&amp;feature=youtu.be" target="_blank">‚ÄúThe model is the product‚Äù</a> versus <a href="https://www.youtube.com/watch?v=EEw2PpL-_NM" target="_blank">‚ÄúThe model is NOT the product‚Äù</a>.</p>
<p><strong>‚ÄúThe model is the product‚Äù:</strong> </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/4dUFIRj-BWo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p><strong>‚ÄúThe model is NOT the product‚Äù:</strong> </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EEw2PpL-_NM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p><a href="../../../blog/posts/evals-faq/will-these-evaluation-methods-still-be-relevant-in-5-10-years-given-how-fast-ai-is-changing.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-do-i-make-the-case-for-investing-in-evaluations-to-my-team" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-make-the-case-for-investing-in-evaluations-to-my-team">Q: How do I make the case for investing in evaluations to my team?</h2>
<p>Don‚Äôt try to sell your team on ‚Äúevals‚Äù. Instead, show them what you find when you look at the data.</p>
<p>Start by doing the error analysis yourself. Look at 50 to 100 real user conversations and find the most common ways the product is failing. Use these findings to tell a story with data.</p>
<p>Present your team with:</p>
<ul>
<li>A list of the top failure modes you discovered.</li>
<li>Metrics showing how often high-impact errors are happening.</li>
<li>Surprising ways that users are interacting with the product.</li>
<li>Reports on the bugs you found and fixed, framed as ‚Äúprevented production issues‚Äù.</li>
</ul>
<p>This approach builds trust. Don‚Äôt just show dashboards and metrics; tell the story of what you‚Äôre finding in the data. By narrating your findings, you teach the team what you‚Äôre learning, providing immediate value. When you fix an issue, show how the error rate for that specific problem went down. Soon, your team will see the progress and ask how you‚Äôre doing it. Let results instead of methods lead the conversation.</p>
<p>This is similar to classic machine learning projects, where outcomes are speculative and progress is bounded by <a href="https://hamel.dev/blog/posts/field-guide/#your-ai-roadmap-should-count-experiments-not-features" target="_blank">iterating on experiments</a>. In this situation, it‚Äôs important that you share the learnings from each experiment to show progress and encourage investment.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-make-the-case-for-investing-in-evaluations-to-my-team.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="error-analysis-data-collection" class="level1">
<h1>Error Analysis &amp; Data Collection</h1>
<section id="q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed" class="level2">
<h2 class="anchored" data-anchor-id="q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed">Q: Why is "error analysis" so important in LLM evals, and how is it performed?</h2>
<p>Error analysis is <strong>the most important activity in evals</strong>. Error analysis helps you decide what evals to write in the first place. It allows you to identify failure modes unique to your application and data. The process involves:</p>
<section id="creating-a-dataset" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-dataset">1. Creating a Dataset</h3>
<p>Gathering representative traces of user interactions with the LLM. If you do not have any data, you can generate synthetic data to get started.</p>
</section>
<section id="open-coding" class="level3">
<h3 class="anchored" data-anchor-id="open-coding">2. Open Coding</h3>
<p>Human annotator(s) (ideally a benevolent dictator) review and write open-ended notes about traces, noting any issues. This process is akin to ‚Äújournaling‚Äù and is adapted from qualitative research methodologies. When beginning, it is recommended to focus on noting the first failure observed in a trace, as upstream errors can cause downstream issues, though you can also tag all independent failures if feasible. A <a href="https://hamel.dev/blog/posts/llm-judge/#step-1-find-the-principal-domain-expert" target="_blank">domain expert</a> should be performing this step.</p>
</section>
<section id="axial-coding" class="level3">
<h3 class="anchored" data-anchor-id="axial-coding">3. Axial Coding</h3>
<p>Categorize the open-ended notes into a ‚Äúfailure taxonomy.‚Äù. In other words, group similar failures into distinct categories. This is the most important step. At the end, count the number of failures in each category. You can use a LLM to help with this step.</p>
</section>
<section id="iterative-refinement" class="level3">
<h3 class="anchored" data-anchor-id="iterative-refinement">4. Iterative Refinement</h3>
<p>Keep iterating on more traces until you reach <a href="https://delvetool.com/blog/theoreticalsaturation" target="_blank">theoretical saturation</a>, meaning new traces do not seem to reveal new failure modes or information to you. As a rule of thumb, you should aim to review at least 100 traces.</p>
<p>You should frequently revisit this process. There are advanced ways to <a href="how-can-i-efficiently-sample-production-traces-for-review.html" target="_blank">sample data more efficiently</a>, like clustering, sorting by user feedback, and sorting by high probability failure patterns. Over time, you‚Äôll develop a ‚Äúnose‚Äù for where to look for failures in your data.</p>
<p>Do not skip error analysis. It ensures that the evaluation metrics you develop are supported by real application behaviors instead of counter-productive generic metrics (which most platforms nudge you to use). For examples of how error analysis can be helpful, see <a href="https://www.youtube.com/watch?v=e2i6JbU2R-s" target="_blank">this video</a>, or this <a href="https://hamel.dev/blog/posts/field-guide/" target="_blank">blog post</a>.</p>
<p>Here is a visualization of the error analysis process by one of our students, <a href="https://www.linkedin.com/in/pawel-huryn/" target="_blank">Pawel Huryn</a> - including how it fits into the overall evaluation process:</p>
<p><img src="https://hamel.dev/blog/posts/evals-faq/pawel-error-analysis.png" class="img-fluid"></p>
<p><a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-how-do-i-surface-problematic-traces-for-review-beyond-user-feedback" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-surface-problematic-traces-for-review-beyond-user-feedback">Q: How do I surface problematic traces for review beyond user feedback?</h2>
<p>While user feedback is a good way to narrow in on problematic traces, other methods are also useful. Here are three complementary approaches:</p>
<section id="start-with-random-sampling" class="level3">
<h3 class="anchored" data-anchor-id="start-with-random-sampling">Start with random sampling</h3>
<p>The simplest approach is reviewing a random sample of traces. If you find few issues, escalate to stress testing: create queries that deliberately test your prompt constraints to see if the AI follows your rules.</p>
</section>
<section id="use-evals-for-initial-screening" class="level3">
<h3 class="anchored" data-anchor-id="use-evals-for-initial-screening">Use evals for initial screening</h3>
<p>Use existing evals to find problematic traces and potential issues. Once you‚Äôve identified these, you can proceed with the typical evaluation process starting with error analysis.</p>
</section>
<section id="leverage-efficient-sampling-strategies" class="level3">
<h3 class="anchored" data-anchor-id="leverage-efficient-sampling-strategies">Leverage efficient sampling strategies</h3>
<p>For more sophisticated trace discovery, use outlier detection, metric-based sorting, and stratified sampling to find interesting traces. Generic metrics can serve as exploration signals to identify traces worth reviewing, even if they don‚Äôt directly measure quality.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-surface-problematic-traces-for-review-beyond-user-feedback.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-how-often-should-i-re-run-error-analysis-on-my-production-system" class="level2">
<h2 class="anchored" data-anchor-id="q-how-often-should-i-re-run-error-analysis-on-my-production-system">Q: How often should I re-run error analysis on my production system?</h2>
<p>Re-run error analysis when making significant changes: new features, prompt updates, model switches, or major bug fixes. A useful heuristic is to set a goal for reviewing <em>at least</em> 100+ fresh traces each review cycle. Typical review cycles we‚Äôve seen range from 2-4 weeks. See this FAQ on how to sample traces effectively.</p>
<p>Between major analyses, review 10-20 traces weekly, focusing on outliers: unusually long conversations, sessions with multiple retries, or traces flagged by automated monitoring. Adjust frequency based on system stability and usage growth. New systems need weekly analysis until failure patterns stabilize. Mature systems might need only monthly analysis unless usage patterns change. Always analyze after incidents, user complaint spikes, or metric drift. Scaling usage introduces new edge cases.</p>
<p><a href="../../../blog/posts/evals-faq/how-often-should-i-re-run-error-analysis-on-my-production-system.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-what-is-the-best-approach-for-generating-synthetic-data" class="level2">
<h2 class="anchored" data-anchor-id="q-what-is-the-best-approach-for-generating-synthetic-data">Q: What is the best approach for generating synthetic data?</h2>
<p>A common mistake is prompting an LLM to <code>"give me test queries"</code> without structure, resulting in generic, repetitive outputs. A structured approach using dimensions produces far better synthetic data for testing LLM applications.</p>
<p><strong>Start by defining dimensions</strong>: categories that describe different aspects of user queries. Each dimension captures one type of variation in user behavior. For example:</p>
<ul>
<li>For a recipe app, dimensions might include Dietary Restriction (<em>vegan</em>, <em>gluten-free</em>, <em>none</em>), Cuisine Type (<em>Italian</em>, <em>Asian</em>, <em>comfort food</em>), and Query Complexity (<em>simple request</em>, <em>multi-step</em>, <em>edge case</em>).</li>
<li>For a customer support bot, dimensions could be Issue Type (<em>billing</em>, <em>technical</em>, <em>general</em>), Customer Mood (<em>frustrated</em>, <em>neutral</em>, <em>happy</em>), and Prior Context (<em>new issue</em>, <em>follow-up</em>, <em>resolved</em>).</li>
</ul>
<p><strong>Start with failure hypotheses</strong>. If you lack intuition about failure modes, use your application extensively or recruit friends to use it. Then choose dimensions targeting those likely failures.</p>
<p><strong>Create tuples manually first</strong>: Write 20 tuples by hand‚Äîspecific combinations selecting one value from each dimension. Example: (<em>Vegan</em>, <em>Italian</em>, <em>Multi-step</em>). This manual work helps you understand your problem space.</p>
<p><strong>Scale with two-step generation</strong>:</p>
<ol type="1">
<li><strong>Generate structured tuples</strong>: Have the LLM create more combinations like (<em>Gluten-free</em>, <em>Asian</em>, <em>Simple</em>)</li>
<li><strong>Convert tuples to queries</strong>: In a separate prompt, transform each tuple into natural language</li>
</ol>
<p>This separation avoids repetitive phrasing. The (<em>Vegan</em>, <em>Italian</em>, <em>Multi-step</em>) tuple becomes: <code>"I need a dairy-free lasagna recipe that I can prep the day before."</code></p>
<section id="generation-approaches" class="level3">
<h3 class="anchored" data-anchor-id="generation-approaches">Generation approaches</h3>
<p>You can generate tuples two ways:</p>
<p><strong>Cross product then filter</strong>: Generate all dimension combinations, then filter with an LLM. Guarantees coverage including edge cases. Use when most combinations are valid.</p>
<p><strong>Direct LLM generation</strong>: Ask the LLM to generate tuples directly. More realistic but tends toward generic outputs and misses rare scenarios. Use when many dimension combinations are invalid.</p>
<p><strong>Fix obvious problems first</strong>: Don‚Äôt generate synthetic data for issues you can fix immediately. If your prompt doesn‚Äôt mention dietary restrictions, fix the prompt rather than generating specialized test queries.</p>
<p>After iterating on your tuples and prompts, <strong>run these synthetic queries through your actual system to capture full traces</strong>. Sample 100 traces for error analysis. This number provides enough traces to manually review and identify failure patterns without being overwhelming.</p>
<p><a href="../../../blog/posts/evals-faq/what-is-the-best-approach-for-generating-synthetic-data.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-are-there-scenarios-where-synthetic-data-may-not-be-reliable" class="level2">
<h2 class="anchored" data-anchor-id="q-are-there-scenarios-where-synthetic-data-may-not-be-reliable">Q: Are there scenarios where synthetic data may not be reliable?</h2>
<p>Yes: synthetic data can mislead or mask issues. For guidance on generating synthetic data when appropriate, see What is the best approach for generating synthetic data?</p>
<p>Common scenarios where synthetic data fails:</p>
<ol type="1">
<li><p><strong>Complex domain-specific content</strong>: LLMs often miss the structure, nuance, or quirks of specialized documents (e.g., legal filings, medical records, technical forms). Without real examples, critical edge cases are missed.</p></li>
<li><p><strong>Low-resource languages or dialects</strong>: For low-resource languages or dialects, LLM-generated samples are often unrealistic. Evaluations based on them won‚Äôt reflect actual performance.</p></li>
<li><p><strong>When validation is impossible</strong>: If you can‚Äôt verify synthetic sample realism (due to domain complexity or lack of ground truth), real data is important for accurate evaluation.</p></li>
<li><p><strong>High-stakes domains</strong>: In high-stakes domains (medicine, law, emergency response), synthetic data often lacks subtlety and edge cases. Errors here have serious consequences, and manual validation is difficult.</p></li>
<li><p><strong>Underrepresented user groups</strong>: For underrepresented user groups, LLMs may misrepresent context, values, or challenges. Synthetic data can reinforce biases in the training data of the LLM.</p></li>
</ol>
<p><a href="../../../blog/posts/evals-faq/are-there-scenarios-where-synthetic-data-may-not-be-reliable.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries">Q: How do I approach evaluation when my system handles diverse user queries?</h2>
<blockquote class="blockquote">
<p>Complex applications often support vastly different query patterns‚Äîfrom ‚ÄúWhat‚Äôs the return policy?‚Äù to ‚ÄúCompare pricing trends across regions for products matching these criteria.‚Äù Each query type exercises different system capabilities, leading to confusion on how to design eval criteria.</p>
</blockquote>
<p><strong><em><a href="https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc" target="_blank">Error Analysis</a> is all you need.</em></strong> Your evaluation strategy should emerge from observed failure patterns (e.g.&nbsp;error analysis), not predetermined query classifications. Rather than creating a massive evaluation matrix covering every query type you can imagine, let your system‚Äôs actual behavior guide where you invest evaluation effort.</p>
<p>During error analysis, you‚Äôll likely discover that certain query categories share failure patterns. For instance, all queries requiring temporal reasoning might struggle regardless of whether they‚Äôre simple lookups or complex aggregations. Similarly, queries that need to combine information from multiple sources might fail in consistent ways. These patterns discovered through error analysis should drive your evaluation priorities. It could be that query category is a fine way to group failures, but you don‚Äôt know that until you‚Äôve analyzed your data.</p>
<p>To see an example of basic error analysis in action, <a href="https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc" target="_blank">see this video</a>.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/e2i6JbU2R-s" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><a href="../../../blog/posts/evals-faq/how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-can-i-efficiently-sample-production-traces-for-review" class="level2">
<h2 class="anchored" data-anchor-id="q-how-can-i-efficiently-sample-production-traces-for-review">Q: How can I efficiently sample production traces for review?</h2>
<p>It can be cumbersome to review traces randomly, especially when most traces don‚Äôt have an error. These sampling strategies help you find traces more likely to reveal problems:</p>
<ul>
<li><strong>Outlier detection:</strong> Sort by any metric (response length, latency, tool calls) and review extremes.</li>
<li><strong>User feedback signals:</strong> Prioritize traces with negative feedback, support tickets, or escalations.</li>
<li><strong>Metric-based sorting:</strong> Generic metrics can serve as exploration signals to find interesting traces. Review both high and low scores and treat them as exploration clues. Based on what you learn, you can build custom evaluators for the failure modes you find.</li>
<li><strong>Stratified sampling:</strong> Group traces by key dimensions (user type, feature, query category) and sample from each group.</li>
<li><strong>Embedding clustering:</strong> Generate embeddings of queries and cluster them to reveal natural groupings. Sample proportionally from each cluster, but oversample small clusters for edge cases. There‚Äôs no right answer for clustering‚Äîit‚Äôs an exploration technique to surface patterns you might miss manually.</li>
</ul>
<p>As you get more sophisticated with how you sample, you can incorporate these tactics into the design of your annotation tools.</p>
<p><a href="../../../blog/posts/evals-faq/how-can-i-efficiently-sample-production-traces-for-review.html" class="faq-individual-link">‚Üó Focus view</a></p>
<hr>
<div class="cta" style="text-align: center;">
<p><strong>üëâ <em>Want to learn more about AI Evals? Check out our <a href="https://bit.ly/evals-ai" target="_blank">AI Evals course</a></em></strong>. It‚Äôs a live cohort with hands on exercises and office hours. Here is a <a href="https://bit.ly/evals-ai" target="_blank">15% discount code</a> for readers. üëà</p>
</div>
<hr>
</section>
</section>
<section id="evaluation-design-methodology" class="level1">
<h1>Evaluation Design &amp; Methodology</h1>
<section id="q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales" class="level2">
<h2 class="anchored" data-anchor-id="q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales">Q: Why do you recommend binary (pass/fail) evaluations instead of 1-5 ratings (Likert scales)?</h2>
<blockquote class="blockquote">
<p>Engineers often believe that Likert scales (1-5 ratings) provide more information than binary evaluations, allowing them to track gradual improvements. However, this added complexity often creates more problems than it solves in practice.</p>
</blockquote>
<p>Binary evaluations force clearer thinking and more consistent labeling. Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, detecting statistical differences requires larger sample sizes, and annotators often default to middle values to avoid making hard decisions.</p>
<p>Having binary options forces people to make a decision rather than hiding uncertainty in middle values. Binary decisions are also faster to make during error analysis - you don‚Äôt waste time debating whether something is a 3 or 4.</p>
<p>For tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using a scale. For example, instead of rating factual accuracy 1-5, you could track ‚Äú4 out of 5 expected facts included‚Äù as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria.</p>
<p>Start with binary labels to understand what ‚Äòbad‚Äô looks like. Numeric labels are advanced and usually not necessary.</p>
<p><a href="../../../blog/posts/evals-faq/why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-should-i-practice-eval-driven-development" class="level2">
<h2 class="anchored" data-anchor-id="q-should-i-practice-eval-driven-development">Q: Should I practice eval-driven development?</h2>
<p><strong>Generally no.</strong> Eval-driven development (writing evaluators before implementing features) sounds appealing but creates more problems than it solves. Unlike traditional software where failure modes are predictable, LLMs have infinite surface area for potential failures. You can‚Äôt anticipate what will break.</p>
<p>A better approach is to start with error analysis. Write evaluators for errors you discover, not errors you imagine. This avoids getting blocked on what to evaluate and prevents wasted effort on metrics that have no impact on actual system quality.</p>
<p><strong>Exception:</strong> Eval-driven development may work for specific constraints where you know exactly what success looks like. If adding ‚Äúnever mention competitors,‚Äù writing that evaluator early may be acceptable.</p>
<p>Most importantly, always do a cost-benefit analysis before implementing an eval. Ask whether the failure mode justifies the investment. Error analysis reveals which failures actually matter for your users.</p>
<p><a href="../../../blog/posts/evals-faq/should-i-practice-eval-driven-development.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-should-i-build-automated-evaluators-for-every-failure-mode-i-find" class="level2">
<h2 class="anchored" data-anchor-id="q-should-i-build-automated-evaluators-for-every-failure-mode-i-find">Q: Should I build automated evaluators for every failure mode I find?</h2>
<p>Focus automated evaluators on failures that persist after fixing your prompts. Many teams discover their LLM doesn‚Äôt meet preferences they never actually specified - like wanting short responses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before building complex evaluation infrastructure.</p>
<p>Consider the cost hierarchy of different evaluator types. Simple assertions and reference-based checks (comparing against known correct answers) are cheap to build and maintain. LLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and coordination between developers, PMs, and domain experts. This cost difference should shape your evaluation strategy.</p>
<p>Only build expensive evaluators for problems you‚Äôll iterate on repeatedly. Since LLM-as-Judge comes with significant overhead, save it for persistent generalization failures - not issues you can fix trivially. Start with cheap code-based checks where possible: regex patterns, structural validation, or execution tests. Reserve complex evaluation for subjective qualities that can‚Äôt be captured by simple rules.</p>
<p><a href="../../../blog/posts/evals-faq/should-i-build-automated-evaluators-for-every-failure-mode-i-find.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-should-i-use-ready-to-use-evaluation-metrics" class="level2">
<h2 class="anchored" data-anchor-id="q-should-i-use-ready-to-use-evaluation-metrics">Q: Should I use "ready-to-use" evaluation metrics?</h2>
<p><strong>No.&nbsp;Generic evaluations waste time and create false confidence.</strong> (Unless you‚Äôre using them for exploration).</p>
<p>One instructor noted:</p>
<blockquote class="blockquote">
<p>‚ÄúAll you get from using these prefab evals is you don‚Äôt know what they actually do and in the best case they waste your time and in the worst case they create an illusion of confidence that is unjustified.‚Äù<sup>1</sup></p>
</blockquote>
<p>Generic evaluation metrics are everywhere. Eval libraries contain scores like helpfulness, coherence, quality, etc. promising easy evaluation. These metrics measure abstract qualities that may not matter for your use case. Good scores on them don‚Äôt mean your system works.</p>
<p>Instead, conduct error analysis to understand failures. Define binary failure modes based on real problems. Create custom evaluators for those failures and validate them against human judgment. Essentially, the entire evals process.</p>
<p>Experienced practitioners may still use these metrics, just not how you‚Äôd expect. As Picasso said: ‚ÄúLearn the rules like a pro, so you can break them like an artist.‚Äù Once you understand why generic metrics fail as evaluations, you can repurpose them as exploration tools to find interesting traces (explained in the next FAQ).</p>
<p><a href="../../../blog/posts/evals-faq/should-i-use-ready-to-use-evaluation-metrics.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-are-similarity-metrics-bertscore-rouge-etc.-useful-for-evaluating-llm-outputs" class="level2">
<h2 class="anchored" data-anchor-id="q-are-similarity-metrics-bertscore-rouge-etc.-useful-for-evaluating-llm-outputs">Q: Are similarity metrics (BERTScore, ROUGE, etc.) useful for evaluating LLM outputs?</h2>
<p>Generic metrics like BERTScore, ROUGE, cosine similarity, etc. are not useful for evaluating LLM outputs in most AI applications. Instead, we recommend using error analysis to identify metrics specific to your application‚Äôs behavior. We recommend designing binary pass/fail.) evals (using LLM-as-judge) or code-based assertions.</p>
<p>As an example, consider a real estate CRM assistant. Suggesting showings that aren‚Äôt available (can be tested with an assertion) or confusing client personas (can be tested with a LLM-as-judge) is problematic . Generic metrics like similarity or verbosity won‚Äôt catch this. A relevant quote from the course:</p>
<blockquote class="blockquote">
<p>‚ÄúThe abuse of generic metrics is endemic. Many eval vendors promote off the shelf metrics, which ensnare engineers into superfluous tasks.‚Äù</p>
</blockquote>
<p>Similarity metrics aren‚Äôt always useless. They have utility in domains like search and recommendation (and therefore can be useful for optimizing and debugging retrieval for RAG). For example, cosine similarity between embeddings can measure semantic closeness in retrieval systems, and average pairwise similarity can assess output diversity (where lower similarity indicates higher diversity).</p>
<p><a href="../../../blog/posts/evals-faq/are-similarity-metrics-bertscore-rouge-etc-useful-for-evaluating-llm-outputs.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-can-i-use-the-same-model-for-both-the-main-task-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="q-can-i-use-the-same-model-for-both-the-main-task-and-evaluation">Q: Can I use the same model for both the main task and evaluation?</h2>
<p>For LLM-as-Judge selection, using the same model is usually fine because the judge is doing a different task than your main LLM pipeline. While <a href="https://arxiv.org/pdf/2508.06709">research has shown</a> that models can exhibit bias when evaluating their own outputs, what ultimately matters is how well your judge aligns with human judgments. The judges we recommend building do scoped binary classification tasks. We‚Äôve found that iterative alignment with human labels is usually achievable on this constrained task.</p>
<p>Focus on achieving high True Positive Rate (TPR) and True Negative Rate (TNR) with your judge on a held out labeled test set. If you struggle to achieve good alignment with human scores, then consider trying a different model. However onboarding new model providers may involve non-trivial effort in some organizations, which is why we don‚Äôt advocate for using different models by default unless there‚Äôs a specific alignment issue.</p>
<p>When selecting judge models, start with the most capable models available to establish strong alignment with human judgments. You can optimize for cost later once you‚Äôve established reliable evaluation criteria.</p>
<p><a href="../../../blog/posts/evals-faq/can-i-use-the-same-model-for-both-the-main-task-and-evaluation.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-do-we-evaluate-a-models-ability-to-express-uncertainty-or-know-what-it-doesnt-know" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-we-evaluate-a-models-ability-to-express-uncertainty-or-know-what-it-doesnt-know">Q: How do we evaluate a model‚Äôs ability to express uncertainty or "know what it doesn‚Äôt know"?</h2>
<p>Many applications require a model that can refuse to answer a question when it lacks sufficient information. To evaluate whether this refusal behavior is well-calibrated, you need to test if the model refuses at the appropriate times without refusing to answer questions it <em>should</em> be able to answer.</p>
<p>To do this effectively, you should construct an evaluation set that has the following components:</p>
<ol type="1">
<li><strong>Answerable Questions:</strong> Scenarios where a correct, verifiable answer is present in the model‚Äôs provided context or general knowledge.</li>
<li><strong>Unanswerable Questions:</strong> Scenarios designed to tempt the model to hallucinate. These include questions with false premises, queries about information explicitly missing from context, or topics far outside its knowledge base.</li>
</ol>
<p>While the exact proportion isn‚Äôt critical, a balanced set with a roughly equal number of answerable and unanswerable questions is a good starting point. The diversity and difficulty of the questions are more important than the precise ratio.</p>
<p>The evaluation itself is a binary (Pass/Fail) check of the model‚Äôs judgment. A ‚ÄúPass‚Äù requires the model to satisfy two conditions: it must answer the answerable questions while also refusing to answer the unanswerable ones. A failure is defined as providing a fabricated answer to an unanswerable question, which indicates poor calibration.</p>
<p>In the research literature, this capability is known as ‚ÄúAbstention Ability.‚Äù To improve this behavior, it is worth <a href="https://arxiv.org/search/?query=Abstention+Ability&amp;searchtype=all">searching for this term on Arxiv</a> to understand the latest techniques.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-we-evaluate-a-models-ability-to-express-uncertainty-or-know-what-it-doesnt-know.html" class="faq-individual-link">‚Üó Focus view</a></p>
<hr>
<div class="cta" style="text-align: center;">
<p><strong>üëâ <em>Want to learn more about AI Evals? Check out our <a href="https://bit.ly/evals-ai" target="_blank">AI Evals course</a></em></strong>. It‚Äôs a live cohort with hands on exercises and office hours. Here is a <a href="https://bit.ly/evals-ai" target="_blank">15% discount code</a> for readers. üëà</p>
</div>
<hr>
</section>
</section>
<section id="human-annotation-process" class="level1">
<h1>Human Annotation &amp; Process</h1>
<section id="q-how-many-people-should-annotate-my-llm-outputs" class="level2">
<h2 class="anchored" data-anchor-id="q-how-many-people-should-annotate-my-llm-outputs">Q: How many people should annotate my LLM outputs?</h2>
<p>For most small to medium-sized companies, appointing a single domain expert as a ‚Äúbenevolent dictator‚Äù is the most effective approach. This person‚Äîwhether it‚Äôs a psychologist for a mental health chatbot, a lawyer for legal document analysis, or a customer service director for support automation‚Äîbecomes the definitive voice on quality standards.</p>
<p>A single expert eliminates annotation conflicts and prevents the paralysis that comes from ‚Äútoo many cooks in the kitchen‚Äù. The benevolent dictator can incorporate input and feedback from others, but they drive the process. If you feel like you need five subject matter experts to judge a single interaction, it‚Äôs a sign your product scope might be too broad.</p>
<p>However, larger organizations or those operating across multiple domains (like a multinational company with different cultural contexts) may need multiple annotators. When you do use multiple people, you‚Äôll need to measure their agreement using metrics like Cohen‚Äôs Kappa, which accounts for agreement beyond chance. However, use your judgment. Even in larger companies, a single expert is often enough.</p>
<p>Start with a benevolent dictator whenever feasible. Only add complexity when your domain demands it.</p>
<p><a href="../../../blog/posts/evals-faq/how-many-people-should-annotate-my-llm-outputs.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-should-product-managers-and-engineers-collaborate-on-error-analysis-how" class="level2">
<h2 class="anchored" data-anchor-id="q-should-product-managers-and-engineers-collaborate-on-error-analysis-how">Q: Should product managers and engineers collaborate on error analysis? How?</h2>
<p>At the outset, collaborate to establish shared context. Engineers catch technical issues like retrieval issues and tool errors. PMs identify product failures like unmet user expectations, confusing responses, or missing features users expect.</p>
<p>As time goes on you should lean towards a benevolent dictator for error analysis: a domain expert or PM who understands user needs. Empower domain experts to evaluate actual outcomes rather than technical implementation. Ask ‚ÄúHas an appointment been made?‚Äù not ‚ÄúDid the tool call succeed?‚Äù The best way to empower the domain expert is to give them custom annotation tools that display system outcomes alongside traces. Show the confirmation, generated email, or database update that validates goal completion. Keep all context on one screen so non-technical reviewers focus on results.</p>
<p><a href="../../../blog/posts/evals-faq/should-product-managers-and-engineers-collaborate-on-error-analysis-how.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-should-i-outsource-annotation-labeling-to-a-third-party" class="level2">
<h2 class="anchored" data-anchor-id="q-should-i-outsource-annotation-labeling-to-a-third-party">Q: Should I outsource annotation &amp; labeling to a third party?</h2>
<p>Outsourcing error analysis is usually a big mistake (with some exceptions). The core of evaluation is building the product intuition that only comes from systematically analyzing your system‚Äôs failures. You should be extremely skeptical of this process being delegated.</p>
<section id="the-dangers-of-outsourcing" class="level3">
<h3 class="anchored" data-anchor-id="the-dangers-of-outsourcing"><strong>The Dangers of Outsourcing</strong></h3>
<p>When you outsource annotation, you often break the feedback loop between observing a failure and understanding how to improve the product. Problems with outsourcing include:</p>
<ul>
<li>Superficial Labeling: Even well-defined metrics require nuanced judgment that external teams lack. A critical misstep in error analysis is excluding domain experts from the labeling process. Outsourcing this task to those without domain expertise, like general developers or IT staff, often leads to superficial or incorrect labeling.<br>
</li>
<li>Loss of Unspoken Knowledge: A principal domain expert possesses tacit knowledge and user understanding that cannot be fully captured in a rubric. Involving these experts helps uncover their preferences and expectations, which they might not be able to fully articulate upfront.<br>
</li>
<li>Annotation Conflicts and Misalignment: Without a shared context, external annotators can create more disagreement than they resolve. Achieving alignment is a challenge even for internal teams, which means you will spend even more time on this process.</li>
</ul>
</section>
<section id="the-recommended-approach-build-internal-capability" class="level3">
<h3 class="anchored" data-anchor-id="the-recommended-approach-build-internal-capability"><strong>The Recommended Approach: Build Internal Capability</strong></h3>
<p>Instead of outsourcing, focus on building an efficient internal evaluation process.</p>
<p>1. Appoint a ‚ÄúBenevolent Dictator‚Äù. For most teams, the most effective strategy is to appoint a single, internal domain expert as the final decision-maker on quality. This individual sets the standard, ensures consistency, and develops a sense of ownership.</p>
<p>2. Use a collaborative workflow for multiple annotators. If multiple annotators are necessary, follow a structured process to ensure alignment: * Draft an initial rubric with clear Pass/Fail definitions and examples. * Have each annotator label a shared set of traces independently to surface differences in interpretation. * Measure Inter-Annotator Agreement (IAA) using a chance-corrected metric like Cohen‚Äôs Kappa. * Facilitate alignment sessions to discuss disagreements and refine the rubric. * Iterate on this process until agreement is consistently high.</p>
</section>
<section id="how-to-handle-capacity-constraints" class="level3">
<h3 class="anchored" data-anchor-id="how-to-handle-capacity-constraints"><strong>How to Handle Capacity Constraints</strong></h3>
<p>Building internal capacity does not mean you have to label every trace. Use these strategies to manage the workload:</p>
<ul>
<li>Smart Sampling: Review a small, representative sample of traces thoroughly. It is more effective to analyze 100 diverse traces to find patterns than to superficially label thousands.<br>
</li>
<li>The ‚ÄúThink-Aloud‚Äù Protocol: To make the most of limited expert time, use this technique from usability testing. Ask an expert to verbalize their thought process while reviewing a handful of traces. This method can uncover deep insights in a single one-hour session.<br>
</li>
<li>Build Lightweight Custom Tools: Build custom annotation tools to streamline the review process, increasing throughput.</li>
</ul>
</section>
<section id="exceptions-for-external-help" class="level3">
<h3 class="anchored" data-anchor-id="exceptions-for-external-help"><strong>Exceptions for External Help</strong></h3>
<p>While outsourcing the core error analysis process is not recommended, there are some scenarios where external help is appropriate:</p>
<ul>
<li>Purely Mechanical Tasks: For highly objective, unambiguous tasks like identifying a phone number or validating an email address, external annotators can be used after a rigorous internal process has defined the rubric.<br>
</li>
<li>Tasks Without Product Context: Well-defined tasks that don‚Äôt require understanding your product‚Äôs specific requirements can be outsourced. Translation is a good example: it requires linguistic expertise but not deep product knowledge.<br>
</li>
<li>Engaging Subject Matter Experts: Hiring external SMEs to act as your internal domain experts is not outsourcing; it is bringing the necessary expertise into your evaluation process. For example, <a href="https://www.ankihub.net/" target="_blank">AnkiHub</a> hired 4th-year medical students to evaluate their RAG systems for medical content rather than outsourcing to generic annotators.</li>
</ul>
<p><a href="../../../blog/posts/evals-faq/should-i-outsource-annotation-and-labeling-to-a-third-party.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-what-parts-of-evals-can-be-automated-with-llms" class="level2">
<h2 class="anchored" data-anchor-id="q-what-parts-of-evals-can-be-automated-with-llms">Q: What parts of evals can be automated with LLMs?</h2>
<p>LLMs can speed up parts of your eval workflow, but they can‚Äôt replace human judgment where your expertise is essential. For example, if you let an LLM handle all of <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html" target="_blank">error analysis</a> (i.e., reviewing and annotating traces), you might overlook failure cases that matter for your product. Suppose users keep mentioning ‚Äúlag‚Äù in feedback, but the LLM lumps these under generic ‚Äúperformance issues‚Äù instead of creating a ‚Äúlatency‚Äù category. You‚Äôd miss a recurring complaint about slow response times and fail to prioritize a fix.</p>
<p>That said, LLMs are valuable tools for accelerating certain parts of the evaluation workflow <em>when used with oversight</em>.</p>
<section id="here-are-some-areas-where-llms-can-help" class="level3">
<h3 class="anchored" data-anchor-id="here-are-some-areas-where-llms-can-help">Here are some areas where LLMs can help:</h3>
<ul>
<li><strong>First-pass axial coding:</strong> After you‚Äôve open coded 30‚Äì50 traces yourself, use an LLM to organize your raw failure notes into proposed groupings. This helps you quickly spot patterns, but always review and refine the clusters yourself. <em>Note: If you aren‚Äôt familiar with axial and open coding, see <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html" target="_blank">this faq</a>.</em></li>
<li><strong>Mapping annotations to failure modes:</strong> Once you‚Äôve defined failure categories, you can ask an LLM to suggest which categories apply to each new trace (e.g., ‚ÄúGiven this annotation: [open_annotation] and these failure modes: [list_of_failure_modes], which apply?‚Äù).<br>
</li>
<li><strong>Suggesting prompt improvements:</strong> When you notice recurring problems, have the LLM propose concrete changes to your prompts. Review these suggestions before adopting any changes.<br>
</li>
<li><strong>Analyzing annotation data:</strong> Use LLMs or AI-powered notebooks to find patterns in your labels, such as ‚Äúreports of lag increase 3x during peak usage hours‚Äù or ‚Äúslow response times are mostly reported from users on mobile devices.‚Äù</li>
</ul>
</section>
<section id="however-you-shouldnt-outsource-these-activities-to-an-llm" class="level3">
<h3 class="anchored" data-anchor-id="however-you-shouldnt-outsource-these-activities-to-an-llm">However, you shouldn‚Äôt outsource these activities to an LLM:</h3>
<ul>
<li><strong>Initial open coding:</strong> Always read through the raw traces yourself at the start. This is how you discover new types of failures, understand user pain points, and build intuition about your data. Never skip this or delegate it.<br>
</li>
<li><strong>Validating failure taxonomies:</strong> LLM-generated groupings need your review. For example, an LLM might group both ‚Äúapp crashes after login‚Äù and ‚Äúlogin takes too long‚Äù under a single ‚Äúlogin issues‚Äù category, even though one is a stability problem and the other is a performance problem. Without your intervention, you‚Äôd miss that these issues require different fixes.<br>
</li>
<li><strong>Ground truth labeling:</strong> For any data used for testing/validating LLM-as-Judge evaluators, hand-validate each label. LLMs can make mistakes that lead to unreliable benchmarks.<br>
</li>
<li><strong>Root cause analysis:</strong> LLMs may point out obvious issues, but only human review will catch patterns like errors that occur in specific workflows or edge cases‚Äîsuch as bugs that happen only when users paste data from Excel.</li>
</ul>
<p>In conclusion, start by examining data manually to understand what‚Äôs actually going wrong. Use LLMs to scale what you‚Äôve learned, not to avoid looking at data.</p>
<p><a href="../../../blog/posts/evals-faq/what-parts-of-evals-can-be-automated-with-llms.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-should-i-stop-writing-prompts-manually-in-favor-of-automated-tools" class="level2">
<h2 class="anchored" data-anchor-id="q-should-i-stop-writing-prompts-manually-in-favor-of-automated-tools">Q: Should I stop writing prompts manually in favor of automated tools?</h2>
<p>Automating prompt engineering can be tempting, but you should be skeptical of tools that promise to optimize prompts for you, especially in early stages of development. When you write a prompt, you are forced to clarify your assumptions and externalize your requirements. Good writing is good thinking <sup>2</sup>. If you delegate this task to an automated tool too early, you risk never fully understanding your own requirements or the model‚Äôs failure modes.</p>
<p>This is because automated prompt optimization typically hill-climb a predefined evaluation metric. It can refine a prompt to perform better on known failures, but it cannot discover <em>new</em> ones. Discovering new errors requires error analysis. Furthermore, research shows that evaluation criteria tends to shift after reviewing a model‚Äôs outputs, a phenomenon known as ‚Äúcriteria drift‚Äù <sup>3</sup>. This means that evaluation is an iterative, human-driven sensemaking process, not a static target that can be set once and handed off to an optimizer.</p>
<p>A pragmatic approach is to use LLMs to improve your prompt based on open coding (open-ended notes about traces). This way, you maintain a human in the loop who is looking at the data and externalizing their requirements. Once you have a high-quality set of evals, prompt optimization can be effective for that last mile of performance.</p>
<p><a href="../../../blog/posts/evals-faq/should-i-stop-writing-prompts-manually-in-favor-of-automated-tools.html" class="faq-individual-link">‚Üó Focus view</a></p>
<hr>
<div class="cta" style="text-align: center;">
<p><strong>üëâ <em>Want to learn more about AI Evals? Check out our <a href="https://bit.ly/evals-ai" target="_blank">AI Evals course</a></em></strong>. It‚Äôs a live cohort with hands on exercises and office hours. Here is a <a href="https://bit.ly/evals-ai" target="_blank">15% discount code</a> for readers. üëà</p>
</div>
<hr>
</section>
</section>
<section id="tools-infrastructure" class="level1">
<h1>Tools &amp; Infrastructure</h1>
<section id="q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf" class="level2">
<h2 class="anchored" data-anchor-id="q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf">Q: Should I build a custom annotation tool or use something off-the-shelf?</h2>
<p><strong>Build a custom annotation tool.</strong> This is the single most impactful investment you can make for your AI evaluation workflow. With AI-assisted development tools like Cursor or Lovable, you can build a tailored interface in hours. I often find that teams with custom annotation tools iterate ~10x faster.</p>
<p>Custom tools excel because:</p>
<ul>
<li>They show all your context from multiple systems in one place</li>
<li>They can render your data in a product specific way (images, widgets, markdown, buttons, etc.)</li>
<li>They‚Äôre designed for your specific workflow (custom filters, sorting, progress bars, etc.)</li>
</ul>
<p>Off-the-shelf tools may be justified when you need to coordinate dozens of distributed annotators with enterprise access controls. Even then, many teams find the configuration overhead and limitations aren‚Äôt worth it.</p>
<p><a href="https://youtu.be/fA4pe9bE0LY" target="_blank">Isaac‚Äôs Anki flashcard annotation app</a> shows the power of custom tools‚Äîhandling 400+ results per query with keyboard navigation and domain-specific evaluation criteria that would be nearly impossible to configure in a generic tool.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/fA4pe9bE0LY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><a href="../../../blog/posts/evals-faq/should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs" class="level2">
<h2 class="anchored" data-anchor-id="q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs">Q: What makes a good custom interface for reviewing LLM outputs?</h2>
<p>Great interfaces make human review fast, clear, and motivating. We recommend building your own annotation tool customized to your domain. The following features are possible enhancements we‚Äôve seen work well, but you don‚Äôt need all of them. The screenshots shown are illustrative examples to clarify concepts. In practice, I rarely implement all these features in a single app. It‚Äôs ultimately a judgment call based on your specific needs and constraints.</p>
<section id="render-traces-intelligently-not-generically" class="level3">
<h3 class="anchored" data-anchor-id="render-traces-intelligently-not-generically"><strong>1. Render Traces Intelligently, Not Generically</strong>:</h3>
<p>Present the trace in a way that‚Äôs intuitive for the domain. If you‚Äôre evaluating generated emails, render them to look like emails. If the output is code, use syntax highlighting. Allow the reviewer to see the full trace (user input, tool calls, and LLM reasoning), but keep less important details in collapsed sections that can be expanded. Here is an example of a custom annotation tool for reviewing real estate assistant emails:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/images/emailinterface1.png" class="img-fluid figure-img" style="width:75.0%" target="_blank"></p>
<figcaption>A custom interface for reviewing emails for a real estate assistant.</figcaption>
</figure>
</div>
</section>
<section id="show-progress-and-support-keyboard-navigation" class="level3">
<h3 class="anchored" data-anchor-id="show-progress-and-support-keyboard-navigation"><strong>2. Show Progress and Support Keyboard Navigation</strong>:</h3>
<p>Keep reviewers in a state of flow by minimizing friction and motivating completion. Include progress indicators (e.g., ‚ÄúTrace 45 of 100‚Äù) to keep the review session bounded and encourage completion. Enable hotkeys for navigating between traces (e.g., N for next), applying labels, and saving notes quickly. Below is an illustration of these features:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/images/hotkey.png" class="img-fluid figure-img" style="width:75.0%" target="_blank"></p>
<figcaption>An annotation interface with a progress bar and hotkey guide</figcaption>
</figure>
</div>
</section>
<section id="trace-navigation-through-clustering-filtering-and-search" class="level3">
<h3 class="anchored" data-anchor-id="trace-navigation-through-clustering-filtering-and-search"><strong>3. Trace navigation through clustering, filtering, and search</strong>:</h3>
<p>Allow reviewers to filter traces by metadata or search by keywords. Semantic search helps find conceptually similar problems. Clustering similar traces (like grouping by user persona) lets reviewers spot recurring issues and explore hypotheses. Below is an illustration of these features:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/images/group1.png" class="img-fluid figure-img" style="width:75.0%" target="_blank"></p>
<figcaption>Cluster view showing groups of emails, such as property-focused or client-focused examples. Reviewers can drill into a group to see individual traces.</figcaption>
</figure>
</div>
</section>
<section id="prioritize-labeling-traces-you-think-might-be-problematic" class="level3">
<h3 class="anchored" data-anchor-id="prioritize-labeling-traces-you-think-might-be-problematic"><strong>4. Prioritize labeling traces you think might be problematic</strong>:</h3>
<p>Surface traces flagged by guardrails, CI failures, or automated evaluators for review. Provide buttons to take actions like adding to datasets, filing bugs, or re-running pipeline tests. Display relevant context (pipeline version, eval scores, reviewer info) directly in the interface to minimize context switching. Below is an illustration of these ideas:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/images/ci.png" class="img-fluid figure-img" style="width:88.0%" target="_blank"></p>
<figcaption>A trace view that allows you to quickly see auto-evaluator verdict, add traces to dataset or open issues. Also shows metadata like pipeline version, reviewer info, and more.</figcaption>
</figure>
</div>
</section>
<section id="general-principle-keep-it-minimal" class="level3">
<h3 class="anchored" data-anchor-id="general-principle-keep-it-minimal">General Principle: Keep it minimal</h3>
<p>Keep your annotation interface minimal. Only incorporate these ideas if they provide a benefit that outweighs the additional complexity and maintenance overhead.</p>
<p><a href="../../../blog/posts/evals-faq/what-makes-a-good-custom-interface-for-reviewing-llm-outputs.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself" class="level2">
<h2 class="anchored" data-anchor-id="q-what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself">Q: What gaps in eval tooling should I be prepared to fill myself?</h2>
<p>Most eval tools handle the basics well: logging complete traces, tracking metrics, prompt playgrounds, and annotation queues. These are table stakes. Here are four areas where you‚Äôll likely need to supplement existing tools.</p>
<p>Watch for vendors addressing these gaps: it‚Äôs a strong signal they understand practitioner needs.</p>
<section id="error-analysis-and-pattern-discovery" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-and-pattern-discovery">1. Error Analysis and Pattern Discovery</h3>
<p>After reviewing traces where your AI fails, can your tooling automatically cluster similar issues? For instance, if multiple traces show the assistant using casual language for luxury clients, you need something that recognizes this broader ‚Äúpersona-tone mismatch‚Äù pattern. We recommend building capabilities that use AI to suggest groupings, rewrite your observations into clearer failure taxonomies, help find similar cases through semantic search, etc.</p>
</section>
<section id="ai-powered-assistance-throughout-the-workflow" class="level3">
<h3 class="anchored" data-anchor-id="ai-powered-assistance-throughout-the-workflow">2. AI-Powered Assistance Throughout the Workflow</h3>
<p>The most effective workflows use AI to accelerate every stage of evaluation. During error analysis, you want an LLM helping categorize your open-ended observations into coherent failure modes. For example, you might annotate several traces with notes like ‚Äúwrong tone for investor,‚Äù ‚Äútoo casual for luxury buyer,‚Äù etc. Your tooling should recognize these as the same underlying pattern and suggest a unified ‚Äúpersona-tone mismatch‚Äù category.</p>
<p>You‚Äôll also want AI assistance in proposing fixes. After identifying 20 cases where your assistant omits pet policies from property summaries, can your workflow analyze these failures and suggest specific prompt modifications? Can it draft refinements to your SQL generation instructions when it notices patterns of missing WHERE clauses?</p>
<p>Additionally, good workflows help you conduct data analysis of your annotations and traces. I like using notebooks with AI in-the-loop like <a href="https://julius.ai/" target="_blank">Julius</a>,<a href="https://hex.tech" target="_blank">Hex</a> or <a href="https://solveit.fast.ai/" target="_blank">SolveIt</a>. These help me discover insights like ‚Äúlocation ambiguity errors spike 3x when users mention neighborhood names‚Äù or ‚Äútone mismatches occur 80% more often in email generation than other modalities.‚Äù</p>
</section>
<section id="custom-evaluators-over-generic-metrics" class="level3">
<h3 class="anchored" data-anchor-id="custom-evaluators-over-generic-metrics">3. Custom Evaluators Over Generic Metrics</h3>
<p>Be prepared to build most of your evaluators from scratch. Generic metrics like ‚Äúhallucination score‚Äù or ‚Äúhelpfulness rating‚Äù rarely capture what actually matters for your application‚Äîlike proposing unavailable showing times or omitting budget constraints from emails. In our experience, successful teams spend most of their effort on application-specific metrics.</p>
</section>
<section id="apis-that-support-custom-annotation-apps" class="level3">
<h3 class="anchored" data-anchor-id="apis-that-support-custom-annotation-apps">4. APIs That Support Custom Annotation Apps</h3>
<p>Custom annotation interfaces work best for most teams. This requires observability platforms with thoughtful APIs. I often have to build my own libraries and abstractions just to make bulk data export manageable. You shouldn‚Äôt have to paginate through thousands of requests or handle timeout-prone endpoints just to get your data. Look for platforms that provide true bulk export capabilities and, crucially, APIs that let you write annotations back efficiently.</p>
<p><a href="../../../blog/posts/evals-faq/what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-whats-your-favorite-eval-vendor" class="level2">
<h2 class="anchored" data-anchor-id="q-whats-your-favorite-eval-vendor">Q: What‚Äôs your favorite eval vendor?</h2>
<p>Eval tools are in an intensely competitive space. It would be futile to compare their features. If I tried to do such an analysis, it would be invalidated in a week! Vendors I encounter the most organically in my work are: <a href="https://www.langchain.com/langsmith" target="_blank">Langsmith</a>, <a href="https://arize.com/" target="_blank">Arize</a> and <a href="https://www.braintrust.dev/" target="_blank">Braintrust</a>.</p>
<p>When I help clients with vendor selection, the decision weighs heavily towards who can offer the best support, as opposed to purely features. This changes depending on size of client, use case, etc. Yes - it‚Äôs mainly the human factor that matters, and dare I say, vibes.</p>
<p>I have no favorite vendor. At the core, their features are very similar - and I often build <a href="https://hamel.dev/blog/posts/evals/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf" target="_blank">custom tools</a> on top of them to fit my needs.</p>
<p>Here is a <a href="../../../blog/posts/eval-tools/">video series</a> that has a live commentary on the relative strengths and weaknesses of the three aforementioned vendors.</p>
<p><a href="../../../blog/posts/evals-faq/whats-your-favorite-eval-vendor.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-should-i-version-and-manage-prompts" class="level2">
<h2 class="anchored" data-anchor-id="q-how-should-i-version-and-manage-prompts">Q: How should I version and manage prompts?</h2>
<p>There is an unavoidable tension between keeping prompts close to the code vs.&nbsp;an environment that non-technical stakeholders can access.</p>
<p><strong>My preferred approach is storing prompts in Git.</strong> This treats them as software artifacts that are versioned, reviewed, and deployed atomically with the application code. While the Git command line is unfriendly for non-technical folks, the <a href="https://github.com">GitHub</a> web interface and the GitHub <a href="https://desktop.github.com/">Desktop app</a> make it very approachable. When I was working at GitHub, I worked with many non-technical professionals, including lawyers and accountants, who used these tools effectively. Here is a <a href="https://ben.balter.com/2023/03/02/github-for-non-technical-roles/">blog post</a> aimed at non-technical folks to get started.</p>
<p>Alternatively, most vendors in the LLM tooling space, such as observability platforms like Arize, Braintrust, and LangSmith, offer dedicated prompt management tools. These are accessible for rapid iteration but risk creating additional layers of indirection.</p>
<p><strong>Why prompt management tools often fall short:</strong> AI products typically involve many moving parts: tools, RAG, agents, etc. Prompt management tools are inherently limiting because they can‚Äôt easily execute your application‚Äôs code. Even when they can, there‚Äôs often significant indirection involved, making it difficult to test prompts with your system‚Äôs capabilities.</p>
<p><strong>When possible, a notebook provides a great solution for prompt experimentation</strong> If you have Python entry points into your codebase or your codebase is written in Python, Jupyter notebooks are particularly powerful for this purpose. You can experiment with prompts and iterate on your actual AI agents with their full tool and RAG capabilities. This makes it much easier to understand how your system works in practice. Additionally, you can create widgets and small user interfaces within notebooks, giving you the best of both worlds for experimentation and iteration. To see what this looks like in practice, Teresa Torres gives a fantastic, hands-on walkthrough of how she, as a PM, used notebooks for the entire eval and experimentation lifecycle:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/N-qAOv_PNPc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>If notebooks are not feasible for your code base, an <a href="../../../blog/posts/field-guide/#build-bridges-not-gatekeepers">‚Äãintegrated prompt environment</a>‚Äã can be effective for experimentation. Either way, I prefer to version and manage prompts in Git.</p>
<p><a href="../../../blog/posts/evals-faq/how-should-i-version-and-manage-prompts.html" class="faq-individual-link">‚Üó Focus view</a></p>
<hr>
<div class="cta" style="text-align: center;">
<p><strong>üëâ <em>Want to learn more about AI Evals? Check out our <a href="https://bit.ly/evals-ai" target="_blank">AI Evals course</a></em></strong>. It‚Äôs a live cohort with hands on exercises and office hours. Here is a <a href="https://bit.ly/evals-ai" target="_blank">15% discount code</a> for readers. üëà</p>
</div>
<hr>
</section>
</section>
<section id="production-deployment" class="level1">
<h1>Production &amp; Deployment</h1>
<section id="q-how-are-evaluations-used-differently-in-cicd-vs.-monitoring-production" class="level2">
<h2 class="anchored" data-anchor-id="q-how-are-evaluations-used-differently-in-cicd-vs.-monitoring-production">Q: How are evaluations used differently in CI/CD vs.&nbsp;monitoring production?</h2>
<p>The most important difference between CI vs.&nbsp;production evaluation is the data used for testing.</p>
<p>Test datasets for CI are small (in many cases 100+ examples) and purpose-built. Examples cover core features, regression tests for past bugs, and known edge cases. Since CI tests are run frequently, the cost of each test has to be carefully considered (that‚Äôs why you carefully curate the dataset). Favor assertions or other deterministic checks over LLM-as-judge evaluators.</p>
<p>For evaluating production traffic, you can sample live traces and run evaluators against them asynchronously. Since you usually lack reference outputs on production data, you might rely more on on more expensive reference-free evaluators like LLM-as-judge. Additionally, track confidence intervals for production metrics. If the lower bound crosses your threshold, investigate further.</p>
<p>These two systems are complementary: when production monitoring reveals new failure patterns through error analysis and evals, add representative examples to your CI dataset. This mitigates regressions on new issues.</p>
<p><a href="../../../blog/posts/evals-faq/how-are-evaluations-used-differently-in-cicd-vs-monitoring-production.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-whats-the-difference-between-guardrails-evaluators" class="level2">
<h2 class="anchored" data-anchor-id="q-whats-the-difference-between-guardrails-evaluators">Q: What‚Äôs the difference between guardrails &amp; evaluators?</h2>
<p>Guardrails are <strong>inline safety checks</strong> that sit directly in the request/response path. They validate inputs or outputs <em>before</em> anything reaches a user, so they typically are:</p>
<ul>
<li><strong>Fast and deterministic</strong> ‚Äì typically a few milliseconds of latency budget.</li>
<li><strong>Simple and explainable</strong> ‚Äì regexes, keyword block-lists, schema or type validators, lightweight classifiers.</li>
<li><strong>Targeted at clear-cut, high-impact failures</strong> ‚Äì PII leaks, profanity, disallowed instructions, SQL injection, malformed JSON, invalid code syntax, etc.</li>
</ul>
<p>If a guardrail triggers, the system can redact, refuse, or regenerate the response. Because these checks are user-visible when they fire, false positives are treated as production bugs; teams version guardrail rules, log every trigger, and monitor rates to keep them conservative.</p>
<p>On the other hand, evaluators typically run <strong>after</strong> a response is produced. Evaluators measure qualities that simple rules cannot, such as factual correctness, completeness, etc. Their verdicts feed dashboards, regression tests, and model-improvement loops, but they do not block the original answer.</p>
<p>Evaluators are usually run asynchronously or in batch to afford heavier computation such as a <a href="https://hamel.dev/blog/posts/llm-judge/" target="_blank">LLM-as-a-Judge</a>. Inline use of an LLM-as-Judge is possible <em>only</em> when the latency budget and reliability targets allow it. Slow LLM judges might be feasible in a cascade that runs on the minority of borderline cases.</p>
<p>Apply guardrails for immediate protection against objective failures requiring intervention. Use evaluators for monitoring and improving subjective or nuanced criteria. Together, they create layered protection.</p>
<p>Word of caution: Do not use llm guardrails off the shelf blindly. Always <a href="https://hamel.dev/blog/posts/prompt/" target="_blank">look at the prompt</a>.</p>
<p><a href="../../../blog/posts/evals-faq/whats-the-difference-between-guardrails-evaluators.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-can-my-evaluators-also-be-used-to-automatically-fix-or-correct-outputs-in-production" class="level2">
<h2 class="anchored" data-anchor-id="q-can-my-evaluators-also-be-used-to-automatically-fix-or-correct-outputs-in-production">Q: Can my evaluators also be used to automatically <em>fix</em> or <em>correct</em> outputs in production?</h2>
<p>Yes, but only a specific subset of them. This is the distinction between an <strong>evaluator</strong> and a <strong>guardrail</strong> that we previously discussed. As a reminder:</p>
<ul>
<li><strong>Evaluators</strong> typically run <em>asynchronously</em> after a response has been generated. They measure quality but don‚Äôt interfere with the user‚Äôs immediate experience.<br>
</li>
<li><strong>Guardrails</strong> run <em>synchronously</em> in the critical path of the request, before the output is shown to the user. Their job is to prevent high-impact failures in real-time.</li>
</ul>
<p>There are two important decision criteria for deciding whether to use an evaluator as a guardrail:</p>
<ol type="1">
<li><p><strong>Latency &amp; Cost</strong>: Can the evaluator run fast enough and cheaply enough in the critical request path without degrading user experience?</p></li>
<li><p><strong>Error Rate Trade-offs</strong>: What‚Äôs the cost-benefit balance between false positives (blocking good outputs and frustrating users) versus false negatives (letting bad outputs reach users and causing harm)? In high-stakes domains like medical advice, false negatives may be more costly than false positives. In creative applications, false positives that block legitimate creativity may be more harmful than occasional quality issues.</p></li>
</ol>
<p>Most guardrails are designed to be <strong>fast</strong> (to avoid harming user experience) and have a <strong>very low false positive rate</strong> (to avoid blocking valid responses). For this reason, you would almost never use a slow or non-deterministic LLM-as-Judge as a synchronous guardrail. However, these tradeoffs might be different for your use case.</p>
<p><a href="../../../blog/posts/evals-faq/can-my-evaluators-also-be-used-to-automatically-fix-or-correct-outputs-in-production.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-much-time-should-i-spend-on-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="q-how-much-time-should-i-spend-on-model-selection">Q: How much time should I spend on model selection?</h2>
<p>Many developers fixate on model selection as the primary way to improve their LLM applications. Start with error analysis to understand your failure modes before considering model switching. As Hamel noted in office hours, ‚ÄúI suggest not thinking of switching model as the main axes of how to improve your system off the bat without evidence. Does error analysis suggest that your model is the problem?‚Äù</p>
<p><a href="../../../blog/posts/evals-faq/how-much-time-should-i-spend-on-model-selection.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="domain-specific-applications" class="level1">
<h1>Domain-Specific Applications</h1>
<section id="q-is-rag-dead" class="level2">
<h2 class="anchored" data-anchor-id="q-is-rag-dead">Q: Is RAG dead?</h2>
<p>Question: Should I avoid using RAG for my AI application after reading that <a href="https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for" target="_blank">‚ÄúRAG is dead‚Äù</a> for coding agents?</p>
<blockquote class="blockquote">
<p>Many developers are confused about when and how to use RAG after reading articles claiming ‚ÄúRAG is dead.‚Äù Understanding what RAG actually means versus the narrow marketing definitions will help you make better architectural decisions for your AI applications.</p>
</blockquote>
<p>The viral article claiming RAG is dead specifically argues against using <em>naive vector database retrieval</em> for autonomous coding agents, not RAG as a whole. This is a crucial distinction that many developers miss due to misleading marketing.</p>
<p>RAG simply means Retrieval-Augmented Generation - using retrieval to provide relevant context that improves your model‚Äôs output. The core principle remains essential: your LLM needs the right context to generate accurate answers. The question isn‚Äôt whether to use retrieval, but how to retrieve effectively.</p>
<p>For coding applications, naive vector similarity search often fails because code relationships are complex and contextual. Instead of abandoning retrieval entirely, modern coding assistants like Claude Code <a href="https://x.com/pashmerepat/status/1926717705660375463?s=46" target="_blank">still uses retrieval</a> ‚Äîthey just employ agentic search instead of relying solely on vector databases, similar to how human developers work.</p>
<p>You have multiple retrieval strategies available, ranging from simple keyword matching to embedding similarity to LLM-powered relevance filtering. The optimal approach depends on your specific use case, data characteristics, and performance requirements. Many production systems combine multiple strategies or use multi-hop retrieval guided by LLM agents.</p>
<p>Unfortunately, ‚ÄúRAG‚Äù has become a buzzword with no shared definition. Some people use it to mean any retrieval system, others restrict it to vector databases. Focus on the ultimate goal: getting your LLM the context it needs to succeed. Whether that‚Äôs through vector search, agentic exploration, or hybrid approaches is a product and engineering decision.</p>
<p>Rather than following categorical advice to avoid or embrace RAG, experiment with different retrieval approaches and measure what works best for your application. For more info on RAG evaluation and optimization, see <a href="../../../notes/llm/rag/not_dead.html">this series of posts</a>.</p>
<p><a href="../../../blog/posts/evals-faq/is-rag-dead.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-should-i-approach-evaluating-my-rag-system" class="level2">
<h2 class="anchored" data-anchor-id="q-how-should-i-approach-evaluating-my-rag-system">Q: How should I approach evaluating my RAG system?</h2>
<p>RAG systems have two distinct components that require different evaluation approaches: retrieval and generation.</p>
<p>The retrieval component is a search problem. Evaluate it using traditional information retrieval (IR) metrics. Common examples include Recall@k (of all relevant documents, how many did you retrieve in the top k?), Precision@k (of the k documents retrieved, how many were relevant?), or MRR (how high up was the first relevant document?). The specific metrics you choose depend on your use case. These metrics are pure search metrics that measure whether you‚Äôre finding the right documents (more on this below).</p>
<p>To evaluate retrieval, create a dataset of queries paired with their relevant documents. Generate this synthetically by taking documents from your corpus, extracting key facts, then generating questions those facts would answer. This reverse process gives you query-document pairs for measuring retrieval performance without manual annotation.</p>
<p>For the generation component‚Äîhow well the LLM uses retrieved context, whether it hallucinates, whether it answers the question‚Äîuse the same evaluation procedures covered throughout this course: error analysis to identify failure modes, collecting human labels, building LLM-as-judge evaluators, and validating those judges against human annotations.</p>
<p>Jason Liu‚Äôs <a href="https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/" target="_blank">‚ÄúThere Are Only 6 RAG Evals‚Äù</a> provides a framework that maps well to this separation. His Tier 1 covers traditional IR metrics for retrieval. Tiers 2 and 3 evaluate relationships between Question, Context, and Answer‚Äîlike whether the context is relevant (C|Q), whether the answer is faithful to context (A|C), and whether the answer addresses the question (A|Q).</p>
<p>In addition to Jason‚Äôs six evals, error analysis on your specific data may reveal domain-specific failure modes that warrant their own metrics. For example, a medical RAG system might consistently fail to distinguish between drug dosages for adults versus children, or a legal RAG might confuse jurisdictional boundaries. These patterns emerge only through systematic review of actual failures. Once identified, you can create targeted evaluators for these specific issues beyond the general framework.</p>
<p>Finally, when implementing Jason‚Äôs Tier 2 and 3 metrics, don‚Äôt just use prompts off the shelf. The standard LLM-as-judge process requires several steps: error analysis, prompt iteration, creating labeled examples, and measuring your judge‚Äôs accuracy against human labels. Once you know your judge‚Äôs True Positive and True Negative rates, you can correct its estimates to determine the actual failure rate in your system. Skip this validation and your judges may not reflect your actual quality criteria.</p>
<p>In summary, debug retrieval first using IR metrics, then tackle generation quality using properly validated LLM judges.</p>
<p><a href="../../../blog/posts/evals-faq/how-should-i-approach-evaluating-my-rag-system.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-do-i-choose-the-right-chunk-size-for-my-document-processing-tasks" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-choose-the-right-chunk-size-for-my-document-processing-tasks">Q: How do I choose the right chunk size for my document processing tasks?</h2>
<p>Unlike RAG, where chunks are optimized for retrieval, document processing assumes the model will see every chunk. The goal is to split text so the model can reason effectively without being overwhelmed. Even if a document fits within the context window, it might be better to break it up. Long inputs can degrade performance due to attention bottlenecks, especially in the middle of the context. Two task types require different strategies:</p>
<section id="fixed-output-tasks-large-chunks" class="level3">
<h3 class="anchored" data-anchor-id="fixed-output-tasks-large-chunks">1. Fixed-Output Tasks ‚Üí Large Chunks</h3>
<p>These are tasks where the output length doesn‚Äôt grow with input: extracting a number, answering a specific question, classifying a section. For example:</p>
<ul>
<li>‚ÄúWhat‚Äôs the penalty clause in this contract?‚Äù</li>
<li>‚ÄúWhat was the CEO‚Äôs salary in 2023?‚Äù</li>
</ul>
<p>Use the largest chunk (with caveats) that likely contains the answer. This reduces the number of queries and avoids context fragmentation. However, avoid adding irrelevant text. Models are sensitive to distraction, especially with large inputs. The middle parts of a long input might be under-attended. Furthermore, if cost and latency are a bottleneck, you should consider preprocessing or filtering the document (via keyword search or a lightweight retriever) to isolate relevant sections before feeding a huge chunk.</p>
</section>
<section id="expansive-output-tasks-smaller-chunks" class="level3">
<h3 class="anchored" data-anchor-id="expansive-output-tasks-smaller-chunks">2. Expansive-Output Tasks ‚Üí Smaller Chunks</h3>
<p>These include summarization, exhaustive extraction, or any task where output grows with input. For example:</p>
<ul>
<li>‚ÄúSummarize each section‚Äù</li>
<li>‚ÄúList all customer complaints‚Äù</li>
</ul>
<p>In these cases, smaller chunks help preserve reasoning quality and output completeness. The standard approach is to process each chunk independently, then aggregate results (e.g., map-reduce). When sizing your chunks, try to respect content boundaries like paragraphs, sections, or chapters. Chunking also helps mitigate output limits. By breaking the task into pieces, each piece‚Äôs output can stay within limits.</p>
</section>
<section id="general-guidance" class="level3">
<h3 class="anchored" data-anchor-id="general-guidance">General Guidance</h3>
<p>It‚Äôs important to recognize <strong>why chunk size affects results</strong>. A larger chunk means the model has to reason over more information in one go ‚Äì essentially, a heavier cognitive load. LLMs have limited capacity to <strong>retain and correlate details across a long text</strong>. If too much is packed in, the model might prioritize certain parts (commonly the beginning or end) and overlook or ‚Äúforget‚Äù details in the middle. This can lead to overly coarse summaries or missed facts. In contrast, a smaller chunk bounds the problem: the model can pay full attention to that section. You are trading off <strong>global context for local focus</strong>.</p>
<p>No rule of thumb can perfectly determine the best chunk size for your use case ‚Äì <strong>you should validate with experiments</strong>. The optimal chunk size can vary by domain and model. I treat chunk size as a hyperparameter to tune.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-choose-the-right-chunk-size-for-my-document-processing-tasks.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-how-do-i-debug-multi-turn-conversation-traces" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-debug-multi-turn-conversation-traces">Q: How do I debug multi-turn conversation traces?</h2>
<p>Start simple. Check if the whole conversation met the user‚Äôs goal with a pass/fail judgment. Look at the entire trace and focus on the first upstream failure. Read the user-visible parts first to understand if something went wrong. Only then dig into the technical details like tool calls and intermediate steps.</p>
<section id="multi-agent-trace-logging" class="level3">
<h3 class="anchored" data-anchor-id="multi-agent-trace-logging">Multi-agent trace logging</h3>
<p>For multi-agent flows, assign a session or trace ID to each user request and log every message with its source (which agent or tool), trace ID, and position in the sequence. This lets you reconstruct the full path from initial query to final result across all agents.</p>
</section>
<section id="annotation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="annotation-strategy">Annotation strategy</h3>
<p>Annotate only the first failure in the trace initially‚Äîdon‚Äôt worry about downstream failures since these often cascade from the first issue. Fixing upstream failures often resolves dependent downstream failures automatically. As you gain experience, you can annotate independent failure modes within the same trace to speed up overall error analysis.</p>
</section>
<section id="simplify-when-possible" class="level3">
<h3 class="anchored" data-anchor-id="simplify-when-possible">Simplify when possible</h3>
<p>When you find a failure, reproduce it with the simplest possible test case. Here‚Äôs an example: suppose a shopping bot gives the wrong return policy on turn 4 of a conversation. Before diving into the full multi-turn complexity, simplify it to a single turn: ‚ÄúWhat is the return window for product X1000?‚Äù If it still fails, you‚Äôve proven the error isn‚Äôt about conversation context - it‚Äôs likely a basic retrieval or knowledge issue you can debug more easily.</p>
</section>
<section id="test-case-generation" class="level3">
<h3 class="anchored" data-anchor-id="test-case-generation">Test case generation</h3>
<p>You have two main approaches. First, simulate users with another LLM to create realistic multi-turn conversations. Second, use ‚ÄúN-1 testing‚Äù where you provide the first N-1 turns of a real conversation and test what happens next. The N-1 approach often works better since it uses actual conversation prefixes rather than fully synthetic interactions, but is less flexible.</p>
<p>The key is balancing thoroughness with efficiency. Not every multi-turn failure requires multi-turn analysis.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-debug-multi-turn-conversation-traces.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
</section>
<section id="q-how-do-i-evaluate-sessions-with-human-handoffs" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-evaluate-sessions-with-human-handoffs">Q: How do I evaluate sessions with human handoffs?</h2>
<p>Capture the complete user journey in your traces, including human handoffs. The trace continues until the user‚Äôs need is resolved or the session ends, not when AI hands off to a human. Log the handoff decision, why it occurred, context transferred, wait time, human actions, final resolution, and whether the human had sufficient context. Many failures occur at handoff boundaries where AI hands off too early, too late, or without proper context.</p>
<p>Evaluate handoffs as potential failure modes during error analysis. Ask: Was the handoff necessary? Did the AI provide adequate context? Track both handoff quality and handoff rate. Sometimes the best improvement reduces handoffs entirely rather than improving handoff execution.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-evaluate-sessions-with-human-handoffs.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-do-i-evaluate-complex-multi-step-workflows" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-evaluate-complex-multi-step-workflows">Q: How do I evaluate complex multi-step workflows?</h2>
<p>Log the entire workflow from initial trigger to final business outcome. Include LLM calls, tool usage, human approvals, and database writes in your traces. You will need this visibility to properly diagnose failures.</p>
<p>Use both outcome and process metrics. Outcome metrics verify the final result meets requirements: Was the business case complete? Accurate? Properly formatted? Process metrics evaluate efficiency: step count, time taken, resource usage. Process failures are often easier to debug since they‚Äôre more deterministic, so tackle them first.</p>
<p>Segment your error analysis by workflow stages. Early stage failures (understanding user input) differ from middle stage failures (data processing) and late stage failures (formatting output). Early stage improvements have more impact since errors cascade in LLM chains.</p>
<p>Use transition failure matrices to analyze where workflows break. Create a matrix showing the last successful state versus where the first failure occurred. This reveals failure hotspots and guides where to invest debugging effort.</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-evaluate-complex-multi-step-workflows.html" class="faq-individual-link">‚Üó Focus view</a></p>
</section>
<section id="q-how-do-i-evaluate-agentic-workflows" class="level2">
<h2 class="anchored" data-anchor-id="q-how-do-i-evaluate-agentic-workflows">Q: How do I evaluate agentic workflows?</h2>
<p>We recommend evaluating agentic workflows in two phases:</p>
<p><strong>1. End-to-end task success.</strong> Treat the agent as a black box and ask ‚Äúdid we meet the user‚Äôs goal?‚Äù. Define a precise success rule per task (exact answer, correct side-effect, etc.) and measure with human or <a href="https://hamel.dev/blog/posts/llm-judge/" target="_blank">aligned LLM judges</a>. Take note of the first upstream failure when conducting error analysis.</p>
<p>Once error analysis reveals which workflows fail most often, move to step-level diagnostics to understand why they‚Äôre failing.</p>
<p><strong>2. Step-level diagnostics.</strong> Assuming that you have sufficiently <a href="https://hamel.dev/blog/posts/evals/#logging-traces" target="_blank">instrumented your system</a> with details of tool calls and responses, you can score individual components such as: - <em>Tool choice</em>: was the selected tool appropriate? - <em>Parameter extraction</em>: were inputs complete and well-formed? - <em>Error handling</em>: did the agent recover from empty results or API failures? - <em>Context retention</em>: did it preserve earlier constraints? - <em>Efficiency</em>: how many steps, seconds, and tokens were spent? - <em>Goal checkpoints</em>: for long workflows verify key milestones.</p>
<p>Example: ‚ÄúFind Berkeley homes under $1M and schedule viewings‚Äù breaks into: parameters extracted correctly, relevant listings retrieved, availability checked, and calendar invites sent. Each checkpoint can pass or fail independently, making debugging tractable.</p>
<p><strong>Use transition failure matrices to understand error patterns.</strong> Create a matrix where rows represent the last successful state and columns represent where the first failure occurred. This is a great way to understand where the most failures occur.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/images/shreya_matrix.png" class="img-fluid figure-img" style="width:75.0%" target="_blank"></p>
<figcaption>Transition failure matrix showing hotspots in text-to-SQL agent workflow</figcaption>
</figure>
</div>
<p>Transition matrices transform overwhelming agent complexity into actionable insights. Instead of drowning in individual trace reviews, you can immediately see that GenSQL ‚Üí ExecSQL transitions cause 12 failures while DecideTool ‚Üí PlanCal causes only 2. This data-driven approach guides where to invest debugging effort. Here is another <a href="https://www.figma.com/deck/nwRlh5renu4s4olaCsf9lG/Failure-is-a-Funnel?node-id=2009-927&amp;t=GJlTtxQ8bLJaQ92A-1" target="_blank">example</a> from Bryan Bischof, that is also a text-to-SQL agent:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hamel.dev/blog/posts/evals-faq/images/bischof_matrix.png" class="img-fluid figure-img" style="width:75.0%" target="_blank"></p>
<figcaption>Bischof, Bryan ‚ÄúFailure is A Funnel - Data Council, 2025‚Äù</figcaption>
</figure>
</div>
<p>In this example, Bryan shows variation in transition matrices across experiments. How you organize your transition matrix depends on the specifics of your application. For example, Bryan‚Äôs text-to-SQL agent has an inherent sequential workflow which he exploits for further analytical insight. You can watch his <a href="https://youtu.be/R_HnI9oTv3c?si=hRRhDiydHU5k6ikc" target="_blank">full talk</a> for more details.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/R_HnI9oTv3c" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><strong>Creating Test Cases for Agent Failures</strong></p>
<p>Creating test cases for agent failures follows the same principles as our previous FAQ on debugging multi-turn conversation traces (i.e.&nbsp;try to reproduce the error in the simplest way possible, only use multi-turn tests when the failure actually requires conversation context, etc.).</p>
<p><a href="../../../blog/posts/evals-faq/how-do-i-evaluate-agentic-workflows.html" class="faq-individual-link">‚Üó Focus view</a></p>
<hr>
<div class="cta" style="text-align: center;">
<p><strong>üëâ <em>Want to learn more about AI Evals? Check out our <a href="https://bit.ly/evals-ai" target="_blank">AI Evals course</a></em></strong>. It‚Äôs a live cohort with hands on exercises and office hours. Here is a <a href="https://bit.ly/evals-ai" target="_blank">15% discount code</a> for readers. üëà</p>
</div>
<hr>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://www.linkedin.com/in/intellectronica/" target="_blank">Eleanor Berger</a>, our wonderful TA.‚Ü©Ô∏é</p></li>
<li id="fn2"><p>Paul Graham, <a href="https://paulgraham.com/writes.html" target="_blank">‚ÄúWrites and Write-Nots‚Äù</a>‚Ü©Ô∏é</p></li>
<li id="fn3"><p>Shreya Shankar, et al., <a href="https://arxiv.org/abs/2404.12272" target="_blank">‚ÄúWho Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences‚Äù</a>‚Ü©Ô∏é</p></li>
</ol>
</section></div> ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/</guid>
  <pubDate>Tue, 18 Nov 2025 23:15:30 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: What‚Äôs your favorite eval vendor?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/whats-your-favorite-eval-vendor.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Eval tools are in an intensely competitive space. It would be futile to compare their features. If I tried to do such an analysis, it would be invalidated in a week! Vendors I encounter the most organically in my work are: <a href="https://www.langchain.com/langsmith" target="_blank">Langsmith</a>, <a href="https://arize.com/" target="_blank">Arize</a> and <a href="https://www.braintrust.dev/" target="_blank">Braintrust</a>.</p>
<p>When I help clients with vendor selection, the decision weighs heavily towards who can offer the best support, as opposed to purely features. This changes depending on size of client, use case, etc. Yes - it‚Äôs mainly the human factor that matters, and dare I say, vibes.</p>
<p>I have no favorite vendor. At the core, their features are very similar - and I often build <a href="https://hamel.dev/blog/posts/evals/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf" target="_blank">custom tools</a> on top of them to fit my needs.</p>
<p>Here is a <a href="../../../blog/posts/eval-tools/">video series</a> that has a live commentary on the relative strengths and weaknesses of the three aforementioned vendors.</p>
<p><a href="../../../blog/posts/evals-faq/#q-seriously-hamel-stop-the-bullshit-whats-your-favorite-eval-vendor" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/whats-your-favorite-eval-vendor.html</guid>
  <pubDate>Tue, 18 Nov 2025 23:05:47 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How should I version and manage prompts?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-should-i-version-and-manage-prompts.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>There is an unavoidable tension between keeping prompts close to the code vs.&nbsp;an environment that non-technical stakeholders can access.</p>
<p><strong>My preferred approach is storing prompts in Git.</strong> This treats them as software artifacts that are versioned, reviewed, and deployed atomically with the application code. While the Git command line is unfriendly for non-technical folks, the <a href="https://github.com">GitHub</a> web interface and the GitHub <a href="https://desktop.github.com/">Desktop app</a> make it very approachable. When I was working at GitHub, I worked with many non-technical professionals, including lawyers and accountants, who used these tools effectively. Here is a <a href="https://ben.balter.com/2023/03/02/github-for-non-technical-roles/">blog post</a> aimed at non-technical folks to get started.</p>
<p>Alternatively, most vendors in the LLM tooling space, such as observability platforms like Arize, Braintrust, and LangSmith, offer dedicated prompt management tools. These are accessible for rapid iteration but risk creating additional layers of indirection.</p>
<p><strong>Why prompt management tools often fall short:</strong> AI products typically involve many moving parts: tools, RAG, agents, etc. Prompt management tools are inherently limiting because they can‚Äôt easily execute your application‚Äôs code. Even when they can, there‚Äôs often significant indirection involved, making it difficult to test prompts with your system‚Äôs capabilities.</p>
<p><strong>When possible, a notebook provides a great solution for prompt experimentation</strong> If you have Python entry points into your codebase or your codebase is written in Python, Jupyter notebooks are particularly powerful for this purpose. You can experiment with prompts and iterate on your actual AI agents with their full tool and RAG capabilities. This makes it much easier to understand how your system works in practice. Additionally, you can create widgets and small user interfaces within notebooks, giving you the best of both worlds for experimentation and iteration. To see what this looks like in practice, Teresa Torres gives a fantastic, hands-on walkthrough of how she, as a PM, used notebooks for the entire eval and experimentation lifecycle:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/N-qAOv_PNPc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>If notebooks are not feasible for your code base, an <a href="../../../blog/posts/field-guide/#build-bridges-not-gatekeepers">‚Äãintegrated prompt environment</a>‚Äã can be effective for experimentation. Either way, I prefer to version and manage prompts in Git.</p>
<p><a href="../../../blog/posts/evals-faq/#q-how-should-i-version-and-manage-prompts" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-should-i-version-and-manage-prompts.html</guid>
  <pubDate>Mon, 27 Oct 2025 16:59:33 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Can I use the same model for both the main task and evaluation?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/can-i-use-the-same-model-for-both-the-main-task-and-evaluation.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>For LLM-as-Judge selection, using the same model is usually fine because the judge is doing a different task than your main LLM pipeline. While <a href="https://arxiv.org/pdf/2508.06709">research has shown</a> that models can exhibit bias when evaluating their own outputs, what ultimately matters is how well your judge aligns with human judgments. The judges we recommend building do <a href="../../../blog/posts/evals-faq/why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales.html" target="_blank">scoped binary classification tasks</a>. We‚Äôve found that iterative alignment with human labels is usually achievable on this constrained task.</p>
<p>Focus on achieving high True Positive Rate (TPR) and True Negative Rate (TNR) with your judge on a held out labeled test set. If you struggle to achieve good alignment with human scores, then consider trying a different model. However onboarding new model providers may involve non-trivial effort in some organizations, which is why we don‚Äôt advocate for using different models by default unless there‚Äôs a specific alignment issue.</p>
<p>When selecting judge models, start with the most capable models available to establish strong alignment with human judgments. You can optimize for cost later once you‚Äôve established reliable evaluation criteria.</p>
<p><a href="../../../blog/posts/evals-faq/#q-can-i-use-the-same-model-for-both-the-main-task-and-evaluation" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/can-i-use-the-same-model-for-both-the-main-task-and-evaluation.html</guid>
  <pubDate>Mon, 27 Oct 2025 16:59:33 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How do we evaluate a model‚Äôs ability to express uncertainty or ‚Äúknow what it doesn‚Äôt know‚Äù?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-do-we-evaluate-a-models-ability-to-express-uncertainty-or-know-what-it-doesnt-know.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Many applications require a model that can refuse to answer a question when it lacks sufficient information. To evaluate whether this refusal behavior is well-calibrated, you need to test if the model refuses at the appropriate times without refusing to answer questions it <em>should</em> be able to answer.</p>
<p>To do this effectively, you should construct an evaluation set that has the following components:</p>
<ol type="1">
<li><strong>Answerable Questions:</strong> Scenarios where a correct, verifiable answer is present in the model‚Äôs provided context or general knowledge.</li>
<li><strong>Unanswerable Questions:</strong> Scenarios designed to tempt the model to hallucinate. These include questions with false premises, queries about information explicitly missing from context, or topics far outside its knowledge base.</li>
</ol>
<p>While the exact proportion isn‚Äôt critical, a balanced set with a roughly equal number of answerable and unanswerable questions is a good starting point. The diversity and difficulty of the questions are more important than the precise ratio.</p>
<p>The evaluation itself is a binary (Pass/Fail) check of the model‚Äôs judgment. A ‚ÄúPass‚Äù requires the model to satisfy two conditions: it must answer the answerable questions while also refusing to answer the unanswerable ones. A failure is defined as providing a fabricated answer to an unanswerable question, which indicates poor calibration.</p>
<p>In the research literature, this capability is known as ‚ÄúAbstention Ability.‚Äù To improve this behavior, it is worth <a href="https://arxiv.org/search/?query=Abstention+Ability&amp;searchtype=all">searching for this term on Arxiv</a> to understand the latest techniques.</p>
<p><a href="../../../blog/posts/evals-faq/#q-how-do-we-evaluate-a-models-ability-to-express-uncertainty-or-know-what-it-doesnt-know" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-do-we-evaluate-a-models-ability-to-express-uncertainty-or-know-what-it-doesnt-know.html</guid>
  <pubDate>Mon, 27 Oct 2025 16:59:33 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How do I make the case for investing in evaluations to my team?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-do-i-make-the-case-for-investing-in-evaluations-to-my-team.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Don‚Äôt try to sell your team on ‚Äúevals‚Äù. Instead, show them what you find when you look at the data.</p>
<p>Start by doing the <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html" target="_blank">error analysis</a> yourself. Look at 50 to 100 real user conversations and find the most common ways the product is failing. Use these findings to tell a story with data.</p>
<p>Present your team with:</p>
<ul>
<li>A list of the top failure modes you discovered.</li>
<li>Metrics showing how often high-impact errors are happening.</li>
<li>Surprising ways that users are interacting with the product.</li>
<li>Reports on the bugs you found and fixed, framed as ‚Äúprevented production issues‚Äù.</li>
</ul>
<p>This approach builds trust. Don‚Äôt just show dashboards and metrics; tell the story of what you‚Äôre finding in the data. By narrating your findings, you teach the team what you‚Äôre learning, providing immediate value. When you fix an issue, show how the error rate for that specific problem went down. Soon, your team will see the progress and ask how you‚Äôre doing it. Let results instead of methods lead the conversation.</p>
<p>This is similar to classic machine learning projects, where outcomes are speculative and progress is bounded by <a href="https://hamel.dev/blog/posts/field-guide/#your-ai-roadmap-should-count-experiments-not-features" target="_blank">iterating on experiments</a>. In this situation, it‚Äôs important that you share the learnings from each experiment to show progress and encourage investment.</p>
<p><a href="../../../blog/posts/evals-faq/#q-how-do-i-make-the-case-for-investing-in-evaluations-to-my-team" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-do-i-make-the-case-for-investing-in-evaluations-to-my-team.html</guid>
  <pubDate>Mon, 27 Oct 2025 16:59:33 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Selecting The Right AI Evals Tool</title>
  <dc:creator>Hamel Husain</dc:creator>
  <link>https://hamel.dev/blog/posts/eval-tools/</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->




<p>Over the past year, I‚Äôve focused heavily on <a href="../../../blog/posts/evals-faq/index.html">AI Evals</a>, both in my consulting work and teaching. A question I get constantly is, ‚ÄúWhat‚Äôs the best tool for evals?‚Äù. I‚Äôve always resisted answering directly for two reasons. First, people focus too much on tools instead of the process, thinking the tool will be an off-the-shelf solution when it rarely is. Second, the tools change so quickly that comparisons become outdated immediately.</p>
<p>Having used many of the popular eval tools, I can genuinely say that no single one is superior in every dimension. The ‚Äúbest‚Äù tool depends on your team‚Äôs skillset, technical stack, and maturity.</p>
<p>Instead of a feature-by-feature comparison, I think it‚Äôs more valuable to show you <em>how</em> a panel of data scientists skilled in evals assesses these tools. As part of my AI Evals <a href="https://evals.info" target="_blank">course</a>, we had three of the most dominant vendors‚ÄîLangsmith, Braintrust, and Arize Phoenix complete the same homework <a href="https://github.com/ai-evals-course/recipe-chatbot" target="_blank">assignment</a>. This gave us a unique opportunity to see how they tackle the exact same challenge.</p>
<p>We recorded the entire process and live commentary, which is available below. We think this might be helpful in learning about the kinds of things you should consider when selecting a tool for your team.</p>
<p><em>Thanks to <a href="https://www.sh-reya.com/">Shreya Shankar</a> and <a href="https://x.com/BEBischof">Bryan Bischof</a> for serving as the panelists (alongside me).</em></p>
<section id="langsmith" class="level2">
<h2 class="anchored" data-anchor-id="langsmith">Langsmith</h2>
<p>With <a href="https://x.com/hwchase17">Harrison Chase</a>, CEO of LangChain.</p>
<div style="width: 70%; margin: auto;">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/y0vm_fjkejo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</section>
<section id="braintrust" class="level2">
<h2 class="anchored" data-anchor-id="braintrust">Braintrust</h2>
<p>With <a href="https://x.com/waydegilliam">Wayde Gilliam</a>, former developer relations at Braintrust.</p>
<div style="width: 70%; margin: auto;">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/97iykOemOn4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</section>
<section id="arize-phoenix" class="level2">
<h2 class="anchored" data-anchor-id="arize-phoenix">Arize Phoenix</h2>
<p>With <a href="https://www.linkedin.com/in/sallyann-delucia-59a381172/">SallyAnn DeLucia</a>, Technical AI Product Leader at Arize.</p>
<div style="width: 70%; margin: auto;">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/wcYnzHJlUR0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<hr>
</section>
<section id="criteria-for-assessing-ai-evals-tools" class="level2">
<h2 class="anchored" data-anchor-id="criteria-for-assessing-ai-evals-tools">Criteria for Assessing AI Evals Tools</h2>
<p>Here are themes that consistently surfaced during our review.</p>
<section id="workflow-and-developer-experience" class="level3">
<h3 class="anchored" data-anchor-id="workflow-and-developer-experience">1. Workflow and Developer Experience</h3>
<p>Reducing friction is more important than any single feature. Concretely, you should be mindful of the time it takes to go from observing a failure to iterating on a solution. For example, we appreciated the ability to go from viewing a single trace to experimenting with that same trace in a playground. For some teams with data-science backgrounds, a notebook-centric workflow is ideal as it provides transparency and control. This happens to be my preferred workflow as well.</p>
<p>When considering a notebook-centric workflow, its important to pay attention to the ergonmics of the sdk. This often boils down to the quality of the documentation and integration with existing data tools.</p>
</section>
<section id="human-in-the-loop-support" class="level3">
<h3 class="anchored" data-anchor-id="human-in-the-loop-support">2. Human-in-the-Loop Support</h3>
<p>The best tools don‚Äôt try to automate away the human; they empower them. Since error analysis is the highest ROI activity in AI engineering, a tool‚Äôs ability to support efficient human review is paramount. Prioritize tools with first-class support for manual annotation and error analysis. As of this writing, one thing that is missing from many tools is <a href="../../../blog/posts/evals-faq/index.html#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed" target="_blank">axial coding</a>.</p>
</section>
<section id="transparency-and-control-vs.-magic" class="level3">
<h3 class="anchored" data-anchor-id="transparency-and-control-vs.-magic">3. Transparency and Control vs.&nbsp;‚ÄúMagic‚Äù</h3>
<p>Be deeply skeptical of features that promise full automation without human validation, as these can create a powerful and dangerous illusion of confidence. For example, be wary of features where an AI agent both creates an evaluation rubric and then immediately scores the outputs. This ‚Äústacking of abstractions‚Äù often hides flaws behind a high score. Favor tools that give you control and visibility.</p>
</section>
<section id="ecosystem-integration-vs.-walled-gardens" class="level3">
<h3 class="anchored" data-anchor-id="ecosystem-integration-vs.-walled-gardens">4. Ecosystem Integration vs.&nbsp;Walled Gardens</h3>
<p>An eval tool should fit your stack, not force you to fit its stack. Assess how well a tool integrates with your existing technologies. Also, beware of proprietary DSLs as they can add friction. Finally, the ability to export data into common formats for analysis in a variety of environments is a must-have.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>The right choice of tool depends on your team‚Äôs workflow, skillset, and specific needs. I hope seeing how our panel approached this evaluation provides a better framework for making your own decision.</p>
<p>As for me personally, I tend to use these tools as a backend data store and use Jupyter notebooks as well as my own <a href="../../../blog/posts/evals-faq/index.html#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf" target="_blank">custom built</a> annotation interfaces for most of my needs.</p>
<hr>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Caution</span>Appendix: Vendor Snapshots (As of Mid-2025)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p><strong>You should take these notes with a grain of salt. I recommend watch the videos above to get a sense of how we applied these criteria and where you might differ according to your neeeds.</strong></p>
<section id="langsmith-evaluation-notes" class="level3">
<h3 class="anchored" data-anchor-id="langsmith-evaluation-notes">Langsmith Evaluation Notes</h3>
<p><strong>Overall Sentiment</strong> The overall workflow is intuitive, especially for those new to formal evaluation processes. The UI guides you through creating datasets, running experiments, and annotating results.</p>
<p><strong>Positive Feedback / What We Liked</strong></p>
<ul>
<li><strong>Seamless Workflow from Trace to Playground:</strong> The transition from inspecting a trace to experimenting with it in the playground is very smooth.</li>
<li><strong>AI-Assisted Prompt Improvement:</strong> The ‚ÄúPrompt Canvas‚Äù feature is a powerful tool for prompt engineering.</li>
<li><strong>Dataset Creation and Management:</strong> You can easily create datasets by uploading files, and the schema detection helps structure the data correctly.</li>
<li><strong>Experimentation and Evaluation:</strong> The ‚ÄúAnnotation Queue‚Äù is a dedicated interface for human review and labeling of traces, which is more efficient than using spreadsheets.</li>
</ul>
<p><strong>Critiques and Areas for Improvement</strong></p>
<ul>
<li><strong>Limited Side-by-Side Comparison:</strong> The UI doesn‚Äôt make it easy to see side-by-side comparisons of different prompt versions and their outputs.</li>
<li><strong>UI/UX Concerns:</strong> The UI can feel a bit cluttered, with a lot of options and information presented at once.</li>
<li><strong>Potential for Over-Automation:</strong> Features like AI-generated examples, while convenient, can lead to homogenous data.</li>
</ul>
</section>
<section id="braintrust-evaluation-notes" class="level3">
<h3 class="anchored" data-anchor-id="braintrust-evaluation-notes">Braintrust Evaluation Notes</h3>
<p><strong>Overall Sentiment</strong> The panel had a generally positive view of Braintrust, highlighting its clean UI and structured approach to evaluations. The tool‚Äôs emphasis on human-in-the-loop workflows was a significant strength.</p>
<p><strong>Positive Feedback / What We Liked</strong></p>
<ul>
<li><strong>Focus on a Structured Evals Process:</strong> The demonstration emphasized a solid, methodical approach, starting by involving subject-matter experts to create an initial dataset.</li>
<li><strong>Clean and Intuitive User Interface (UI):</strong> The panel found the UI to be clean and easier to navigate than other tools, with a particularly readable trace viewing screen.</li>
<li><strong>Strong Support for Human-in-the-Loop Workflows:</strong> The platform has dedicated UIs designed for human review and annotation, which is critical for creating high-quality datasets and performing error analysis.</li>
<li><strong>The ‚ÄúMoney Table‚Äù:</strong> After annotating traces with failure modes, the final dataset view is an actionable output that allows teams to quickly sort, filter, and quantify the most common failure modes.</li>
</ul>
<p><strong>Critiques and Areas for Improvement</strong></p>
<ul>
<li><strong>The ‚ÄúLoop‚Äù AI Scorer:</strong> The most significant concern was the ‚ÄúLoop‚Äù feature, an AI agent that creates an evaluation rubric and then immediately scores the outputs, which could lead to a false sense of security.</li>
<li><strong>Reliance on a Proprietary Query Language (BTQL):</strong> The panel viewed the use of ‚ÄúBTQL‚Äù with mild skepticism, stating a preference for exporting data to a Jupyter notebook.</li>
<li><strong>Clunky Data Workflows:</strong> The process for generating and refining synthetic data seemed inefficient, requiring downloading and re-uploading data between steps.</li>
</ul>
</section>
<section id="arize-phoenix-evaluation-notes" class="level3">
<h3 class="anchored" data-anchor-id="arize-phoenix-evaluation-notes">Arize Phoenix Evaluation Notes</h3>
<p><strong>Overall Sentiment</strong> The panel had a generally positive view of Phoenix, with one panelist calling it one of his ‚Äúfavorite open source eval tools.‚Äù The tool is positioned as a developer-first, notebook-centric platform.</p>
<p><strong>Positive Feedback / What We Liked</strong></p>
<ul>
<li><strong>Notebook-Centric Workflow:</strong> The entire evaluation process was driven from a Jupyter notebook, giving the developer transparency and control. The ability to export annotated data back into a Pandas DataFrame was a powerful feature.</li>
<li><strong>UI &amp; Developer Experience:</strong> The prompt management UI was praised for being clear and easy to understand. The tight integration between traces and the ‚ÄúPlayground‚Äù was also noted as a smooth workflow.</li>
<li><strong>Open Source &amp; Local-First Approach:</strong> Phoenix can be run entirely locally, providing a sense of control and transparency. As an open-source tool, it was noted for being ‚Äúhackable.‚Äù</li>
</ul>
<p><strong>Critiques and Areas for Improvement</strong></p>
<ul>
<li><strong>UI Readability:</strong> The text in the output panes was difficult to read during the demonstration, with a possible lack of markdown rendering for model outputs.</li>
<li><strong>Metrics and Visualization:</strong> The tool displays point statistics for each run, but the panel found this of limited use and expressed a desire for aggregate visualizations like histograms to identify outliers.</li>
<li><strong>Prompt Management and Testing:</strong> The prompt editor treats the system prompt as one large, monolithic block of text. A more component-based approach where individual instructions could be toggled on and off (‚Äúablated‚Äù) would be preferable for systematic testing.</li>
</ul>
</section>
</div>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>Evals</category>
  <guid>https://hamel.dev/blog/posts/eval-tools/</guid>
  <pubDate>Wed, 01 Oct 2025 07:00:00 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/eval-tools/cover-img-2.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Should product managers and engineers collaborate on error analysis? How?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/should-product-managers-and-engineers-collaborate-on-error-analysis-how.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>At the outset, collaborate to establish shared context. Engineers catch technical issues like retrieval issues and tool errors. PMs identify product failures like unmet user expectations, confusing responses, or missing features users expect.</p>
<p>As time goes on you should lean towards a <a href="../../../blog/posts/evals-faq/how-many-people-should-annotate-my-llm-outputs.html">benevolent dictator</a> for error analysis: a domain expert or PM who understands user needs. Empower domain experts to evaluate actual outcomes rather than technical implementation. Ask ‚ÄúHas an appointment been made?‚Äù not ‚ÄúDid the tool call succeed?‚Äù The best way to empower the domain expert is to give them <a href="../../../blog/posts/evals-faq/what-makes-a-good-custom-interface-for-reviewing-llm-outputs.html">custom annotation tools</a> that display system outcomes alongside traces. Show the confirmation, generated email, or database update that validates goal completion. Keep all context on one screen so non-technical reviewers focus on results.</p>
<p><a href="../../../blog/posts/evals-faq/#q-should-product-managers-and-engineers-collaborate-on-error-analysis-how" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/should-product-managers-and-engineers-collaborate-on-error-analysis-how.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Should I build automated evaluators for every failure mode I find?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/should-i-build-automated-evaluators-for-every-failure-mode-i-find.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Focus automated evaluators on failures that persist after fixing your prompts. Many teams discover their LLM doesn‚Äôt meet preferences they never actually specified - like wanting short responses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before building complex evaluation infrastructure.</p>
<p>Consider the cost hierarchy of different evaluator types. Simple assertions and reference-based checks (comparing against known correct answers) are cheap to build and maintain. LLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and coordination between developers, PMs, and domain experts. This cost difference should shape your evaluation strategy.</p>
<p>Only build expensive evaluators for problems you‚Äôll iterate on repeatedly. Since LLM-as-Judge comes with significant overhead, save it for persistent generalization failures - not issues you can fix trivially. Start with cheap code-based checks where possible: regex patterns, structural validation, or execution tests. Reserve complex evaluation for subjective qualities that can‚Äôt be captured by simple rules.</p>
<p><a href="../../../blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/should-i-build-automated-evaluators-for-every-failure-mode-i-find.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Why is ‚Äúerror analysis‚Äù so important in LLM evals, and how is it performed?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Error analysis is <strong>the most important activity in evals</strong>. Error analysis helps you decide what evals to write in the first place. It allows you to identify failure modes unique to your application and data. The process involves:</p>
<section id="creating-a-dataset" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-dataset">1. Creating a Dataset</h3>
<p>Gathering representative traces of user interactions with the LLM. If you do not have any data, you can <a href="../../../blog/posts/evals-faq/what-is-the-best-approach-for-generating-synthetic-data.html" target="_blank">generate synthetic data</a> to get started.</p>
</section>
<section id="open-coding" class="level3">
<h3 class="anchored" data-anchor-id="open-coding">2. Open Coding</h3>
<p>Human annotator(s) (ideally a <a href="../../../blog/posts/evals-faq/how-many-people-should-annotate-my-llm-outputs.html" target="_blank">benevolent dictator</a>) review and write open-ended notes about traces, noting any issues. This process is akin to ‚Äújournaling‚Äù and is adapted from qualitative research methodologies. When beginning, it is recommended to focus on noting the <a href="../../../blog/posts/evals-faq/how-do-i-debug-multi-turn-conversation-traces.html" target="_blank">first failure</a> observed in a trace, as upstream errors can cause downstream issues, though you can also tag all independent failures if feasible. A <a href="https://hamel.dev/blog/posts/llm-judge/#step-1-find-the-principal-domain-expert" target="_blank">domain expert</a> should be performing this step.</p>
</section>
<section id="axial-coding" class="level3">
<h3 class="anchored" data-anchor-id="axial-coding">3. Axial Coding</h3>
<p>Categorize the open-ended notes into a ‚Äúfailure taxonomy.‚Äù. In other words, group similar failures into distinct categories. This is the most important step. At the end, count the number of failures in each category. You can use a LLM to help with this step.</p>
</section>
<section id="iterative-refinement" class="level3">
<h3 class="anchored" data-anchor-id="iterative-refinement">4. Iterative Refinement</h3>
<p>Keep iterating on more traces until you reach <a href="https://delvetool.com/blog/theoreticalsaturation" target="_blank">theoretical saturation</a>, meaning new traces do not seem to reveal new failure modes or information to you. As a rule of thumb, you should aim to review at least 100 traces.</p>
<p>You should frequently revisit this process. There are advanced ways to <a href="how-can-i-efficiently-sample-production-traces-for-review.html" target="_blank">sample data more efficiently</a>, like clustering, sorting by user feedback, and sorting by high probability failure patterns. Over time, you‚Äôll develop a ‚Äúnose‚Äù for where to look for failures in your data.</p>
<p>Do not skip error analysis. It ensures that the evaluation metrics you develop are supported by real application behaviors instead of counter-productive generic metrics (which most platforms nudge you to use). For examples of how error analysis can be helpful, see <a href="https://www.youtube.com/watch?v=e2i6JbU2R-s" target="_blank">this video</a>, or this <a href="https://hamel.dev/blog/posts/field-guide/" target="_blank">blog post</a>.</p>
<p>Here is a visualization of the error analysis process by one of our students, <a href="https://www.linkedin.com/in/pawel-huryn/" target="_blank">Pawel Huryn</a> - including how it fits into the overall evaluation process:</p>
<p><img src="https://hamel.dev/blog/posts/evals-faq/pawel-error-analysis.png" class="img-fluid"></p>
<p><a href="../../../blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>


</section>

 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: What‚Äôs a minimum viable evaluation setup?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/whats-a-minimum-viable-evaluation-setup.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Start with <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html" target="_blank">error analysis</a>, not infrastructure. Spend 30 minutes manually reviewing 20-50 LLM outputs whenever you make significant changes. Use one <a href="../../../blog/posts/evals-faq/how-many-people-should-annotate-my-llm-outputs.html" target="_blank">domain expert</a> who understands your users as your quality decision maker (a ‚Äú<a href="../../../blog/posts/evals-faq/how-many-people-should-annotate-my-llm-outputs.html" target="_blank">benevolent dictator</a>‚Äù).</p>
<p>If possible, <strong>use notebooks</strong> to help you review traces and analyze data. In our opinion, this is the single most effective tool for evals because you can write arbitrary code, visualize data, and iterate quickly. You can even build your own <a href="../../../blog/posts/evals-faq/what-makes-a-good-custom-interface-for-reviewing-llm-outputs.html" target="_blank">custom annotation interface</a> right inside notebooks, as shown in this <a href="https://youtu.be/aqKUwPKBkB0?si=5KDmMQnRzO_Ce9xH" target="_blank">video</a>.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/aqKUwPKBkB0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><a href="../../../blog/posts/evals-faq/#q-whats-a-minimum-viable-evaluation-setup" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/whats-a-minimum-viable-evaluation-setup.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How do I approach evaluation when my system handles diverse user queries?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<blockquote class="blockquote">
<p>Complex applications often support vastly different query patterns‚Äîfrom ‚ÄúWhat‚Äôs the return policy?‚Äù to ‚ÄúCompare pricing trends across regions for products matching these criteria.‚Äù Each query type exercises different system capabilities, leading to confusion on how to design eval criteria.</p>
</blockquote>
<p><strong><em><a href="https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc" target="_blank">Error Analysis</a> is all you need.</em></strong> Your evaluation strategy should emerge from observed failure patterns (e.g.&nbsp;error analysis), not predetermined query classifications. Rather than creating a massive evaluation matrix covering every query type you can imagine, let your system‚Äôs actual behavior guide where you invest evaluation effort.</p>
<p>During error analysis, you‚Äôll likely discover that certain query categories share failure patterns. For instance, all queries requiring temporal reasoning might struggle regardless of whether they‚Äôre simple lookups or complex aggregations. Similarly, queries that need to combine information from multiple sources might fail in consistent ways. These patterns discovered through error analysis should drive your evaluation priorities. It could be that query category is a fine way to group failures, but you don‚Äôt know that until you‚Äôve analyzed your data.</p>
<p>To see an example of basic error analysis in action, <a href="https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc" target="_blank">see this video</a>.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/e2i6JbU2R-s" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><a href="../../../blog/posts/evals-faq/#q-how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: What are LLM Evals?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/what-are-llm-evals.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>If you are completely new to product-specific LLM evals (not foundation model benchmarks), see these posts: <a href="../../../blog/posts/evals/index.html" target="_blank">part 1</a>, <a href="../../../blog/posts/llm-judge/index.html" target="_blank">part 2</a> and <a href="../../../blog/posts/field-guide/index.html" target="_blank">part 3</a>. Otherwise, keep reading.</p>
<div class="grid">
<div class="g-col-4">
<p><a href="https://hamel.dev/evals" target="_blank"><img src="https://hamel.dev/blog/posts/evals/images/diagram-cover.png" class="img-fluid"></a></p>
<p><a href="https://hamel.dev/evals" target="_blank"><strong>Your AI Product Needs Eval (Evaluation Systems)</strong></a></p>
<p><strong>Contents:</strong></p>
<ol type="1">
<li>Motivation</li>
<li>Iterating Quickly == Success<br>
</li>
<li>Case Study: Lucy, A Real Estate AI Assistant</li>
<li>The Types Of Evaluation
<ol type="a">
<li>Level 1: Unit Tests</li>
<li>Level 2: Human &amp; Model Eval</li>
<li>Level 3: A/B Testing</li>
<li>Evaluating RAG</li>
</ol></li>
<li>Eval Systems Unlock Superpowers For Free
<ol type="a">
<li>Fine-Tuning</li>
<li>Data Synthesis &amp; Curation</li>
<li>Debugging</li>
</ol></li>
</ol>
</div>
<div class="g-col-4">
<p><a href="https://hamel.dev/llm-judge/" target="_blank"><img src="https://hamel.dev/blog/posts/llm-judge/images/cover_img.png" class="img-fluid"></a></p>
<p><a href="https://hamel.dev/llm-judge/" target="_blank"><strong>Creating a LLM-as-a-Judge That Drives Business Results</strong></a></p>
<p><strong>Contents:</strong></p>
<ol type="1">
<li>The Problem: AI Teams Are Drowning in Data</li>
<li>Step 1: Find The Principal Domain Expert</li>
<li>Step 2: Create a Dataset</li>
<li>Step 3: Direct The Domain Expert to Make Pass/Fail Judgments with Critiques</li>
<li>Step 4: Fix Errors</li>
<li>Step 5: Build Your LLM as A Judge, Iteratively</li>
<li>Step 6: Perform Error Analysis</li>
<li>Step 7: Create More Specialized LLM Judges (if needed)</li>
<li>Recap of Critique Shadowing</li>
<li>Resources</li>
</ol>
</div>
<div class="g-col-4">
<p><a href="https://hamel.dev/field-guide" target="_blank"><img src="https://hamel.dev/blog/posts/field-guide/images/field_guide_2.png" class="img-fluid"></a></p>
<p><a href="https://hamel.dev/field-guide" target="_blank"><strong>A Field Guide to Rapidly Improving AI Products</strong></a></p>
<p><strong>Contents:</strong></p>
<ol type="1">
<li>How error analysis consistently reveals the highest-ROI improvements</li>
<li>Why a simple data viewer is your most important AI investment</li>
<li>How to empower domain experts (not just engineers) to improve your AI</li>
<li>Why synthetic data is more effective than you think</li>
<li>How to maintain trust in your evaluation system</li>
<li>Why your AI roadmap should count experiments, not features</li>
</ol>
</div>
</div>
<p><a href="../../../blog/posts/evals-faq/#q-what-are-llm-evals" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/what-are-llm-evals.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How are evaluations used differently in CI/CD vs.¬†monitoring production?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-are-evaluations-used-differently-in-cicd-vs-monitoring-production.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>The most important difference between CI vs.&nbsp;production evaluation is the data used for testing.</p>
<p>Test datasets for CI are small (in many cases 100+ examples) and purpose-built. Examples cover core features, regression tests for past bugs, and known edge cases. Since CI tests are run frequently, the cost of each test has to be carefully considered (that‚Äôs why you carefully curate the dataset). Favor assertions or other deterministic checks over LLM-as-judge evaluators.</p>
<p>For evaluating production traffic, you can sample live traces and run evaluators against them asynchronously. Since you usually lack reference outputs on production data, you might rely more on on more expensive reference-free evaluators like LLM-as-judge. Additionally, track confidence intervals for production metrics. If the lower bound crosses your threshold, investigate further.</p>
<p>These two systems are complementary: when production monitoring reveals new failure patterns through error analysis and evals, add representative examples to your CI dataset. This mitigates regressions on new issues.</p>
<p><a href="../../../blog/posts/evals-faq/#q-how-are-evaluations-used-differently-in-cicd-vs-monitoring-production" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-are-evaluations-used-differently-in-cicd-vs-monitoring-production.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How do I evaluate complex multi-step workflows?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-do-i-evaluate-complex-multi-step-workflows.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Log the entire workflow from initial trigger to final business outcome. Include LLM calls, tool usage, human approvals, and database writes in your traces. You will need this visibility to properly diagnose failures.</p>
<p>Use both outcome and process metrics. Outcome metrics verify the final result meets requirements: Was the business case complete? Accurate? Properly formatted? Process metrics evaluate efficiency: step count, time taken, resource usage. Process failures are often easier to debug since they‚Äôre more deterministic, so tackle them first.</p>
<p>Segment your <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html">error analysis</a> by workflow stages. Early stage failures (understanding user input) differ from middle stage failures (data processing) and late stage failures (formatting output). Early stage improvements have more impact since errors cascade in LLM chains.</p>
<p>Use <a href="../../../blog/posts/evals-faq/how-do-i-evaluate-agentic-workflows.html">transition failure matrices</a> to analyze where workflows break. Create a matrix showing the last successful state versus where the first failure occurred. This reveals failure hotspots and guides where to invest debugging effort.</p>
<p><a href="../../../blog/posts/evals-faq/#q-how-do-i-evaluate-complex-multi-step-workflows" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-do-i-evaluate-complex-multi-step-workflows.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: How often should I re-run error analysis on my production system?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/how-often-should-i-re-run-error-analysis-on-my-production-system.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Re-run <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html">error analysis</a> when making significant changes: new features, prompt updates, model switches, or major bug fixes. A useful heuristic is to set a goal for reviewing <em>at least</em> 100+ fresh traces each review cycle. Typical review cycles we‚Äôve seen range from 2-4 weeks. See <a href="../../../blog/posts/evals-faq/how-can-i-efficiently-sample-production-traces-for-review.html" target="_blank">this FAQ</a> on how to sample traces effectively.</p>
<p>Between major analyses, review 10-20 traces weekly, focusing on outliers: unusually long conversations, sessions with multiple retries, or traces flagged by automated monitoring. Adjust frequency based on system stability and usage growth. New systems need weekly analysis until failure patterns stabilize. Mature systems might need only monthly analysis unless usage patterns change. Always analyze after incidents, user complaint spikes, or metric drift. Scaling usage introduces new edge cases.</p>
<p><a href="../../../blog/posts/evals-faq/#q-how-often-should-i-re-run-error-analysis-on-my-production-system" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/how-often-should-i-re-run-error-analysis-on-my-production-system.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Why do you recommend binary (pass/fail) evaluations instead of 1-5 ratings (Likert scales)?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<blockquote class="blockquote">
<p>Engineers often believe that Likert scales (1-5 ratings) provide more information than binary evaluations, allowing them to track gradual improvements. However, this added complexity often creates more problems than it solves in practice.</p>
</blockquote>
<p>Binary evaluations force clearer thinking and more consistent labeling. Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, detecting statistical differences requires larger sample sizes, and annotators often default to middle values to avoid making hard decisions.</p>
<p>Having binary options forces people to make a decision rather than hiding uncertainty in middle values. Binary decisions are also faster to make during error analysis - you don‚Äôt waste time debating whether something is a 3 or 4.</p>
<p>For tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using a scale. For example, instead of rating factual accuracy 1-5, you could track ‚Äú4 out of 5 expected facts included‚Äù as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria.</p>
<p>Start with binary labels to understand what ‚Äòbad‚Äô looks like. Numeric labels are advanced and usually not necessary.</p>
<p><a href="../../../blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Are there scenarios where synthetic data may not be reliable?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/are-there-scenarios-where-synthetic-data-may-not-be-reliable.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Yes: synthetic data can mislead or mask issues. For guidance on generating synthetic data when appropriate, see <a href="../../../blog/posts/evals-faq/what-is-the-best-approach-for-generating-synthetic-data.html">What is the best approach for generating synthetic data?</a></p>
<p>Common scenarios where synthetic data fails:</p>
<ol type="1">
<li><p><strong>Complex domain-specific content</strong>: LLMs often miss the structure, nuance, or quirks of specialized documents (e.g., legal filings, medical records, technical forms). Without real examples, critical edge cases are missed.</p></li>
<li><p><strong>Low-resource languages or dialects</strong>: For low-resource languages or dialects, LLM-generated samples are often unrealistic. Evaluations based on them won‚Äôt reflect actual performance.</p></li>
<li><p><strong>When validation is impossible</strong>: If you can‚Äôt verify synthetic sample realism (due to domain complexity or lack of ground truth), real data is important for accurate evaluation.</p></li>
<li><p><strong>High-stakes domains</strong>: In high-stakes domains (medicine, law, emergency response), synthetic data often lacks subtlety and edge cases. Errors here have serious consequences, and manual validation is difficult.</p></li>
<li><p><strong>Underrepresented user groups</strong>: For underrepresented user groups, LLMs may misrepresent context, values, or challenges. Synthetic data can reinforce biases in the training data of the LLM.</p></li>
</ol>
<p><a href="../../../blog/posts/evals-faq/#q-are-there-scenarios-where-synthetic-data-may-not-be-reliable" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>



 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/are-there-scenarios-where-synthetic-data-may-not-be-reliable.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: Should I use ‚Äúready-to-use‚Äù evaluation metrics?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/should-i-use-ready-to-use-evaluation-metrics.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p><strong>No.&nbsp;Generic evaluations waste time and create false confidence.</strong> (Unless you‚Äôre using them for exploration).</p>
<p>One instructor noted:</p>
<blockquote class="blockquote">
<p>‚ÄúAll you get from using these prefab evals is you don‚Äôt know what they actually do and in the best case they waste your time and in the worst case they create an illusion of confidence that is unjustified.‚Äù<sup>1</sup></p>
</blockquote>
<p>Generic evaluation metrics are everywhere. Eval libraries contain scores like helpfulness, coherence, quality, etc. promising easy evaluation. These metrics measure abstract qualities that may not matter for your use case. Good scores on them don‚Äôt mean your system works.</p>
<p>Instead, conduct <a href="../../../blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html" target="_blank">error analysis</a> to understand failures. Define <a href="../../../blog/posts/evals-faq/why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales.html" target="_blank">binary failure modes</a> based on real problems. Create <a href="../../../blog/posts/evals-faq/should-i-build-automated-evaluators-for-every-failure-mode-i-find.html" target="_blank">custom evaluators</a> for those failures and validate them against human judgment. Essentially, the entire evals process.</p>
<p>Experienced practitioners may still use these metrics, just not how you‚Äôd expect. As Picasso said: ‚ÄúLearn the rules like a pro, so you can break them like an artist.‚Äù Once you understand why generic metrics fail as evaluations, you can repurpose them as exploration tools to <a href="../../../blog/posts/evals-faq/how-can-i-efficiently-sample-production-traces-for-review.html" target="_blank">find interesting traces</a> (explained in the next FAQ).</p>
<p><a href="../../../blog/posts/evals-faq/#q-should-i-use-ready-to-use-evaluation-metrics" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://www.linkedin.com/in/intellectronica/" target="_blank">Eleanor Berger</a>, our wonderful TA.‚Ü©Ô∏é</p></li>
</ol>
</section></div> ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/should-i-use-ready-to-use-evaluation-metrics.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Q: What gaps in eval tooling should I be prepared to fill myself?</title>
  <link>https://hamel.dev/blog/posts/evals-faq/what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself.html</link>
  <description><![CDATA[ 

<!-- Content inserted at the beginning of body tag -->
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PKGWQMKL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
<script async="" data-uid="df8af2b6ed" src="https://hamel.kit.com/df8af2b6ed/index.js"></script>




<p>Most eval tools handle the basics well: logging complete traces, tracking metrics, prompt playgrounds, and annotation queues. These are table stakes. Here are four areas where you‚Äôll likely need to supplement existing tools.</p>
<p>Watch for vendors addressing these gaps: it‚Äôs a strong signal they understand practitioner needs.</p>
<section id="error-analysis-and-pattern-discovery" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-and-pattern-discovery">1. Error Analysis and Pattern Discovery</h3>
<p>After reviewing traces where your AI fails, can your tooling automatically cluster similar issues? For instance, if multiple traces show the assistant using casual language for luxury clients, you need something that recognizes this broader ‚Äúpersona-tone mismatch‚Äù pattern. We recommend building capabilities that use AI to suggest groupings, rewrite your observations into clearer failure taxonomies, help find similar cases through semantic search, etc.</p>
</section>
<section id="ai-powered-assistance-throughout-the-workflow" class="level3">
<h3 class="anchored" data-anchor-id="ai-powered-assistance-throughout-the-workflow">2. AI-Powered Assistance Throughout the Workflow</h3>
<p>The most effective workflows use AI to accelerate every stage of evaluation. During error analysis, you want an LLM helping categorize your open-ended observations into coherent failure modes. For example, you might annotate several traces with notes like ‚Äúwrong tone for investor,‚Äù ‚Äútoo casual for luxury buyer,‚Äù etc. Your tooling should recognize these as the same underlying pattern and suggest a unified ‚Äúpersona-tone mismatch‚Äù category.</p>
<p>You‚Äôll also want AI assistance in proposing fixes. After identifying 20 cases where your assistant omits pet policies from property summaries, can your workflow analyze these failures and suggest specific prompt modifications? Can it draft refinements to your SQL generation instructions when it notices patterns of missing WHERE clauses?</p>
<p>Additionally, good workflows help you conduct data analysis of your annotations and traces. I like using notebooks with AI in-the-loop like <a href="https://julius.ai/" target="_blank">Julius</a>,<a href="https://hex.tech" target="_blank">Hex</a> or <a href="https://solveit.fast.ai/" target="_blank">SolveIt</a>. These help me discover insights like ‚Äúlocation ambiguity errors spike 3x when users mention neighborhood names‚Äù or ‚Äútone mismatches occur 80% more often in email generation than other modalities.‚Äù</p>
</section>
<section id="custom-evaluators-over-generic-metrics" class="level3">
<h3 class="anchored" data-anchor-id="custom-evaluators-over-generic-metrics">3. Custom Evaluators Over Generic Metrics</h3>
<p>Be prepared to build most of your evaluators from scratch. Generic metrics like ‚Äúhallucination score‚Äù or ‚Äúhelpfulness rating‚Äù rarely capture what actually matters for your application‚Äîlike proposing unavailable showing times or omitting budget constraints from emails. In our experience, successful teams spend most of their effort on application-specific metrics.</p>
</section>
<section id="apis-that-support-custom-annotation-apps" class="level3">
<h3 class="anchored" data-anchor-id="apis-that-support-custom-annotation-apps">4. APIs That Support Custom Annotation Apps</h3>
<p>Custom annotation interfaces <a href="../../../blog/posts/evals-faq/should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf.html" target="_blank">work best for most teams</a>. This requires observability platforms with thoughtful APIs. I often have to build my own libraries and abstractions just to make bulk data export manageable. You shouldn‚Äôt have to paginate through thousands of requests or handle timeout-prone endpoints just to get your data. Look for platforms that provide true bulk export capabilities and, crucially, APIs that let you write annotations back efficiently.</p>
<p><a href="../../../blog/posts/evals-faq/#q-what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself" class="faq-back-link" target="_blank">‚Ü©Ô∏é Back to main FAQ</a></p>
<hr>
<p><em>This article is part of our AI Evals FAQ, a collection of common questions (and answers) about LLM evaluation. <a href="../../../blog/posts/evals-faq/">View all FAQs</a> or <a href="../../..">return to the homepage</a>.</em></p>


</section>

 ]]></description>
  <category>LLMs</category>
  <category>evals</category>
  <category>faq</category>
  <category>faq-individual</category>
  <guid>https://hamel.dev/blog/posts/evals-faq/what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself.html</guid>
  <pubDate>Tue, 29 Jul 2025 19:36:50 GMT</pubDate>
  <media:content url="https://hamel.dev/blog/posts/evals-faq/images/eval_faq.png" medium="image" type="image/png" height="96" width="144"/>
</item>
</channel>
</rss>
