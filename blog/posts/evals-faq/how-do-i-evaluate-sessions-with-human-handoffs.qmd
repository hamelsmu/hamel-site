---
title: "Q: How do I evaluate sessions with human handoffs?"
categories: [LLMs, evals, faq, faq-individual]
date: last-modified
image: images/eval_faq.png
exclude-from-listing: true
aliases:
  - /evals-faq/how-do-i-evaluate-sessions-with-human-handoffs
page-navigation: true
toc: true
toc-expand: 2
toc-depth: 3
toc-location: right
---

Capture the complete user journey in your traces, including human handoffs. The trace continues until the user's need is resolved or the session ends, not when AI hands off to a human. Log the handoff decision, why it occurred, context transferred, wait time, human actions, final resolution, and whether the human had sufficient context. Many failures occur at handoff boundaries where AI hands off too early, too late, or without proper context.

Evaluate handoffs as potential failure modes during [error analysis](/blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html). Ask: Was the handoff necessary? Did the AI provide adequate context? Track both handoff quality and handoff rate. Sometimes the best improvement reduces handoffs entirely rather than improving handoff execution.

[â†© Back to main FAQ](/blog/posts/evals-faq/#q-how-do-i-evaluate-sessions-with-human-handoffs){target="_blank" .faq-back-link}

{{< include _faq-context.qmd >}}
