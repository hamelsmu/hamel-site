## Q: How do I make the case for investing in evaluations to my team?

Don't try to sell your team on "evals". Instead, show them what you find when you look at the data.

Start by doing the [error analysis](#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed){target="_blank"} yourself. Look at 50 to 100 real user conversations and find the most common ways the product is failing. Use these findings to tell a story with data.

Present your team with:

* A list of the top failure modes you discovered.
* Metrics showing how often high-impact errors are happening.
* Surprising ways that users are interacting with the product.
* Reports on the bugs you found and fixed, framed as "prevented production issues".

This approach builds trust. Don't just show dashboards and metrics; tell the story of what you're finding in the data. By narrating your findings, you teach the team what you're learning, providing immediate value. When you fix an issue, show how the error rate for that specific problem went down. Soon, your team will see the progress and ask how you're doing it. Let results instead of methods lead the conversation.

This is similar to classic machine learning projects, where outcomes are speculative and progress is bounded by [iterating on experiments](https://hamel.dev/blog/posts/field-guide/#your-ai-roadmap-should-count-experiments-not-features){target="_blank"}. In this situation, it's important that you share the learnings from each experiment to show progress and encourage investment.

[â†— Focus view](/blog/posts/evals-faq/how-do-i-make-the-case-for-investing-in-evaluations-to-my-team.html){.faq-individual-link}