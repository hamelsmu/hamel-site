---
title: "Q: Should I build a custom annotation tool or use something off-the-shelf?"
description: "Build a custom annotation tool. This is the single most impactful investment you can make for your AI evaluation workflow"
categories: [LLMs, evals, faq]
author: 
  - Hamel Husain
  - Shreya Shankar
date: last-modified
image: images/eval_faq.png
exclude-from-listing: true
aliases:
  - /evals-faq/should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf
page-navigation: true
---

**Build a custom annotation tool.** This is the single most impactful investment you can make for your AI evaluation workflow. With AI-assisted development tools like Cursor or Lovable, you can build a tailored interface in hours. I often find that teams with custom annotation tools iterate ~10x faster.

Custom tools excel because:

- They show all your context from multiple systems in one place
- They can render your data in a product specific way (images, widgets, markdown, buttons, etc.)
- They're designed for your specific workflow (custom filters, sorting, progress bars, etc.)

Off-the-shelf tools may be justified when you need to coordinate dozens of distributed annotators with enterprise access controls. Even then, many teams find the configuration overhead and limitations aren't worth it.

[Isaac's Anki flashcard annotation app](https://youtu.be/fA4pe9bE0LY) shows the power of custom toolsâ€”handling 400+ results per query with keyboard navigation and domain-specific evaluation criteria that would be nearly impossible to configure in a generic tool.

{{< include _faq-context.qmd >}}
