---
title: "Q: How should I version and manage prompts?"
categories: [LLMs, evals, faq, faq-individual]
date: last-modified
image: images/eval_faq.png
exclude-from-listing: true
aliases:
  - /evals-faq/how-should-i-version-and-manage-prompts
page-navigation: true
toc: true
toc-expand: 2
toc-depth: 3
toc-location: right
---

There is an unavoidable tension between keeping prompts close to the code vs. an environment that non-technical stakeholders can access.

**My preferred approach is storing prompts in Git.** This treats them as software artifacts that are versioned, reviewed, and deployed atomically with the application code. While the Git command line is unfriendly for non-technical folks, the [GitHub](https://github.com) web interface and the GitHub [Desktop app](https://desktop.github.com/) make it very approachable. When I was working at GitHub, I worked with many non-technical professionals, including lawyers and accountants, who used these tools effectively.  Here is a good [blog post](https://ben.balter.com/2023/03/02/github-for-non-technical-roles/) aimed at non-technical folks to get started.

Alternatively, most vendors in the LLM tooling space, such as observability platforms like Arize, Braintrust, and LangSmith, offer dedicated prompt management tools. These are accessible for rapid iteration by product managers but risk creating additional layers of indirection. 

An appealing aspect of the aforementioned prompt management tools is the ability to experiment with prompts and version them in one interface.  However, I find Jupyter notebooks are more powerful for this purpose. To see what this looks like in practice, Teresa Torres gives a fantastic, hands-on walkthrough of how she, as a PM, used notebooks for the entire eval and experimentation lifecycle:

{{< video https://youtu.be/N-qAOv_PNPc >}}

[â†© Back to main FAQ](/blog/posts/evals-faq/#q-how-should-i-version-and-manage-prompts){target="_blank" .faq-back-link}

{{< include _faq-context.qmd >}}
