---
title: "Q: Can my evaluators also be used to automatically *fix* or *correct* outputs in production?"
categories: [LLMs, evals, faq, faq-individual]
date: last-modified
image: images/eval_faq.png
aliases:
  - /evals-faq/can-my-evaluators-also-be-used-to-automatically-fix-or-correct-outputs-in-production
page-navigation: true
---

Yes, but only a specific subset of them. This is the distinction between an **evaluator** and a **guardrail** that we [previously discussed](/blog/posts/evals-faq/whats-the-difference-between-guardrails-evaluators.html). As a reminder:

- **Evaluators** typically run *asynchronously* after a response has been generated. They measure quality but don't interfere with the user's immediate experience.  
- **Guardrails** run *synchronously* in the critical path of the request, before the output is shown to the user. Their job is to prevent high-impact failures in real-time.

There are two important decision criteria for deciding whether to use an evaluator as a guardrail:

1. **Latency & Cost**: Can the evaluator run fast enough and cheaply enough in the critical request path without degrading user experience?

2. **Error Rate Trade-offs**: What's the cost-benefit balance between false positives (blocking good outputs and frustrating users) versus false negatives (letting bad outputs reach users and causing harm)? In high-stakes domains like medical advice, false negatives may be more costly than false positives. In creative applications, false positives that block legitimate creativity may be more harmful than occasional quality issues.  

Most guardrails are designed to be **fast** (to avoid harming user experience) and have a **very low false positive rate** (to avoid blocking valid responses). For this reason, you would almost never use a slow or non-deterministic LLM-as-Judge as a synchronous guardrail.  However, these tradeoffs might be different for your use case.

<hr>
<div class="cta" style="text-align: center;">
<strong>ðŸ‘‰ <em>We are teaching our last and final cohort of our [AI Evals course](https://bit.ly/evals-ai) next month</strong> (we have to get back to building). Here is a [35% discount code](https://bit.ly/evals-ai) for readers.</em> ðŸ‘ˆ
</div>

[^1]: [Eleanor Berger](https://www.linkedin.com/in/intellectronica/), our wonderful TA.
[^2]: Paul Graham, ["Writes and Write-Nots"](https://paulgraham.com/writes.html)
[^3]: Shreya Shankar, et al., ["Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences"](https://arxiv.org/abs/2404.12272)

[â†© Back to main FAQ](/blog/posts/evals-faq/#q-can-my-evaluators-also-be-used-to-automatically-fix-or-correct-outputs-in-production){.faq-back-link}

{{< include _faq-context.qmd >}}
