---
title: "Q: Are there scenarios where synthetic data may not be reliable?"
categories: [LLMs, evals, faq, faq-individual]
date: last-modified
image: images/eval_faq.png
exclude-from-listing: true
aliases:
  - /evals-faq/are-there-scenarios-where-synthetic-data-may-not-be-reliable
page-navigation: true
toc: true
toc-expand: 2
toc-depth: 3
toc-location: right
---

Yes: synthetic data can mislead or mask issues. For guidance on generating synthetic data when appropriate, see [What is the best approach for generating synthetic data?](/blog/posts/evals-faq/what-is-the-best-approach-for-generating-synthetic-data.html)

Common scenarios where synthetic data fails:

1. **Complex domain-specific content**: LLMs often miss the structure, nuance, or quirks of specialized documents (e.g., legal filings, medical records, technical forms). Without real examples, critical edge cases are missed.

2. **Low-resource languages or dialects**: For low-resource languages or dialects, LLM-generated samples are often unrealistic. Evaluations based on them won't reflect actual performance.

3. **When validation is impossible**: If you can't verify synthetic sample realism (due to domain complexity or lack of ground truth), real data is important for accurate evaluation.

4. **High-stakes domains**: In high-stakes domains (medicine, law, emergency response), synthetic data often lacks subtlety and edge cases. Errors here have serious consequences, and manual validation is difficult.

5. **Underrepresented user groups**: For underrepresented user groups, LLMs may misrepresent context, values, or challenges. Synthetic data can reinforce biases in the training data of the LLM.

[â†© Back to main FAQ](/blog/posts/evals-faq/#q-are-there-scenarios-where-synthetic-data-may-not-be-reliable){target="_blank" .faq-back-link}

{{< include _faq-context.qmd >}}
