---
title: "Q: How do I surface problematic traces for review beyond user feedback?"
categories: [LLMs, evals, faq, faq-individual]
date: last-modified
image: images/eval_faq.png
exclude-from-listing: true
aliases:
  - /evals-faq/how-do-i-surface-problematic-traces-for-review-beyond-user-feedback
  - /evals-faq/how-do-i-surface-problematic-traces-for-review
page-navigation: true
toc: true
toc-expand: 2
toc-depth: 3
toc-location: right
---

While user feedback is a good way to narrow in on problematic traces, other methods are also useful. Here are three complementary approaches:

### Start with random sampling

The simplest approach is reviewing a random sample of traces. If you find few issues, escalate to stress testing: create queries that deliberately test your prompt constraints to see if the AI follows your rules. 

### Use evals for initial screening

Use existing evals to find problematic traces and potential issues. Once you've identified these, you can proceed with the typical evaluation process starting with [error analysis](/blog/posts/evals-faq/why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed.html).

### Leverage efficient sampling strategies

For more sophisticated trace discovery, [use outlier detection, metric-based sorting, and stratified sampling](/blog/posts/evals-faq/how-can-i-efficiently-sample-production-traces-for-review.html) to find interesting traces. [Generic metrics can serve as exploration signals](/blog/posts/evals-faq/should-i-use-ready-to-use-evaluation-metrics.html) to identify traces worth reviewing, even if they don't directly measure quality.

[â†© Back to main FAQ](/blog/posts/evals-faq/#q-how-do-i-surface-problematic-traces-for-review-beyond-user-feedback){target="_blank" .faq-back-link}

{{< include _faq-context.qmd >}}
