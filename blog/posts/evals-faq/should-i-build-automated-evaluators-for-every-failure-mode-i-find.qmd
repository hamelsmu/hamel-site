---
title: "Q: Should I build automated evaluators for every failure mode I find?"
categories: [LLMs, evals, faq, faq-individual]
date: last-modified
image: images/eval_faq.png
aliases:
  - /evals-faq/should-i-build-automated-evaluators-for-every-failure-mode-i-find
page-navigation: true
toc: true
toc-expand: 2
toc-depth: 3
toc-location: right
---

Focus automated evaluators on failures that persist after fixing your prompts. Many teams discover their LLM doesn't meet preferences they never actually specified - like wanting short responses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before building complex evaluation infrastructure.

Consider the cost hierarchy of different evaluator types. Simple assertions and reference-based checks (comparing against known correct answers) are cheap to build and maintain. LLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and coordination between developers, PMs, and domain experts. This cost difference should shape your evaluation strategy.

Only build expensive evaluators for problems you'll iterate on repeatedly. Since LLM-as-Judge comes with significant overhead, save it for persistent generalization failures - not issues you can fix trivially. Start with cheap code-based checks where possible: regex patterns, structural validation, or execution tests. Reserve complex evaluation for subjective qualities that can't be captured by simple rules.

[↩ Back to main FAQ](/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find){target="_blank" .faq-back-link}

[↩ Back to main FAQ](/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find){.faq-back-link}

{{< include _faq-context.qmd >}}
