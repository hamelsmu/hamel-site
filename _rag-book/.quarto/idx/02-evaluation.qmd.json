{"title":"Modern IR Evaluation for RAG","markdown":{"headingText":"Modern IR Evaluation for RAG","containsRefs":false,"markdown":"\n*Based on a presentation by Nandan Thakur*\n\n[Nandan Thakur](https://thakur-nandan.github.io/){target=\"_blank\"} is a researcher at the University of Waterloo and a key contributor to major Information Retrieval (IR) benchmarks, including BEIR and the new FreshStack. His talk explains why traditional IR evals designed for search engines may be insufficient for RAG systems. He argues that LLM-generated answers often carry different retrieval goals which necessitate different IR metrics.\n\n## Introduction and Speaker Background\n\n![](../notes/llm/rag/p2-images/slide_1.png)\n\n*([Timestamp: 00:00:00](https://www.youtube.com/watch?v=Trps2swgeOg&t=0s))*\n\nThe title slide for Nandan's talk, \"Modern IR Evaluation in the RAG Era.\"\n\n![](../notes/llm/rag/p2-images/slide_2.png)\n\n*([Timestamp: 00:00:14](https://www.youtube.com/watch?v=Trps2swgeOg&t=14s))*\n\nNandan introduces himself as a fourth-year Ph.D. student at the University of Waterloo. He outlines his background, including research at UKP-TU and internships at Google Research and Databricks. He highlights his work on the BEIR, MIRACL, and FreshStack benchmarks, and the TREC RAG track.\n\n![](../notes/llm/rag/p2-images/slide_3.png)\n\n*([Timestamp: 00:01:09](https://www.youtube.com/watch?v=Trps2swgeOg&t=69s))*\n\nNandan outlines the presentation's three parts: a history of traditional IR evaluation, an explanation of why evaluation needs to change for RAG, and a deep dive into the FreshStack benchmark as a modern solution.\n\n## The History of Information Retrieval\n\n![](../notes/llm/rag/p2-images/slide_4.png)\n\n*([Timestamp: 00:01:45](https://www.youtube.com/watch?v=Trps2swgeOg&t=105s))*\n\nWhile RAG is new, Information Retrieval is a field with over 60 years of history. The slide contrasts an early Google interface with a modern one to show the evolution of web search.\n\n![](../notes/llm/rag/p2-images/slide_5.png)\n\n*([Timestamp: 00:01:50](https://www.youtube.com/watch?v=Trps2swgeOg&t=110s))*\n\nNandan emphasizes IR's history by showing a 1965 paper on the SMART Retrieval System, an early automated document retrieval system. He also introduces the Text Retrieval Conference (TREC), an influential conference since the 1990s that continues to produce IR benchmarks and standards.\n\n![](../notes/llm/rag/p2-images/slide_6.png)\n\n*([Timestamp: 00:03:00](https://www.youtube.com/watch?v=Trps2swgeOg&t=180s))*\n\nA diagram from NIST illustrates the breadth of TREC's evaluation tasks from 1992 to 2020. These tracks range from classic ad-hoc retrieval to specialized areas like multilingual search and human-in-the-loop evaluation, demonstrating the field's ongoing evolution.\n\n## The Cranfield Paradigm\n\n![](../notes/llm/rag/p2-images/slide_7.png)\n\n*([Timestamp: 00:03:54](https://www.youtube.com/watch?v=Trps2swgeOg&t=234s))*\n\nNandan introduces the **Cranfield Paradigm**, the foundation of traditional IR evaluation developed in the 1960s. It established the concept of a **test collection**, comprising three components:\n\n1.  **Topics:** A fixed set of user queries.\n2.  **Corpus:** A fixed collection of documents.\n3.  **Relevance Judgments:** Human-annotated labels indicating which documents are relevant to which queries.\n\nThis three-part structure remains the basis for most IR benchmarks today.\n\n![](../notes/llm/rag/p2-images/slide_8.png)\n\n*([Timestamp: 00:06:00](https://www.youtube.com/watch?v=Trps2swgeOg&t=360s))*\n\nNandan shows examples of modern test collections. He highlights **BEIR** for its diversity of tasks, **MIRACL** for multilingual retrieval, and the typical **TREC** query structure, which includes a `title`, `description`, and detailed `narrative`.\n\n## The BEIR Benchmark\n\n![](../notes/llm/rag/p2-images/slide_9.png)\n\n*([Timestamp: 00:07:20](https://www.youtube.com/watch?v=Trps2swgeOg&t=440s))*\n\nThis slide introduces the BEIR (Benchmarking-IR) benchmark, which was among the first to popularize zero-shot evaluation for retrieval models.\n\n![](../notes/llm/rag/p2-images/slide_10.png)\n\n*([Timestamp: 00:07:38](https://www.youtube.com/watch?v=Trps2swgeOg&t=458s))*\n\nNandan explains **zero-shot evaluation**, where a model is tested on a domain or task it has not seen during training. This contrasts with *in-domain* evaluation (training and testing on similar data). Zero-shot evaluation is more realistic because high-quality, labeled training data for niche use cases is scarce and expensive to create.\n\n## Problems with Current Benchmarks\n\n![](../notes/llm/rag/p2-images/slide_11.png)\n\n*([Timestamp: 00:10:09](https://www.youtube.com/watch?v=Trps2swgeOg&t=609s))*\n\nNandan explains the motivation for BEIR. Around 2020-2021, the field focused heavily on the MSMARCO dataset, leading to **saturation** (performance plateaus) and **overfitting**. BEIR was created to combat this by providing a diverse set of datasets to test a model's generalization ability beyond a single domain.\n\n![](../notes/llm/rag/p2-images/slide_12.png)\n\n*([Timestamp: 00:11:10](https://www.youtube.com/watch?v=Trps2swgeOg&t=670s))*\n\nNandan explains that BEIR is no longer a truly \"zero-shot\" benchmark. Researchers now often include BEIR's training sets in their model development pipelines. This, along with private models using unknown training data, repeats the overfitting problem that BEIR was designed to solve.\n\n![](../notes/llm/rag/p2-images/slide_13.png)\n\n*([Timestamp: 00:13:20](https://www.youtube.com/watch?v=Trps2swgeOg&t=800s))*\n\nNandan highlights a practical issue: leaderboards are now too crowded to be useful. The MTEB leaderboard contains over 400 models, with the top contenders separated by marginal scores. This makes it difficult for practitioners to select a model and raises the question of how these models perform on other, more specialized tasks.\n\n![](../notes/llm/rag/p2-images/slide_14.png)\n\n*([Timestamp: 00:14:43](https://www.youtube.com/watch?v=Trps2swgeOg&t=883s))*\n\nThis slide summarizes the limitations of existing test collections like BEIR. They are often static, leading to data contamination risk. They can suffer from incomplete \"shallow labeling\" from human annotators. They may also lack realistic question distributions, prompting even the creators of benchmarks like HotpotQA to advise against their use for modern agentic systems.\n\n## The RAG Era Changes Everything\n\n![](../notes/llm/rag/p2-images/slide_15.png)\n\n*([Timestamp: 00:17:28](https://www.youtube.com/watch?v=Trps2swgeOg&t=1048s))*\n\nNandan contrasts the old and new search paradigms. \"Search back then\" shows a ranked list of links, while \"Search now\" shows a generated answer block with citations, characteristic of RAG systems.\n\n![](../notes/llm/rag/p2-images/slide_16.png)\n\n*([Timestamp: 00:18:20](https://www.youtube.com/watch?v=Trps2swgeOg&t=1100s))*\n\nThis slide diagrams the architectural shift. Before RAG, a search model returned a ranked list of documents to the user. In the RAG era, the search model provides retrieved documents as context to an LLM, which then generates a response for the user.\n\n## Different Users, Different Goals\n\n![](../notes/llm/rag/p2-images/slide_17.png)\n\n*([Timestamp: 00:19:10](https://www.youtube.com/watch?v=Trps2swgeOg&t=1150s))*\n\nNandan contrasts the two user types. A traditional search user is impatient, asks short queries, and scans a ranked list to click the first relevant link. A modern RAG user is patient, asks longer queries, and waits for a synthesized summary with citations, which they may use for verification.\n\n## The Evaluation Mismatch\n\n![](../notes/llm/rag/p2-images/slide_18.png)\n\n*([Timestamp: 00:21:08](https://www.youtube.com/watch?v=Trps2swgeOg&t=1268s))*\n\nThis slide presents the talk's central argument. Traditional metrics like **MRR** (Mean Reciprocal Rank) and **NDCG** (Normalized Discounted Cumulative Gain) were designed for the traditional objective: \"Did we rank the relevant page at #1?\" The new RAG objective is: \"Did we fetch *every piece of evidence* needed for the LLM to answer this question?\" For this new goal, MRR and NDCG may be insufficient on their own, as they do not measure comprehensive evidence collection or redundancy.\n\n![](../notes/llm/rag/p2-images/slide_19.png)\n\n*([Timestamp: 00:23:20](https://www.youtube.com/watch?v=Trps2swgeOg&t=1400s))*\n\nThe argument is not to discard traditional relevance but to expand the evaluation criteria for RAG. While **Relevancy is [still] important**, it must now be balanced with new goals like finding a **minimal spanning document set**. This concept captures the need for a set of documents that is not only relevant but also comprehensively covers all aspects of an answer without being redundant.\n\n## Introducing FreshStack\n\n![](../notes/llm/rag/p2-images/slide_20.png)\n\n*([Timestamp: 00:24:50](https://www.youtube.com/watch?v=Trps2swgeOg&t=1490s))*\n\nNandan introduces **FreshStack**, a modern IR benchmark developed with Databricks. It is designed to evaluate retrieval for RAG on technical documents.\n\n![](../notes/llm/rag/p2-images/slide_21.png)\n\n*([Timestamp: 00:25:00](https://www.youtube.com/watch?v=Trps2swgeOg&t=1500s))*\n\nThe motivation for FreshStack was to create a realistic RAG benchmark that overcomes the limitations of existing academic benchmarks, which are often static and artificially easy. The framework was designed to use real user questions, ground answers in real-time documents, be scalable, and be new to avoid data contamination.\n\n## FreshStack Data Sources\n\n![](../notes/llm/rag/p2-images/slide_22.png)\n\n*([Timestamp: 00:26:16](https://www.youtube.com/watch?v=Trps2swgeOg&t=1576s))*\n\nFreshStack sources its queries from **Stack Overflow**, an ideal source for long, complex, real-world questions with community-vetted answers. To mitigate data contamination, the benchmark uses questions from five recent and niche topics asked primarily in 2023 and 2024.\n\n![](../notes/llm/rag/p2-images/slide_23.png)\n\n*([Timestamp: 00:27:30](https://www.youtube.com/watch?v=Trps2swgeOg&t=1650s))*\n\nThe document corpus comes from the **GitHub Repositories** of the corresponding topics. This provides a constantly updated source of technical documentation and code. An interesting finding is that for technical queries, the questions can be significantly longer than the answers.\n\n## The FreshStack Pipeline\n\n![](../notes/llm/rag/p2-images/slide_24.png)\n\n*([Timestamp: 00:28:18](https://www.youtube.com/watch?v=Trps2swgeOg&t=1698s))*\n\nNandan explains the three-step automated pipeline for building FreshStack:\n\n1.  **Nuggetization:** A Stack Overflow answer is broken down by GPT-4o into essential, atomic facts or \"nuggets.\"\n2.  **Oracle Retrieval:** A diverse pool of candidate documents is retrieved from the corpus using a hybrid of models.\n3.  **Support w/ Nuggets:** A GPT-4o judge checks which retrieved document chunks support each individual nugget, creating fine-grained relevance judgments.\n\n![](../notes/llm/rag/p2-images/slide_25.png)\n\n*([Timestamp: 00:29:50](https://www.youtube.com/watch?v=Trps2swgeOg&t=1790s))*\n\nThis slide shows a concrete example of nuggetization. An answer to a `Chroma.from_documents` error is broken down into four key facts: the cause of the error, the required import, the initialization step, and the function call.\n\n![](../notes/llm/rag/p2-images/slide_26.png)\n\n*([Timestamp: 00:30:50](https://www.youtube.com/watch?v=Trps2swgeOg&t=1850s))*\n\nThis slide illustrates the final steps. After a document is retrieved, the system checks which of the four nuggets it supports. This process creates nugget-level relevance labels, forming the basis for the new evaluation metrics.\n\n## FreshStack Evaluation Metrics\n\n![](../notes/llm/rag/p2-images/slide_27.png)\n\n*([Timestamp: 00:31:26](https://www.youtube.com/watch?v=Trps2swgeOg&t=1886s))*\n\nNandan introduces the three metrics used in FreshStack, which provide a holistic view of RAG retrieval performance:\n\n1.  **Diversity (alpha-nDCG@10):** Measures non-redundancy, penalizing the retrieval of multiple documents that support the same fact.\n2.  **Grounding (Coverage@20):** Measures the percentage of unique nuggets supported by the retrieved documents, directly evaluating evidence collection.\n3.  **Relevance (Recall@50):** A traditional metric that serves as a foundational check on whether the retrieved documents are on-topic.\n\nThis multi-faceted approach augments traditional relevance with metrics tailored to the specific goals of RAG.\n\n## Key Results and Findings\n\n![](../notes/llm/rag/p2-images/slide_28.png)\n\n*([Timestamp: 00:33:19](https://www.youtube.com/watch?v=Trps2swgeOg&t=1999s))*\n\nNandan presents results from the benchmark. Key findings include that current retrieval techniques struggle on these realistic tasks, and no single model performs best across all topics. The large gap between current model performance and the theoretical \"Oracle\" maximum indicates significant room for improvement.\n\n![](../notes/llm/rag/p2-images/slide_29.png)\n\n*([Timestamp: 00:34:55](https://www.youtube.com/watch?v=Trps2swgeOg&t=2095s))*\n\nNandan shares the public FreshStack leaderboard and a Google Colab notebook. The notebook provides a script for users to evaluate their own models on FreshStack using its multi-dimensional metrics.\n\n![](../notes/llm/rag/p2-images/slide_30.png)\n\n*([Timestamp: 00:36:01](https://www.youtube.com/watch?v=Trps2swgeOg&t=2161s))*\n\nThis slide summarizes the talk's main points. Traditional IR evaluation may be insufficient for RAG depending on the use case. Benchmarks like BEIR are now suffering from overfitting. Often, the goal of RAG retrieval is evidence collection, requiring metrics that evaluate diversity, informativeness, and correctness in addition to relevance.\n\n![](../notes/llm/rag/p2-images/slide_31.png)\n\n*([Timestamp: 00:37:15](https://www.youtube.com/watch?v=Trps2swgeOg&t=2235s))*\n\nNandan concludes by thanking his collaborators. The slide's meme reinforces his message: good evaluations are essential for developing better models.\n\n## Chapter Reflections\n\nNandan's message was to consider other retrieval metrics beyond relevance based on your product's needs. He argued that we must sometimes reconsider what \"good\" retrieval means. For the stack overflow use case, he considered multiple dimensions of performance:\n\n*   **Grounding (or Coverage):** Did the retrieval system fetch *all* the evidence needed to construct a complete and accurate answer? A missing fact can lead to an incomplete or incorrect generation, even if the retrieved documents are otherwise highly relevant.\n*   **Diversity:** Are the retrieved documents efficiently informative? Retrieving multiple documents that repeat the same information is less valuable than retrieving a set of documents that each contribute a unique and essential fact.\n*   **Relevance:** Is the retrieved information on-topic? This remains a fundamental check. A diverse and well-grounded set of documents is useless if it pertains to the wrong subject.\n\nThis is not a call to discard traditional metrics but to **augment** them. The FreshStack benchmark, with its blend of Recall, Coverage, and Diversity metrics, is an example of this.\n\n\n\n## Video\n\nHere is the full video:\n\n{{< video https://youtu.be/Trps2swgeOg >}}\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"css":["custom.css"],"output-file":"02-evaluation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"include-in-header":[{"text":"\\usepackage{fancyhdr}\n\\pagestyle{fancy}\n\\fancyhf{}\n\\fancyhead[C]{\\textcolor{brown!50!red!30!black}{AI Evals Course: 15\\% off at \\href{https://bit.ly/evals-ai}{bit.ly/evals-ai}}}\n\\fancyfoot[C]{\\thepage}\n% Fix text justification issues\n\\usepackage{microtype}\n\\tolerance=9999\n\\emergencystretch=10pt\n\\hyphenpenalty=10000\n\\exhyphenpenalty=100\n\\raggedbottom\n% Use ragged right to avoid justification issues\n\\usepackage{ragged2e}\n\\RaggedRight\n% Reduce spacing around images significantly\n\\setlength{\\belowcaptionskip}{1pt}\n\\setlength{\\abovecaptionskip}{3pt}\n\\setlength{\\intextsep}{4pt plus 1pt minus 1pt}\n\\setlength{\\textfloatsep}{6pt plus 1pt minus 1pt}\n"}],"output-file":"02-evaluation.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"documentclass":"scrbook","classoption":["oneside","openany"],"colorlinks":true,"linkcolor":"blue","urlcolor":"blue","citecolor":"blue"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}