{"title":"Optimizing Retrieval with Reasoning Models","markdown":{"headingText":"Optimizing Retrieval with Reasoning Models","containsRefs":false,"markdown":"\n*Based on a presentation by Orion Weller*\n\n[Orion Weller](https://orionweller.github.io/){target=\"_blank\"} from Johns Hopkins University focuses on embedding the instruction-following and reasoning capabilities of modern Large Language Models (LLMs) directly into the retrieval process.\n\nIn his talk, Orion argues that while LLMs have improved RAG, the core retrieval step has remained static. He introduces a paradigm where instruction-following and reasoning are baked directly into retrieval models, a fundamental shift from using LLMs for query rewriting or as generic rerankers.\n\n## LLM Capabilities: Instruction Following and Reasoning\n\n![](../notes/llm/rag/p3-images/slide_2.png)\n\n*([Timestamp: 00:00:18](https://youtu.be/YB3b-wPbSH8?t=18s))*\n\nThe talk begins by highlighting the user-facing interfaces of modern LLMs like ChatGPT, which have set new expectations for how we interact with AI. One key capability of LLMs is **instruction following**: executing complex, multi-part natural language instructions with high fidelity.\n\n![](../notes/llm/rag/p3-images/slide_4.png)\n\n*([Timestamp: 00:00:36](https://youtu.be/YB3b-wPbSH8?t=36s))*\n\nOrion shows the result of a pirate-themed haiku prompt. The model successfully adheres to all constraints: it generates a haiku, maintains a pirate style, and mentions \"RAG,\" demonstrating a level of instruction following that is a recent and significant advancement.\n\n![](../notes/llm/rag/p3-images/slide_5.png)\n\n*([Timestamp: 00:00:58](https://youtu.be/YB3b-wPbSH8?t=58s))*\n\nA second key capability is **reasoning**, also known as test-time compute or \"thinking.\" The slide shows a model verbalizing its thought process to solve a problem, generating intermediate \"thinking tokens\" that outline its step-by-step logic before providing the final answer.\n\n## The Search Paradigm Hasn't Changed\n\n![](../notes/llm/rag/p3-images/slide_7.png)\n\n*([Timestamp: 00:01:52](https://youtu.be/YB3b-wPbSH8?t=112s))*\n\nTo illustrate how little the search paradigm has changed, Orion shows Google's interface from 1999.\n\n![](../notes/llm/rag/p3-images/slide_8.png)\n\n*([Timestamp: 00:01:58](https://youtu.be/YB3b-wPbSH8?t=118s))*\n\nHe contrasts it with a modern Google search bar. Despite 26 years of development, the fundamental interaction remains the same: a user types keywords, and the system matches them to return a list of links.\n\n![](../notes/llm/rag/p3-images/slide_11.png)\n\n*([Timestamp: 00:02:46](https://youtu.be/YB3b-wPbSH8?t=166s))*\n\nDespite the interface, Orion argues the underlying retrieval process has not evolved. Even in advanced systems, the LLM is often just a \"wrapper.\" The system sends the query to a traditional search engine, gets back a standard list of results, and then uses the LLM to summarize them. The retrieval step itself hasn't gained the new capabilities of the LLM.\n\n## Evolution of Search Paradigms\n\n![](../notes/llm/rag/p3-images/slide_14.png)\n\n*([Timestamp: 00:03:58](https://youtu.be/YB3b-wPbSH8?t=238s))*\n\nTo illustrate current limitations, Orion starts with **Keyword Search**, which relies on exact lexical matching. Given a query and three documents, keyword search matches \"Data Encryption Standards\" and \"Wolves Outside Your Data\" because they contain the keyword \"data.\"\n\nIt fails to retrieve \"Digital Protection\" because it lacks the keyword \"data,\" even though \"digital\" is semantically similar.\n\n![](../notes/llm/rag/p3-images/slide_16.png)\n\n*([Timestamp: 00:04:11](https://youtu.be/YB3b-wPbSH8?t=251s))*\n\nThe next evolution is **Semantic Search**, which matches based on meaning, often by representing queries and documents as vectors in a shared semantic space. A good semantic search model would retrieve all three documents, as it understands the relationship between \"data\" and \"digital,\" and \"privacy\" and \"protection.\"\n\n![](../notes/llm/rag/p3-images/slide_20.png)\n\n*([Timestamp: 00:05:25](https://youtu.be/YB3b-wPbSH8?t=325s))*\n\nOrion introduces **Instruction-based Search**, where the query is a nuanced command. The user wants to find documents about data privacy that also use \"extended metaphors.\"\n\nAn instruction-based search system should understand this meta-level constraint and retrieve only the \"Wolves Outside Your Data\" document, which uses a metaphorical title. It correctly identifies that the other two documents, while topically relevant, do not meet the stylistic instruction.\n\n![](../notes/llm/rag/p3-images/slide_21.png)\n\n*([Timestamp: 00:06:16](https://youtu.be/YB3b-wPbSH8?t=376s))*\n\nOrion pushes the concept to its extreme with **Prompt and Reasoning-based Search**. The query now includes instructions about the desired *behavior* of the search engine, such as \"Have really high recall or I will lose my job.\"\n\nA traditional search engine would misinterpret this, likely searching for documents containing the word \"recall.\" An advanced, reasoning-based retriever should understand the user's intent and adjust its retrieval strategy.\n\n## Understanding Instructions in IR\n\n![](../notes/llm/rag/p3-images/slide_25.png)\n\n*([Timestamp: 00:06:42](https://youtu.be/YB3b-wPbSH8?t=402s))*\n\nWhat is an instruction in the context of IR? Orion breaks it down into several categories:\n\n- **Document attributes** like date, length, or source\n- **NLU aspects**, such as document sentiment or writing style  \n- **Logical conditions**, combining multiple constraints with operators like AND, OR, and NOT\n\nThe space of possible instructions mirrors the complexity of natural language.\n\n## Introducing Promptriever and Rank1\n\n![](../notes/llm/rag/p3-images/slide_32.png)\n\n*([Timestamp: 00:07:45](https://youtu.be/YB3b-wPbSH8?t=465s))*\n\nOrion introduces two models from his research that embody these principles:\n\n1. **Promptriever**: A fast embedding model for following instructions during initial retrieval\n2. **Rank1**: A powerful but slower reranker that uses reasoning and test-time compute for nuanced relevance judgments\n\n## Promptriever: Instruction-Trained Retrieval\n\n![](../notes/llm/rag/p3-images/slide_34.png)\n\n*([Timestamp: 00:08:23](https://youtu.be/YB3b-wPbSH8?t=503s))*\n\nOrion explains the two main retrieval architectures. A **Bi-Encoder** (dense retriever) creates separate query and document embeddings for fast comparison, making it highly scalable. A **Cross-Encoder** (reranker) processes the query and document together for deeper interaction at a higher computational cost. Promptriever is a bi-encoder.\n\n![](../notes/llm/rag/p3-images/slide_36.png)\n\n*([Timestamp: 00:09:10](https://youtu.be/YB3b-wPbSH8?t=550s))*\n\nThe main research question was how to enable fast, scalable bi-encoders to understand complex instructions. The missing ingredient was **training data**. Existing retrieval datasets like MSMARCO lack instructions because users don't type them into traditional search engines.\n\n![](../notes/llm/rag/p3-images/slide_39.png)\n\n*([Timestamp: 00:10:07](https://youtu.be/YB3b-wPbSH8?t=607s))*\n\nThis slide illustrates the process of generating the training data, starting with a standard query. The process uses an existing query-document pair from a standard dataset and uses an LLM to generate a detailed **instruction** that makes the relevance criteria more specific. A crucial part was also generating **instruction negatives** - documents that are relevant to the query but irrelevant to the *instruction*.\n\n## Promptriever Evaluation Results\n\n![](../notes/llm/rag/p3-images/slide_48.png)\n\n*([Timestamp: 00:12:36](https://youtu.be/YB3b-wPbSH8?t=756s))*\n\nOn FollowIR, the baseline RepLLaMA (and all prior embedding models) scored negatively, performing *worse* when given an instruction. Promptriever is the first to achieve a positive score, demonstrating that bi-encoders can learn to follow instructions.\n\n![](../notes/llm/rag/p3-images/slide_57.png)\n\n*([Timestamp: 00:13:58](https://youtu.be/YB3b-wPbSH8?t=838s))*\n\nWithout a prompt, Promptriever performs comparably to the RepLLaMA baseline, showing that instruction-following capabilities don't hurt performance on traditional tasks.\n\n![](../notes/llm/rag/p3-images/slide_58.png)\n\n*([Timestamp: 00:14:13](https://youtu.be/YB3b-wPbSH8?t=853s))*\n\nWhen a generic instruction is added, Promptriever's performance increases significantly, while the baseline's degrades slightly. This demonstrates that Promptriever's retrieval strategy can be controlled with natural language - a form of **zero-shot hyperparameter optimization via prompting**.\n\n## Rank1: Reasoning-Based Reranking\n\n![](../notes/llm/rag/p3-images/slide_67.png)\n\n*([Timestamp: 00:16:08](https://youtu.be/YB3b-wPbSH8?t=968s))*\n\nThe focus now shifts to Rank1, the reasoning-based model. The associated paper's title is \"Rank1: Test-Time Compute for Information Retrieval,\" highlighting its focus on reasoning in the reranking stage.\n\n![](../notes/llm/rag/p3-images/slide_73.png)\n\n*([Timestamp: 00:17:08](https://youtu.be/YB3b-wPbSH8?t=1028s))*\n\nThis slide shows what the reasoning process looks like in information retrieval. Given a query and a document, the model generates a detailed reasoning trace, identifying key phrases, analyzing the relationship between query and document, and questioning its own interpretations before arriving at a final judgment.\n\n![](../notes/llm/rag/p3-images/slide_76.png)\n\n*([Timestamp: 00:18:50](https://youtu.be/YB3b-wPbSH8?t=1130s))*\n\nThis slide shows Rank1's reasoning on a LeetCode problem. Asked to find a similar problem, it correctly identifies the core \"two-pointer approach\" algorithm in the provided document and recognizes that the candidate document also uses the same technique, demonstrating a deep, algorithmic level of understanding.\n\n## Rank1 Performance Results\n\n![](../notes/llm/rag/p3-images/slide_80.png)\n\n*([Timestamp: 00:19:38](https://youtu.be/YB3b-wPbSH8?t=1178s))*\n\nThe evaluation covers tasks testing reasoning (BRIGHT), negation (NevIR), and instruction following (mFollowIR). The baseline model, RankLLaMA, was trained on **10 times more data** than Rank1. Despite being trained on far less data, Rank1 nearly doubles the performance of the baseline on the BRIGHT reasoning benchmark.\n\n![](../notes/llm/rag/p3-images/slide_84.png)\n\n*([Timestamp: 00:20:16](https://youtu.be/YB3b-wPbSH8?t=1216s))*\n\nTo isolate the impact of the reasoning chain, they compared training the same model on the same data, with and without the \"thinking\" part of the training examples. The results show that training the model to generate the reasoning chain leads to a massive 10-point gain in performance. The act of \"thinking\" itself unlocks these advanced capabilities.\n\n## Finding Novel Relevant Documents\n\n![](../notes/llm/rag/p3-images/slide_87.png)\n\n*([Timestamp: 00:20:44](https://youtu.be/YB3b-wPbSH8?t=1244s))*\n\nThey were surprised by low scores on the DL19/DL20 datasets, discovering their model was finding many documents that had never been judged by human annotators because older systems had never retrieved them.\n\n![](../notes/llm/rag/p3-images/slide_91.png)\n\n*([Timestamp: 00:21:39](https://youtu.be/YB3b-wPbSH8?t=1299s))*\n\nReasoning-based models are not just improving scores on old benchmarks; they are **finding new, relevant documents** that previous systems missed. This also suggests the IR community should move on from older evaluation datasets as they may not be equipped to measure modern model capabilities.\n\n## Chapter Takeaways\n\n![](../notes/llm/rag/p3-images/slide_97.png)\n\n*([Timestamp: 00:22:37](https://youtu.be/YB3b-wPbSH8?t=1357s))*\n\nOrion concludes that the overall goal is to create IR systems that work like LLMs, capable of handling queries that combine topic, style, and behavioral instructions.\n\nKey insights from this work:\n\n1. **Promptriever** enables fast bi-encoder retrievers to follow complex instructions through specialized training data\n2. **Rank1** uses reasoning chains to achieve superior performance on complex retrieval tasks\n3. Both models demonstrate that LLM capabilities can be successfully integrated into retrieval systems\n4. Reasoning-based models discover novel relevant documents that traditional systems miss\n\nNew retrievers can directly benefit from rapid LLM advancements. As LLMs get better at reasoning and instruction following, so will the retrieval systems built upon them.\n\n\n\n## Video\n\nHere is the full video:\n\n{{< video https://youtu.be/YB3b-wPbSH8 >}}\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"css":["custom.css"],"output-file":"03-reasoning.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"include-in-header":[{"text":"\\usepackage{fancyhdr}\n\\pagestyle{fancy}\n\\fancyhf{}\n\\fancyhead[C]{\\textcolor{brown!50!red!30!black}{AI Evals Course: 25\\% off at \\href{https://bit.ly/evals-ai}{bit.ly/evals-ai}}}\n\\fancyfoot[C]{\\thepage}\n% Fix text justification issues\n\\usepackage{microtype}\n\\tolerance=9999\n\\emergencystretch=10pt\n\\hyphenpenalty=10000\n\\exhyphenpenalty=100\n\\raggedbottom\n% Use ragged right to avoid justification issues\n\\usepackage{ragged2e}\n\\RaggedRight\n% Reduce spacing around images significantly\n\\setlength{\\belowcaptionskip}{1pt}\n\\setlength{\\abovecaptionskip}{3pt}\n\\setlength{\\intextsep}{4pt plus 1pt minus 1pt}\n\\setlength{\\textfloatsep}{6pt plus 1pt minus 1pt}\n"}],"output-file":"03-reasoning.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"documentclass":"scrbook","classoption":["oneside","openany"],"colorlinks":true,"linkcolor":"blue","urlcolor":"blue","citecolor":"blue"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}