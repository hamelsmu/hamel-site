{"title":"Context Rot: When Long Context Windows Fail","markdown":{"yaml":{"title":"Context Rot: When Long Context Windows Fail","description":"Kelly Hong from Chroma explains 'Context Rot,' a phenomenon where LLM performance degrades with longer inputs, and why thoughtful context engineering is critical for reliable AI applications.","image":"p6-images/crot_cover.png","page-navigation":true,"date":"2025-09-10"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n*Based on a presentation by Kelly Hong*\n\n[Kelly Hong](https://www.linkedin.com/in/kellyshong/){target=\"_blank\"} is a researcher at Chroma who investigates how LLMs handle long-context inputs. Despite marketing claims that models with 1M+ token context windows make RAG obsolete, Kelly's [research](https://research.trychroma.com/context-rot){target=\"_blank\"} reveals that performance degrades significantly as context length increases—a phenomenon she coined \"Context Rot.\" Her experiments across 18 state-of-the-art models show that simply having information in context isn't enough; how that information is presented matters critically for reliable AI applications.\n\n\n![Title Slide](p6-images/slide_1.png)\n\n*([Timestamp: 00:00:00](https://youtu.be/3s_N60u0jEY?t=0s){target=\"_blank\"})*\n\nThis slide introduces the concept of \"Context Rot,\" a term coined by Chroma to describe how an LLM's performance becomes increasingly unreliable as the length of its input context grows. The research evaluates 18 state-of-the-art LLMs and finds that, contrary to the assumption of uniform context processing, performance degrades significantly with longer inputs.\n\n## The Long Context Promise\n\n![The Rise of Long Context Windows](p6-images/slide_2.png)\n\n*([Timestamp: 01:47](https://youtu.be/3s_N60u0jEY?t=107s){target=\"_blank\"})*\n\nMajor LLM providers prominently advertise massive context windows—often 1 million tokens or more—as a key feature of their frontier models like Gemini, Claude, and GPT-4.1. This marketing suggests that models can effectively process and utilize vast amounts of information.\n\n![The Common Assumption: More Context is Better](p6-images/slide_3.png)\n\n*([Timestamp: 02:07](https://youtu.be/3s_N60u0jEY?t=127s){target=\"_blank\"})*\n\nThe availability of large context windows has led to the common assumption that providing more context is always beneficial. This has inspired new use cases, such as large-scale code analysis and extensive document synthesis. Benchmarks like the \"needle in a haystack\" test, which often show near-perfect retrieval accuracy across the entire context window, appear to reinforce this assumption, creating a potentially misleading picture of model capabilities.\n\n## Limitations of Needle in a Haystack\n\n![Explaining the \"Needle in a Haystack\" (NIAH) Benchmark](p6-images/slide_4.png)\n\n*([Timestamp: 03:33](https://youtu.be/3s_N60u0jEY?t=213s){target=\"_blank\"})*\n\nThe Needle in a Haystack (NIAH) test is a simple retrieval task where a specific fact (the \"needle\") is placed within a long document (the \"haystack\"), and the model is asked to retrieve it. Kelly explains that this benchmark primarily assesses direct **lexical matching**. As seen in the example, the query and the needle share many of the same words (\"best writing advice,\" \"college classmate\"). This makes the task relatively easy and not representative of real-world scenarios, which often require more complex **semantic** understanding where direct word overlap is minimal.\n\n## Experiment 1: Semantic vs. Lexical Matching\n\n![Experiment 1: Adding Ambiguity (Semantic vs. Lexical Matching)](p6-images/slide_5.png)\n\n*([Timestamp: 04:49](https://youtu.be/3s_N60u0jEY?t=289s){target=\"_blank\"})*\n\nTo test performance on more realistic tasks, Chroma's first experiment introduced ambiguity. They compared a **lexical matching** task (similar to the original NIAH) with a **semantic matching** task, where the answer contained the same core information but was phrased differently, requiring the model to understand meaning beyond direct word overlap. The results show a clear trend: while performance on lexical matching remains relatively high, performance on the more complex semantic matching task degrades significantly as the input context grows longer.\n\n![Implications of Ambiguity in Real-World Applications](p6-images/slide_6.png)\n\n*([Timestamp: 08:08](https://youtu.be/3s_N60u0jEY?t=488s){target=\"_blank\"})*\n\nThis slide illustrates the real-world implications of the previous experiment using a financial report analysis example. A user is unlikely to know the exact phrasing in a document to formulate a perfect lexical query. Instead, they will ask a more ambiguous, semantic question like \"How is our overseas expansion going?\" This requires the model to connect \"overseas expansion\" to specific countries and revenue figures. As Experiment 1 showed, this is precisely the kind of task where performance degrades with longer contexts.\n\n## Experiment 2: The Impact of Distractors\n\n![Experiment 2: Adding Distractors](p6-images/slide_7.png)\n\n*([Timestamp: 09:39](https://youtu.be/3s_N60u0jEY?t=579s){target=\"_blank\"})*\n\nThe second experiment investigates how performance is affected by **distractors**—pieces of information that are semantically similar to the correct answer but are incorrect. In the example, the correct \"needle\" is writing advice from a \"college classmate.\" The distractors include similar advice from a \"college professor\" or advice about writing essays in different styles. These distractors mimic the kind of noise often found in real-world documents.\n\n![Visualizing the Distractor Setup](p6-images/slide_8.png)\n\nThis slide provides a simple visual model of the experiment. The researchers tested the LLM's performance under three conditions: with no distractors, with one distractor, and with four distractors placed in the context alongside the correct needle.\n\n![Results: Performance Degrades with More Distractors](p6-images/slide_9.png)\n\n*([Timestamp: 11:00](https://youtu.be/3s_N60u0jEY?t=660s){target=\"_blank\"})*\n\nThe results of the distractor experiment show two clear trends. First, across all model groups, performance degrades as the input length increases. Second, performance also degrades as the number of distractors increases. The combination of long context and distracting information proves particularly challenging for LLMs, causing a significant drop in accuracy.\n\n![Implications of Distractors in Domain-Specific Contexts](p6-images/slide_10.png)\n\n*([Timestamp: 11:43](https://youtu.be/3s_N60u0jEY?t=703s){target=\"_blank\"})*\n\nThis experiment is highly relevant to real-world applications, especially in domain-specific contexts like finance or law. Documents in these fields often contain highly similar, templated information where only small details (like a year or a name) differ. These similar pieces of information act as natural distractors, making it difficult for the model to retrieve the correct fact, a problem that is exacerbated by longer contexts.\n\n![Analyzing Failure Modes: Model Hallucinations vs. Abstention](p6-images/slide_11.png)\n\n*([Timestamp: 12:55](https://youtu.be/3s_N60u0jEY?t=775s){target=\"_blank\"})*\n\nWhen the models failed in the 4-distractor condition, the researchers analyzed *how* they failed. A key finding was that models often **hallucinate** by confidently providing an answer based on one of the distractors, rather than **abstaining** (stating \"I don't know\"). This tendency varies by model family: Claude models are more likely to abstain when uncertain, whereas GPT models have the highest rate of hallucination, confidently returning an incorrect answer.\n\n## Experiment 3: Context Structure Matters\n\n![Experiment 3: Shuffling Haystack Content](p6-images/slide_12.png)\n\n*([Timestamp: 14:12](https://youtu.be/3s_N60u0jEY?t=852s){target=\"_blank\"})*\n\nThis experiment tested whether models process context in a structured, order-sensitive manner. A \"needle\" (a sentence about writing advice) was placed in a coherent essay. Because the needle disrupts the essay's logical flow, it stands out. The same needle was also placed in a \"haystack\" of randomly shuffled, unrelated sentences, where it should logically blend in more. The hypothesis was that the model would find it easier to retrieve the needle from the coherent essay where it was an anomaly.\n\n![Surprising Results: Models Perform Better on Shuffled Context](p6-images/slide_13.png)\n\n*([Timestamp: 15:34](https://youtu.be/3s_N60u0jEY?t=934s){target=\"_blank\"})*\n\nCounter-intuitively, the results showed that models performed slightly *better* when the haystack was randomly shuffled. This surprising finding suggests that LLMs do not necessarily process context in the linear, structured way humans do and that a disruption in logical flow can actually make a key piece of information harder, not easier, to find.\n\n## Experiment 4: Conversational Memory\n\n![Experiment 4: Conversational Memory](p6-images/slide_14.png)\n\n*([Timestamp: 17:54](https://youtu.be/3s_N60u0jEY?t=1074s){target=\"_blank\"})*\n\nThis experiment tested conversational memory using the LongMemEval benchmark. Models were tested under two conditions: a \"focused\" condition with only the relevant conversational history (around 100 tokens), and a \"full\" condition where the context was padded with irrelevant conversations up to 120k tokens. The results clearly show that all Claude models perform significantly better in the focused condition, demonstrating that irrelevant information degrades performance quickly.\n\n## Experiment 5: Simple Tasks Aren't Immune\n\n![Experiment 5: Text Replication Task](p6-images/slide_15.png)\n\n*([Timestamp: 19:20](https://youtu.be/3s_N60u0jEY?t=1160s){target=\"_blank\"})*\n\nThis experiment involved a very simple task: replicating a given text of repeated words. Despite the simplicity, all models showed a significant drop in performance as the input length increased. Some models exhibited strange failure modes; for example, at long input lengths, Claude models would refuse to generate the output, citing concerns about copyrighted material, while Gemini models would produce completely random outputs.\n\n## Key Takeaways\n\n![Key Takeaways](p6-images/slide_16.png)\n\n*([Timestamp: 20:15](https://youtu.be/3s_N60u0jEY?t=1215s){target=\"_blank\"})*\n\nThe research provides three takeaways:\n\n1.  LLM performance is not uniform across input lengths, even for simple tasks.\n2.  Simply having the right information in the context is not enough; *how* that information is presented matters significantly.\n3.  As a result, thoughtful **context engineering** is critical for building reliable AI applications.\n\n## Practical Solutions: Context Engineering\n\n![Context Engineering Example: Orchestrator and Subagents](p6-images/slide_17.png)\n\n*([Timestamp: 21:07](https://youtu.be/3s_N60u0jEY?t=1267s){target=\"_blank\"})*\n\nKelly provides a practical example of context engineering for a coding agent with a long-running task.\n\n- **Naive Approach:** Append the entire conversation history, including every tool call and output, to the context. This causes the context to grow quickly and become bloated with irrelevant information (e.g., the full content of a file read), leading to context rot.\n- **Better Approach:** Use a main \"orchestrator\" agent that breaks the task into subtasks and spawns \"subagents\" for each one. Each subagent operates with its own clean, focused context. It completes its subtask and returns only the most relevant information to the orchestrator, which maintains a concise, filtered history. This prevents context overload and improves reliability.\n\n## Further Resources\n\n![Further Reading](p6-images/slide_18.png)\n\n*([Timestamp: 22:47](https://youtu.be/3s_N60u0jEY?t=1367s){target=\"_blank\"})*\n\nThe presentation concludes by directing the audience to the full technical report and other related research on Chroma's website, [research.trychroma.com](https://research.trychroma.com/).\n\n## Q&A Session\n\n- **Is the Needle in a Haystack (NIAH) benchmark pointless?**\n    *([Timestamp: 06:54](https://youtu.be/3s_N60u0jEY?t=414s){target=\"_blank\"})*\n    It's not pointless, but its utility has diminished. It was useful for evaluating older models, which did show performance degradation on the task. However, modern frontier models can now perform very well on this simple, lexically-driven task, which makes the benchmark unrepresentative of real-world use cases that require deeper semantic reasoning.\n\n- **Did the research find that one model consistently resists context rot better than others across all tasks?**\n    *([Timestamp: 23:57](https://youtu.be/3s_N60u0jEY?t=1437s){target=\"_blank\"})*\n    No, performance was \"all over the place\" and highly task-dependent. There was no single model that ranked first across all experiments. For example, Claude Sonnet 4 performed best on the repeated words task, while GPT-4.1 was the top performer on the Needle in a Haystack task. Each model has different strengths, and no model currently excels at all long-context tasks.\n\n- **What is your advice for developers trying to find and mitigate context rot in their applications?**\n    *([Timestamp: 27:32](https://youtu.be/3s_N60u0jEY?t=1652s){target=\"_blank\"})*\n    Start by qualitatively analyzing your system. Run a few examples with both short, focused context and long context bloated with irrelevant information. Compare the outputs: what did the model miss with the long context? What irrelevant information could be removed? There's no single, generalizable solution, as optimal context engineering is highly application-dependent. A good starting point is to carefully examine the data you're providing to the model and how you can make it more concise and relevant.\n\n- **Prior research found a U-shaped retrieval curve, where information at the very beginning and very end of the context is recalled best. Does that still hold true?**\n    *([Timestamp: 29:06](https://youtu.be/3s_N60u0jEY?t=1746s){target=\"_blank\"})*\n    In Chroma's experiments, they did not observe this U-shaped pattern. They tested placing the \"needle\" at various positions throughout the context—from the beginning to the middle to the end—and found no consistent performance advantage for any particular position. While putting important information at the start or end is a common piece of advice, this research suggests it may not be a reliable solution for mitigating context rot.","srcMarkdownNoYaml":"\n\n*Based on a presentation by Kelly Hong*\n\n[Kelly Hong](https://www.linkedin.com/in/kellyshong/){target=\"_blank\"} is a researcher at Chroma who investigates how LLMs handle long-context inputs. Despite marketing claims that models with 1M+ token context windows make RAG obsolete, Kelly's [research](https://research.trychroma.com/context-rot){target=\"_blank\"} reveals that performance degrades significantly as context length increases—a phenomenon she coined \"Context Rot.\" Her experiments across 18 state-of-the-art models show that simply having information in context isn't enough; how that information is presented matters critically for reliable AI applications.\n\n## Introduction\n\n![Title Slide](p6-images/slide_1.png)\n\n*([Timestamp: 00:00:00](https://youtu.be/3s_N60u0jEY?t=0s){target=\"_blank\"})*\n\nThis slide introduces the concept of \"Context Rot,\" a term coined by Chroma to describe how an LLM's performance becomes increasingly unreliable as the length of its input context grows. The research evaluates 18 state-of-the-art LLMs and finds that, contrary to the assumption of uniform context processing, performance degrades significantly with longer inputs.\n\n## The Long Context Promise\n\n![The Rise of Long Context Windows](p6-images/slide_2.png)\n\n*([Timestamp: 01:47](https://youtu.be/3s_N60u0jEY?t=107s){target=\"_blank\"})*\n\nMajor LLM providers prominently advertise massive context windows—often 1 million tokens or more—as a key feature of their frontier models like Gemini, Claude, and GPT-4.1. This marketing suggests that models can effectively process and utilize vast amounts of information.\n\n![The Common Assumption: More Context is Better](p6-images/slide_3.png)\n\n*([Timestamp: 02:07](https://youtu.be/3s_N60u0jEY?t=127s){target=\"_blank\"})*\n\nThe availability of large context windows has led to the common assumption that providing more context is always beneficial. This has inspired new use cases, such as large-scale code analysis and extensive document synthesis. Benchmarks like the \"needle in a haystack\" test, which often show near-perfect retrieval accuracy across the entire context window, appear to reinforce this assumption, creating a potentially misleading picture of model capabilities.\n\n## Limitations of Needle in a Haystack\n\n![Explaining the \"Needle in a Haystack\" (NIAH) Benchmark](p6-images/slide_4.png)\n\n*([Timestamp: 03:33](https://youtu.be/3s_N60u0jEY?t=213s){target=\"_blank\"})*\n\nThe Needle in a Haystack (NIAH) test is a simple retrieval task where a specific fact (the \"needle\") is placed within a long document (the \"haystack\"), and the model is asked to retrieve it. Kelly explains that this benchmark primarily assesses direct **lexical matching**. As seen in the example, the query and the needle share many of the same words (\"best writing advice,\" \"college classmate\"). This makes the task relatively easy and not representative of real-world scenarios, which often require more complex **semantic** understanding where direct word overlap is minimal.\n\n## Experiment 1: Semantic vs. Lexical Matching\n\n![Experiment 1: Adding Ambiguity (Semantic vs. Lexical Matching)](p6-images/slide_5.png)\n\n*([Timestamp: 04:49](https://youtu.be/3s_N60u0jEY?t=289s){target=\"_blank\"})*\n\nTo test performance on more realistic tasks, Chroma's first experiment introduced ambiguity. They compared a **lexical matching** task (similar to the original NIAH) with a **semantic matching** task, where the answer contained the same core information but was phrased differently, requiring the model to understand meaning beyond direct word overlap. The results show a clear trend: while performance on lexical matching remains relatively high, performance on the more complex semantic matching task degrades significantly as the input context grows longer.\n\n![Implications of Ambiguity in Real-World Applications](p6-images/slide_6.png)\n\n*([Timestamp: 08:08](https://youtu.be/3s_N60u0jEY?t=488s){target=\"_blank\"})*\n\nThis slide illustrates the real-world implications of the previous experiment using a financial report analysis example. A user is unlikely to know the exact phrasing in a document to formulate a perfect lexical query. Instead, they will ask a more ambiguous, semantic question like \"How is our overseas expansion going?\" This requires the model to connect \"overseas expansion\" to specific countries and revenue figures. As Experiment 1 showed, this is precisely the kind of task where performance degrades with longer contexts.\n\n## Experiment 2: The Impact of Distractors\n\n![Experiment 2: Adding Distractors](p6-images/slide_7.png)\n\n*([Timestamp: 09:39](https://youtu.be/3s_N60u0jEY?t=579s){target=\"_blank\"})*\n\nThe second experiment investigates how performance is affected by **distractors**—pieces of information that are semantically similar to the correct answer but are incorrect. In the example, the correct \"needle\" is writing advice from a \"college classmate.\" The distractors include similar advice from a \"college professor\" or advice about writing essays in different styles. These distractors mimic the kind of noise often found in real-world documents.\n\n![Visualizing the Distractor Setup](p6-images/slide_8.png)\n\nThis slide provides a simple visual model of the experiment. The researchers tested the LLM's performance under three conditions: with no distractors, with one distractor, and with four distractors placed in the context alongside the correct needle.\n\n![Results: Performance Degrades with More Distractors](p6-images/slide_9.png)\n\n*([Timestamp: 11:00](https://youtu.be/3s_N60u0jEY?t=660s){target=\"_blank\"})*\n\nThe results of the distractor experiment show two clear trends. First, across all model groups, performance degrades as the input length increases. Second, performance also degrades as the number of distractors increases. The combination of long context and distracting information proves particularly challenging for LLMs, causing a significant drop in accuracy.\n\n![Implications of Distractors in Domain-Specific Contexts](p6-images/slide_10.png)\n\n*([Timestamp: 11:43](https://youtu.be/3s_N60u0jEY?t=703s){target=\"_blank\"})*\n\nThis experiment is highly relevant to real-world applications, especially in domain-specific contexts like finance or law. Documents in these fields often contain highly similar, templated information where only small details (like a year or a name) differ. These similar pieces of information act as natural distractors, making it difficult for the model to retrieve the correct fact, a problem that is exacerbated by longer contexts.\n\n![Analyzing Failure Modes: Model Hallucinations vs. Abstention](p6-images/slide_11.png)\n\n*([Timestamp: 12:55](https://youtu.be/3s_N60u0jEY?t=775s){target=\"_blank\"})*\n\nWhen the models failed in the 4-distractor condition, the researchers analyzed *how* they failed. A key finding was that models often **hallucinate** by confidently providing an answer based on one of the distractors, rather than **abstaining** (stating \"I don't know\"). This tendency varies by model family: Claude models are more likely to abstain when uncertain, whereas GPT models have the highest rate of hallucination, confidently returning an incorrect answer.\n\n## Experiment 3: Context Structure Matters\n\n![Experiment 3: Shuffling Haystack Content](p6-images/slide_12.png)\n\n*([Timestamp: 14:12](https://youtu.be/3s_N60u0jEY?t=852s){target=\"_blank\"})*\n\nThis experiment tested whether models process context in a structured, order-sensitive manner. A \"needle\" (a sentence about writing advice) was placed in a coherent essay. Because the needle disrupts the essay's logical flow, it stands out. The same needle was also placed in a \"haystack\" of randomly shuffled, unrelated sentences, where it should logically blend in more. The hypothesis was that the model would find it easier to retrieve the needle from the coherent essay where it was an anomaly.\n\n![Surprising Results: Models Perform Better on Shuffled Context](p6-images/slide_13.png)\n\n*([Timestamp: 15:34](https://youtu.be/3s_N60u0jEY?t=934s){target=\"_blank\"})*\n\nCounter-intuitively, the results showed that models performed slightly *better* when the haystack was randomly shuffled. This surprising finding suggests that LLMs do not necessarily process context in the linear, structured way humans do and that a disruption in logical flow can actually make a key piece of information harder, not easier, to find.\n\n## Experiment 4: Conversational Memory\n\n![Experiment 4: Conversational Memory](p6-images/slide_14.png)\n\n*([Timestamp: 17:54](https://youtu.be/3s_N60u0jEY?t=1074s){target=\"_blank\"})*\n\nThis experiment tested conversational memory using the LongMemEval benchmark. Models were tested under two conditions: a \"focused\" condition with only the relevant conversational history (around 100 tokens), and a \"full\" condition where the context was padded with irrelevant conversations up to 120k tokens. The results clearly show that all Claude models perform significantly better in the focused condition, demonstrating that irrelevant information degrades performance quickly.\n\n## Experiment 5: Simple Tasks Aren't Immune\n\n![Experiment 5: Text Replication Task](p6-images/slide_15.png)\n\n*([Timestamp: 19:20](https://youtu.be/3s_N60u0jEY?t=1160s){target=\"_blank\"})*\n\nThis experiment involved a very simple task: replicating a given text of repeated words. Despite the simplicity, all models showed a significant drop in performance as the input length increased. Some models exhibited strange failure modes; for example, at long input lengths, Claude models would refuse to generate the output, citing concerns about copyrighted material, while Gemini models would produce completely random outputs.\n\n## Key Takeaways\n\n![Key Takeaways](p6-images/slide_16.png)\n\n*([Timestamp: 20:15](https://youtu.be/3s_N60u0jEY?t=1215s){target=\"_blank\"})*\n\nThe research provides three takeaways:\n\n1.  LLM performance is not uniform across input lengths, even for simple tasks.\n2.  Simply having the right information in the context is not enough; *how* that information is presented matters significantly.\n3.  As a result, thoughtful **context engineering** is critical for building reliable AI applications.\n\n## Practical Solutions: Context Engineering\n\n![Context Engineering Example: Orchestrator and Subagents](p6-images/slide_17.png)\n\n*([Timestamp: 21:07](https://youtu.be/3s_N60u0jEY?t=1267s){target=\"_blank\"})*\n\nKelly provides a practical example of context engineering for a coding agent with a long-running task.\n\n- **Naive Approach:** Append the entire conversation history, including every tool call and output, to the context. This causes the context to grow quickly and become bloated with irrelevant information (e.g., the full content of a file read), leading to context rot.\n- **Better Approach:** Use a main \"orchestrator\" agent that breaks the task into subtasks and spawns \"subagents\" for each one. Each subagent operates with its own clean, focused context. It completes its subtask and returns only the most relevant information to the orchestrator, which maintains a concise, filtered history. This prevents context overload and improves reliability.\n\n## Further Resources\n\n![Further Reading](p6-images/slide_18.png)\n\n*([Timestamp: 22:47](https://youtu.be/3s_N60u0jEY?t=1367s){target=\"_blank\"})*\n\nThe presentation concludes by directing the audience to the full technical report and other related research on Chroma's website, [research.trychroma.com](https://research.trychroma.com/).\n\n## Q&A Session\n\n- **Is the Needle in a Haystack (NIAH) benchmark pointless?**\n    *([Timestamp: 06:54](https://youtu.be/3s_N60u0jEY?t=414s){target=\"_blank\"})*\n    It's not pointless, but its utility has diminished. It was useful for evaluating older models, which did show performance degradation on the task. However, modern frontier models can now perform very well on this simple, lexically-driven task, which makes the benchmark unrepresentative of real-world use cases that require deeper semantic reasoning.\n\n- **Did the research find that one model consistently resists context rot better than others across all tasks?**\n    *([Timestamp: 23:57](https://youtu.be/3s_N60u0jEY?t=1437s){target=\"_blank\"})*\n    No, performance was \"all over the place\" and highly task-dependent. There was no single model that ranked first across all experiments. For example, Claude Sonnet 4 performed best on the repeated words task, while GPT-4.1 was the top performer on the Needle in a Haystack task. Each model has different strengths, and no model currently excels at all long-context tasks.\n\n- **What is your advice for developers trying to find and mitigate context rot in their applications?**\n    *([Timestamp: 27:32](https://youtu.be/3s_N60u0jEY?t=1652s){target=\"_blank\"})*\n    Start by qualitatively analyzing your system. Run a few examples with both short, focused context and long context bloated with irrelevant information. Compare the outputs: what did the model miss with the long context? What irrelevant information could be removed? There's no single, generalizable solution, as optimal context engineering is highly application-dependent. A good starting point is to carefully examine the data you're providing to the model and how you can make it more concise and relevant.\n\n- **Prior research found a U-shaped retrieval curve, where information at the very beginning and very end of the context is recalled best. Does that still hold true?**\n    *([Timestamp: 29:06](https://youtu.be/3s_N60u0jEY?t=1746s){target=\"_blank\"})*\n    In Chroma's experiments, they did not observe this U-shaped pattern. They tested placing the \"needle\" at various positions throughout the context—from the beginning to the middle to the end—and found no consistent performance advantage for any particular position. While putting important information at the start or end is a common piece of advice, this research suggests it may not be a reliable solution for mitigating context rot."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"css":["custom.css"],"output-file":"06-context-rot.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","theme":"cosmo","title":"Context Rot: When Long Context Windows Fail","description":"Kelly Hong from Chroma explains 'Context Rot,' a phenomenon where LLM performance degrades with longer inputs, and why thoughtful context engineering is critical for reliable AI applications.","image":"p6-images/crot_cover.png","page-navigation":true,"date":"2025-09-10"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"include-in-header":[{"text":"\\usepackage{fancyhdr}\n\\pagestyle{fancy}\n\\fancyhf{}\n\\fancyhead[C]{\\textcolor{brown!50!red!30!black}{AI Evals Course: 15\\% off at \\href{https://bit.ly/evals-ai}{bit.ly/evals-ai}}}\n\\fancyfoot[C]{\\thepage}\n% Fix text justification issues\n\\usepackage{microtype}\n\\tolerance=9999\n\\emergencystretch=10pt\n\\hyphenpenalty=10000\n\\exhyphenpenalty=100\n\\raggedbottom\n% Use ragged right to avoid justification issues\n\\usepackage{ragged2e}\n\\RaggedRight\n% Reduce spacing around images significantly\n\\setlength{\\belowcaptionskip}{1pt}\n\\setlength{\\abovecaptionskip}{3pt}\n\\setlength{\\intextsep}{4pt plus 1pt minus 1pt}\n\\setlength{\\textfloatsep}{6pt plus 1pt minus 1pt}\n"}],"output-file":"06-context-rot.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"documentclass":"scrbook","classoption":["oneside","openany"],"colorlinks":true,"linkcolor":"blue","urlcolor":"blue","citecolor":"blue","title":"Context Rot: When Long Context Windows Fail","description":"Kelly Hong from Chroma explains 'Context Rot,' a phenomenon where LLM performance degrades with longer inputs, and why thoughtful context engineering is critical for reliable AI applications.","image":"p6-images/crot_cover.png","page-navigation":true,"date":"2025-09-10"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}