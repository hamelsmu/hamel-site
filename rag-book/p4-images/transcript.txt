Going Further: Late Interaction Beats Single Vector Limits
===

[00:00:00] Antoine Chaffin: hello everyone. My name is Anton Japan and I am a research engineer at Lighton. And today I will detail some of the limits of a single vector [00:00:10] search that have been highlighted. By recent usages and evaluations. And then I will introduce multi-vector models, also known as late interaction models [00:00:20] and all they can overcome these challenges.

And to finish, I will briefly present the pilot library that allows you to use as a late interaction model, so you [00:00:30] can also benefit from that. So first really quickly, a few words about me. before joining Lighton, I did an engineering school followed by a [00:00:40] PhD on the subject of multimodal misinformation detection.

During which I studied multimodal information retrieval as well as a guiding generative [00:00:50] model in order to generate better synthetic data. And so at Lighton, I focused more on information retrieval not happily through the scope of [00:01:00] encoders model. And late interaction which led me to being a co-creator of the modern bird encoder model that you might have heard of, as well as the [00:01:10] pilot library which I will introduce a bit later today.

I'm also still working a lot on multimodality, notably through the scope of creating OER [00:01:20] free rack pipeline. For example, with the creation of visual rear ranker, such as the mono coin model we released earlier this year. And so if you are [00:01:30] familiar with those topic, you might have seen me on Twitter where I spend quite a lot of time doing propaganda, this subject and also sheet posting a bit.[00:01:40] 

All right, so that's enough about me. Let's dive into today's subject. So I think you are all pretty familiar with the single vector search [00:01:50] also called d search but allows to search for relevant documents in the knowledge base based on the user query. And so essentially the query and the [00:02:00] documents are fed to a transformer model.

Preferably an encoder such as birds or even better it's modernized version, right? And the [00:02:10] transformer model will create a contextualized vector representation of all of the token in the sequence. So in the query and in the document. And once we [00:02:20] have all of these on embeddings for the query and for the documents, we will actually pull all of them into a single vector.

Through a pooling operations such as max pooling, [00:02:30] mean pulling, or just like taking the first vector of the sequence. And once we have a unique token for the query and a unique token for the document, we can compute [00:02:40] the similarity between the two using a simple, vector similarity operations, such as the cosign similarity.

And so the transformer model are [00:02:50] optimized to create vector but are highly similar if the document is relevant to the query and lower one, if the document is not relevant to the query. And so [00:03:00] we can then perform search by looking for the document, but as the closest vector to the input query from the user.

So I just presented those [00:03:10] single vector model, but I'm pretty sure you are all already very familiar with them as they became the go-to type of search for RAG Pipeline. And they can be [00:03:20] found in all the ragga two in two lines of code boiler plates. And they are very popular because they offer like great performance out of the box.

And [00:03:30] also there is a lot of different model available with different sizes, languages, and domains. So you really have a large range to choose from. And finally, [00:03:40] there are very well integrated now and they're very easy to deploy with one of the various vector TB provider. And APIs that can serve those embedding [00:03:50] so you can easily spin up something that works in just a few minutes.

So obviously as you are all very well aware evaluation is required [00:04:00] to assess the performance of the model. And so the Mtap leaderboard is a very nice resource, but centralized various benchmark results for various different model, [00:04:10] which allows you to select the model that fits your budget and should perform correctly on your domain quite easily.

So one of the most known [00:04:20] benchmark on ME, at least for English retrieval is the beer benchmark. And so it was originally introduced as a nitrogenous [00:04:30] benchmark composed of various domain. And so its propose was to broadly evaluate the performance of a model on various domain in order [00:04:40] to assert that the model was correctly working on a domain, but it wasn't necessarily trained for.

And so it helped a lot in the quest of [00:04:50] training better model. And by doing so, it became the main reference to reports when evaluating a retrieval result in English. However as stated [00:05:00] by the good out slow when a measure becomes a target, it ceases to be a good measure. And so by becoming the standard objective to beat in order to get the [00:05:10] poll number in the paper or in a blog post the benchmark have been gamified and know most of the top models of this leaderboard are actually over [00:05:20] fitted on this domain.

And they can perform pretty poorly in actual use cases. it became more important than ever to run your own evaluation on [00:05:30] your own data and use cases in order to make sure that the model actually perform correctly for you. So beside this, iCloud big from [00:05:40] of beer. Other aspects of evaluation have conducted to the lack of realization of some caveats of the existing model.

For example since most [00:05:50] model only supported an input length of 512 there were very few evaluation of long context capabilities for retrieval model. [00:06:00] And so with the new models supporting longer context length such as eight K, people thought they could just simply use those model to process larger documents.[00:06:10] 

But when you actually evaluate them correctly it appears pretty clearly that although you can feed there's a long document to this model. [00:06:20] The model do not actually support them correctly and the results are really poor. So obviously having evaluation allows to develop techniques to better [00:06:30] train the model to under such context.

For example, most of the existing evaluation benchmark focused on very Bri notion of similarity [00:06:40] where there is a direct lexile, or at the very least, semiotical overlap between the query and the target documents.

in modern use cases, [00:06:50] especially in agent workflow. We require more complex type of retrieval in which the definition of the relevance between the query and the target document [00:07:00] might require some form of reasoning. For example, when prompted with a mathematical problem, finding example that aren't the same problem but require [00:07:10] the same theorem to be solved might be very useful. Yet it is actually very difficult for existing retrieval model. if you're interested in this particular topic [00:07:20] I encourage you to watch Orient Talk from yesterday. It went way deeper into this specific subject, and the talk was really good.

And so yet again [00:07:30] when evaluating the existing model on this new capability, we observe actually very poor results compared to their usual performance on more traditional [00:07:40] benchmarks. And it actually have an hold for a very large model that shows like billion parameters model as the backbone [00:07:50] model.

One common thing that should be noted is that for both of this evaluation pm 25 which is a very simple lexical method, but do not leverage [00:08:00] any deep learning, is actually very competitive. And this also all for out of domain retrieval. Where BM 25 is to this [00:08:10] day very much used in addition to the dense search signal in order to enhance the auto domain capabilities.

And so this might very well be a [00:08:20] need as to why a single vector model are actually struggling. Indeed in the single vector padding we are pulling all the token into [00:08:30] a single one. And so no matter which type of pulling is used we are compressing a lot the information originally present in the document and the [00:08:40] queries.

During the training, the model will learn to keep the information that is helpful to differentiate between the positive documents from the negative documents for the put [00:08:50] query, and it'll actually iner the rest because it needs to compress the information.

And so it needs to save some space essentially. So for example if you train [00:09:00] a model on a movie review. And like all the queries are about the actors, the model will learn to focus on the information about the actor and it'll [00:09:10] actually do not encode any information about the plot the music, et cetera, et cetera.

And so this is why single vector model actually struggles a lot [00:09:20] out of domain because what if in production know the user ask a question about the plot. 

Sorry. We don't have the information and so the model won't be able [00:09:30] to find the relevant document for this query. And likewise what the model should encode if it's now used on like cooking recipe. There isn't [00:09:40] any information about actors in there. And so the model is lost and it doesn't really know what to encode as an information, and so because we have to learn to [00:09:50] compress the information, the model will actually learn one view of the documents. And unfortunately this view is not holistic at all. And so [00:10:00] obviously this is getting worse and worse with larger contexts because the model has to compress the information even more into this single representation.

And so it, it has to [00:10:10] make even more drastic choices and it can even be about, just like including one view of the document of a very large document. It is really challenging because the [00:10:20] representation is so compressed. And so likewise, my intuition about why single vector model struggles a lot on the reasoning intensive retrieval is because to [00:10:30] compress the information, the model has to learn one given notion of similarity.

And so it cannot be applied to a more abstract matching but isn't [00:10:40] present in the training data. So now you can see why BM 25 is actually very competitive on this aspect. Essentially, there is no [00:10:50] compression. So we keep all the information and this is very helpful when applied to new domain, long documents or reasoning intensive virtual.

[00:11:00] However, one of the main drawback of25 is that it's not learned. It actually perform enough matching between the query and the document tokens. [00:11:10] And so if there isn't a direct alignment between the two, for example, if they aren't in the same language or if it's a raw formulation using BM 25 [00:11:20] won't work very well, but know what if I told you that we actually have something that is close to BM 25.

But that is actually [00:11:30] leveraging deep learning. Disparing is known as a multi-vector search or a late interaction search. So the [00:11:40] start of the processing is the very same with a single vector model. We will feed the query and the documents to a transformer model, and it'll create a [00:11:50] contextualized vector representation of all the token.

But this time, instead of pulling all the vector into a single one and computing the IGN similarity, [00:12:00] we will actually keep all of the vectors. And we will use the maximum operator to compute the similarity that is the sum for [00:12:10] all of the query token to its maximum similarity with one of the document vector.

So one fair claim would be that if compression [00:12:20] is the only issue, we could just simply use a very large embedding vector instead of like multiple ones.

But actually this is a bit more [00:12:30] complicated than that because if you only use one vector every time, you will be making an update. During the training, you will actually change all the [00:12:40] representation. And so this makes the training very noisy because all the updates will be conflicting.

For example the first update will add [00:12:50] information about the actors of a movie, and then a subsequent one will add information about the plot of the movie. But by doing so, it'll have to remove parts [00:13:00] of the existing information about the actors because it needs some space to add this information.

So the maximum operator, in [00:13:10] addition to not require to pull the information, it has a built-in mechanism, the maximum operator. And so the update will only update the [00:13:20] information of the document related to the input query. So you can see that as. For the first query, we will be only updating the token [00:13:30] related to the actors.

And for the second query we will only update the token about the plot. And so the intuition is that even if the [00:13:40] model is totally out of domain we can still fall back on a, like soft lexical matching which is very close to BM 25, but without the [00:13:50] R matching part. So this is the theory but how does it translate empirically?

So one of the earliest recognized perk of Colbert model, [00:14:00] so late interaction model is that there are very strong out of domain compared to usual dense model and. I try to illustrate that with a few examples. [00:14:10] So where you can see that Colbert model dominates out of domain evaluation, sometimes even surpassing dance model evaluated in domain.

And [00:14:20] this can be pushed even further because they have been report of Colbert model being bold to generalize to. Languages that haven't been trained on. [00:14:30] And so it is a bit hard to correctly evaluate and illustrate this generalization capabilities because essentially there is an infinite number of [00:14:40] possible out of domain definition.

But really I cannot empathize enough how many practitioner reported that covert model generalizing [00:14:50] way better to their specific use case. So again, the only thing I can say is by I encourage you to just try out this model on your data [00:15:00] and CO it performs compared to dance model.

If you compare the recent GT modern Culbert, but leverage late interaction and you [00:15:10] compare it to the best in class single vector model on the long band benchmark, which is a benchmark meant to evaluate the capabilities of long context retrieval you [00:15:20] can see that it is strongly outperforming all of best dense model.

And what's more even more impressive is that it does so by being trained only on [00:15:30] Ms. Marco with a maximum document length set to 300. And so in addition to be like insanely good at long context retrieval it does [00:15:40] so by only generalizing from short context training. And so training specifically for long context retrieval should maybe noticeably improve [00:15:50] the result further.

So on reasoning intensive retrieval the observation are the same if not even more impressive because reason modern [00:16:00] corporate which is trained on the reason higher data. It actually outperform all the existing model up to 7 billion parameter model while being [00:16:10] only 150 millions parameter.

And so this small model is actually beating model that are 45 times. Bigger than Tanium. [00:16:20] And it is actually even competitive with the ER model itself which is a model that is trained on the exact same data. And [00:16:30] what's really interesting is that it does so actually just.

Thanks to being a late interaction model because when you compare it to a single vector model that is [00:16:40] trained in the same setup, so the same backbone, the same data, you can see that the difference in performance is actually pretty huge. So to conclude on the [00:16:50] many advantages of the late interaction model, I want to mention an additional perks that comes for free.

Essentially, since the maximum operator [00:17:00] is doing alignment between the query token and the document token, we can actually see what match, and thus we have additional explainability [00:17:10] allows for exploration and debugging. Which is very helpful to show the user when you are doing right whereas when you are doing a single vector search, you only have [00:17:20] one global matching score and you don't have any granularity.

besides this additional explainability this granular matching. Also allows to [00:17:30] identify subhan, but matched in the bigger document. And thus we can only use this smaller subhan as the input of the LLM in a rack setting [00:17:40] in order to augment the precision If all of this model are so powerful, why is it like everyone just using them? And to me there is [00:17:50] a few factor explaining this. So first, obviously storing multiple vector instead of one is, and this port is actively [00:18:00] being sold. But right now when using like adapted indexes coupled to quantization and over footprint reduction techniques the com becomes actually quite [00:18:10] acceptable.

Second, because the search is different from the standard cosign search. The Vector DB provider did not necessarily support it. But this isn't [00:18:20] true anymore as most of the well-known providers support it. And finally, maybe I'm a bit biased, but to me, one of the most important reason. Is [00:18:30] that there was a lack of accessible tool to train, use and experiment with this model because to me one of the main reason of the popularity [00:18:40] of single dense model is sentence transformer.

And so to fix this with Rafael Ti we decided to build a pilot, which is a library that extends [00:18:50] sentence transformer to late interaction model. Essentially late interaction model are dense model, but without the pooling operation and the addition of the [00:19:00] maxim operator. So you can actually leverage a very large part of a sentence transformer.

And so by doing so by building upon some transformer we [00:19:10] actually get a lot of perks including the support of all the base model as the backbone model. you can use any encoder, any architecture. You can even use large [00:19:20] language model if this is what you want to do. And we can also benefit from the efficient training.

With the support of multi GPU and MULTINA [00:19:30] training FP and BF 16 precision, et cetera, et cetera. And all of this training is directly monitorable from Weight and Biases which is very helpful to monitor the [00:19:40] training, but also to share with your team when you're training the models. Also sometimes transformer is very well integrated into the a phase [00:19:50] ecosystem.

And so the pilot models are easily shareable on the hub alongside model curves that are automatically created and that include information [00:20:00] about the model, the training data, the training dynamics, and ever the evaluation results. If you run those during the training. And finally, and maybe [00:20:10] most importantly the code is actually very similar to the one of sentence transformer, but you are already familiar with and which in my opinion is very [00:20:20] streamlined.

And so you can almost swap in pilots into existing boiler plates. So you can already leverage the same dataset you are already using. So [00:20:30] really you can just reuse your existing setup. For example here I put a small boiler plates where you can see that we first define a few parameter for the training.

[00:20:40] The base model name the batch size, the number of och. Then you load the data. So you can see that we actually load directly the sensor transformer data. You define the [00:20:50] laws, maybe the in-training evaluator. If you want to run that. Then you create the ance transformer training arguments based on these arguments.

You create a trainer [00:21:00] and you just launch a training and voila. although this boiler plate is actually very simple, it is actually very similar to the [00:21:10] one I used to train the two state of the art model that I presented earlier in this presentation. 

Those two training took like one to three hour on eight H 100. [00:21:20] So really like the training of state of the art model is accessible and you can really just trade out on your data to see how it goes. So training state of [00:21:30] their model is easy. But if you are here, what you really care about are evaluation, right?

And in pilot we also got you covered because Pilot has a [00:21:40] builtin efficient index based on the blood index. Where you can actually store your document alongside their document IDs, and [00:21:50] then you can simply query this index to get the closest document in your database. We also provide helper function to get evaluation [00:22:00] metrics using the output of this index.

And this is using the popular Rex library so you can get various metric easily and quite quickly. [00:22:10] And all of this process is using the standard queries document QLS format. So essentially you can use all the M tab, air dataset, b [00:22:20] dataset, and you can even use your own data because this is the most standard format or at the various, it's easy to get into this format.

We also have a [00:22:30] in-training evaluator, as I mentioned earlier. So if you want to keep track of the evaluation during the training to monitor the training. And pilot is also integrated in Fresh [00:22:40] Stack that Nandan presented. And it'll soon be merged in the mt a library so you can directly run your evaluation in this library.

And so I [00:22:50] just want to finish this talk by presenting the promising research avenue that should make the multi-vector models even more Awesome. So first, as I said a [00:23:00] big subject is the index footprint for multi-vector model. And the recently introduced methods allow to compress the representation easily without losing too [00:23:10] many performance, if any, And we actually are starting to get a reasonable index size. But can actually match the size of a usual dance model if [00:23:20] you consider like reasonably sized document.

And so there is more and more research on this subject in order to try to find like the optimal trade off between [00:23:30] keeping all the tokens and only keeping one. Another promising avenue is applying late interaction to other modalities than just [00:23:40] text. You might have heard of the call poly approach which essentially enabled OER free rack pipeline by applying colvert to text emerge [00:23:50] retrieval.

And so g Gina recently released, and train and released a model trained to do image text matching with both a single vector mode [00:24:00] and a multi vector mode. And the multi vector mode consistently outperformed the single one while sharing the same perks as with the text. Out of domain, [00:24:10] generalization and long context handling.

And so why stop there? Because some work already applied late interaction to even more modality [00:24:20] including our job. And so finally the max operator is pretty naive. And although it has some nice property, but I highlighted earlier there [00:24:30] is definitely room for improvements and notably for the scope of land scoring function.

So this is the end of this talk, which might have [00:24:40] been a bit dense, pun intended. But to summarize the key points really quickly late interaction model allows to overcome the interesting limitation [00:24:50] of single vector search. And this makes them very well suited for your modern, real world use cases and probably a lot of other benefits to be [00:25:00] discovered.

They start to get well supported in the ecosystem. And in order to empower everyone with that model we build pilots which allow you to train [00:25:10] experiment and evaluate late interaction model while being as close as possible to the sentence transformer experience you are already familiar with.

And so [00:25:20] really, you do not have any excuse anymore to not at least try the existing model on your data. And Evan, take it a step further by training your own model on [00:25:30] your existing setup. So thank you for your attention and again if you are interested in this topic, I try to share as much information as I can on the Twitter [00:25:40] and I also try to maintain a good information over sheet post ratio.

So if you're interested, please do not hesitate to come talk with me. I always like to [00:25:50] discuss this subject. Thank you.

[00:25:52] Hamel Husain: Do you mind if I ask you some questions?

[00:25:54] Antoine Chaffin: No, go ahead.

[00:25:55] Hamel Husain: Okay, great. Totally get like why the, single [00:26:00] dense vector approach. Compressing, it doesn't work. And I agree with you that. vast majority of people seems like 99% of people are still [00:26:10] using dense vector approach, like the one vector. And you mentioned some trade offs like the tools may not be as well known, although you did introduce [00:26:20] pilot as a way to, help you train your own models. it seems like you do have pate now. If you do wanna fine tune your own, it seems like there is [00:26:30] to store it without blowing up like crazy. You even talked about the newer research where, you compressed things a bit more on top of, these adapted [00:26:40] indexes and footprint reduction, all that stuff. So like, why do you think it's not. Despite this one additional thing beyond this, what I was [00:26:50] wondering is is there any latency hit you get from using interaction when you're, actually retrieving documents? What is the kind of [00:27:00] effect

[00:27:01] Antoine Chaffin: Yes. So to answer to the first part of the question I think it is still early. So like we build pilot, [00:27:10] but it's still quite new and we are still enhancing it. And most of the. Vector DB provider, like very support called birth model, but for like [00:27:20] maybe a few months at top.

And so like just changing an embedding model in production might be hard for some team sometime. So trying to. [00:27:30] Jump into something this new. Especially if your Vector DB provider did not support it back then. it might be a bit complex to push into production. I think [00:27:40] that right now it is doable to use core modern production especially like.

Most of the actual user do not scale their databases to [00:27:50] like really million and linear of documents. Because again, like with the PLA index, we can scale to Ms. Marco size, which is like 8.8 million documents. I think it's fine. 

[00:28:00] We need people to realize the benefits from them. For example, the reasoning intensive and the long context. It was not that known until a few weeks.

[00:28:10] So it's really just new and like in any technology, it takes time to adapt. And I just think it, it will become mainstream because people will start to [00:28:20] use the model that has been released and the more and more model we have to choose from. Because, for example, all the modern I'm training right now are in English.

Like this is a big [00:28:30] slowdown to push into production because English is not the only language in the world, right? So almost like different people start to train different model of different sites, [00:28:40] different languages, and like with specific domain because it's good out of domain, but it's still better to like, have something closer to your domain.

So I think once we start to [00:28:50] have that there will be more and more, people that push that to production and so it'll grow eventually. And about the difference in speed. Again, it really depends what you're [00:29:00] using because like even an index, it doesn't mean are you using PQVF?

Are you using ya code navigator tool world? What are you using and stuff. I think there is still a bit But I think we [00:29:10] are getting there. It'll still be a bit slower. But I think as lighted yesterday, I don't think in today's standard this matter much.

Like it'll be a few [00:29:20] milliseconds different. But I think for deep research stuff and like the a b, D rag thing. It doesn't matter that much if you really have like [00:29:30] insane performance on the other end. I don't want it to be like very slow. 

And like for example, you can see Rafael, my always some commenter is releasing a lot of a lot of thing in [00:29:40] Rust that are actually like really going even faster than what we had before. And honestly, it's really fast. Like even to search in Ms. Marco, it's really fast. So I think it's [00:29:50] acceptable.

Maybe not for,

[00:29:51] Hamel Husain: sense. You're basically arguing the UX. People's expectations usually aren't to where, you [00:30:00] need it super fast. You have reasoning models and things like that, and if you have a thoughtful ux hey, like we're thinking whatever, and you communicate that and it's, you're saying [00:30:10] even it's like maybe a few milliseconds.

[00:30:11] Antoine Chaffin: Like within the full tip research ecosystem, it won't be the bottleneck at all. Obviously, people right now are very fine with using [00:30:20] eight B model for embeddings and this is way slower than a 115 million parameter model, even if you're using late interaction, 

the inference cost is [00:30:30] so much bigger I still want it to be fast, as fast as possible. But I don't think we are in the same regime than what we were back then, where really, a few milliseconds was [00:30:40] really important. right now, we are used to sending the request and waiting a few I don't know, seconds or I did make the computation, but we are fine [00:30:50] with waiting a bit and I don't really think it's such of a slowdown.

I think it'll get better as we move forward. 

[00:30:57] Hamel Husain: Okay, another question I have [00:31:00] is, comparing to dense vector stuff, if a lot of people are fine tuning their dense vector stuff [00:31:10] also on their data. Do you know anything about, okay, if you fine tune your dense vector stuff and you fine tune, a late interaction model does the gap [00:31:20] still hold, like the late interaction model is still continues to be better? Similarly as it is you showed a bunch of benchmarks and it was pretty [00:31:30] convincing. That hey, like you get this BM 25. Like behavior, which is really cool, which doesn't seem like it can happen at all with a [00:31:40] single, so I get that.

[00:31:41] Antoine Chaffin: there, there isn't a lot of like direct comparison of a single vector model and a multi-vector model trained on the same data. But this [00:31:50] one, for example is a direct comparison of the two. And they are both in domain, right? They are both trained on the same data, which is in domain for the evaluation.

And you can [00:32:00] see a gap. So this is a very specific use case. This is reasoning intensive virtual. So you might argue that it won't all for everything, but again, [00:32:10] Gina, they trained like this is the same model. This is the, this is not like two separate model. This is the same model. It is trained to either use the single dense vector [00:32:20] and the multi vector search.

And you can see that like on image to text retrieval, which is like way closer to. Something way more generic. You can see that there is a huge [00:32:30] difference. So I cannot guarantee for now because there, there isn't enough. Studies that correctly compare both setups in a fair manner. But I will argue that [00:32:40] yes I think in domain it should be better.

And I, I even think that fine tuning a late interaction model put aside the cost, which isn't great, but put [00:32:50] aside the cost, I think it is easier to fine tune the late interaction model. Because you have way less risk to collapse to your new training distribution, right? So [00:33:00] you can still keep the original capabilities way easier than if you were collapsing with your single dense representation.

So I think it's working nicely out of [00:33:10] domain. But I also think that in domain it is strongerthan the tense model, especially on some specific use cases, but also in the general case.

[00:33:19] Hamel Husain: How [00:33:20] easy is it to fine tune with pilot? Is it easy to like, get it going? Any tips, anything to watch out for?

[00:33:27] Antoine Chaffin: Honestly, we have some boiler plates. So [00:33:30] the boiler plates I mentioned to train the two state of the art model, they are openly available alongside the data on the repo. I don't have like really strange lost curve [00:33:40] with late interaction model.

What I would advise is maybe to run like in training evaluation if you can, because this is a very nice signal and if you are trying to [00:33:50] sweep it is really cool to know where it's going instead of waiting the end to evaluate. But honestly, no. All the boiler plates are exactly the same. You [00:34:00] just need to decide the training if you want to do contrastive or distillation and, just use one of the existing boiler plates. We have a nice documentation as well. And again, like if you have any [00:34:10] issue, do not hesitate to open an issue or to reach out to us on Twitter. We try to be as active as possible and we really love to, to help people out. 

[00:34:19] Hamel Husain: Okay,[00:34:20] 

[00:34:20] Antoine Chaffin: work.

[00:34:20] Hamel Husain: a different angle. Let's say is there any kind of common mistakes that you see people make or common points of confusion of people coming from? I. [00:34:30] vector to this regime that they like, misunderstand or get wrong, and anything that comes to your mind, it doesn't have to be, just wanted to ask.

[00:34:39] Antoine Chaffin: No [00:34:40] I don't think so. I didn't have much comment on this aspect. People really managed to get it working. Essentially if you are able to train single vector [00:34:50] model you shouldn't be lost. Within pilot. There isn't like anything different. It, it converges faster and it's a bit more stable in my experience.

But over, but.

[00:34:59] Hamel Husain: [00:35:00] it's easier. If

[00:35:01] Antoine Chaffin: To, to I, yes, to, to me it is easier. Maybe I am a bit biased, but essentially this is all the advice that you can get for [00:35:10] like single vector training. For example, tune the temperature. If you're using contrastive loss, try to get as big batch size as you can. But I detail everything in [00:35:20] the documentation again.

So like the most common pitfall should be okay. Again, like if you have any question do not hesitate to reach out, [00:35:30] and I will update the documentation with this issue to, to help out the others.

[00:35:35] Hamel Husain: Okay. Sounds good. Do you mind going to the last page again, just so if we can end it so [00:35:40] people know how to reach you on all of these different links. But yeah, thank you for coming

[00:35:45] Antoine Chaffin: thank you very much for the invitation. 

[00:35:47] Hamel Husain: Thank you. 

