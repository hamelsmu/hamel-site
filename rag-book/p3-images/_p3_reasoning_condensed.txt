<!-- ---
title: "P3: Instruction Following and Reasoning in Information Retrieval"
description: "Orion Weller from Johns Hopkins University on making retrieval systems understand complex instructions and reason about documents, featuring Promptriever and Rank1."
image: p3-images/slide_64.png
toc-depth: 2
date: 2025-07-09
--- -->

As part of our [LLM Evals course](https://bit.ly/evals-ai){target="_blank"}, I hosted [Orion Weller](https://orionweller.github.io/){target="_blank"} from Johns Hopkins University for our 5-part mini-series on evaluating and optimizing RAG. Orion's research focuses on embedding the instruction-following and reasoning capabilities of modern Large Language Models (LLMs) directly into the retrieval process.

In his talk, Orion argues that while LLMs have improved RAG, the core retrieval step has remained static. He introduces an approach where instruction-following and reasoning are baked directly into retrieval models, a shift from using LLMs for query rewriting or as generic rerankers.

His approach is showcased with two models:

*   **Promptriever (bi-encoder):** Creates "instruction-aware" embeddings by training on a novel dataset containing **instruction negatives**. These are examples where a document is relevant to a query but *not* to its specific instruction (e.g., "find a document using a metaphor"). This forces the model to encode abstract instructions directly into the query embedding, allowing it to surface documents from a massive corpus that a standard retriever would miss.

*   **Rank1 (reranker):** A smaller model fine-tuned by distilling the reasoning traces of a larger model. Orion shows that this approach allows it to uncover novel relevant documents invisible to previous systems.

Below is an annotated version of his presentation with timestamped links.

<div class="cta" style="text-align: center;"> 
<strong>ðŸ‘‰ <em>We are teaching our last and final cohort of our [AI Evals course](https://bit.ly/evals-ai) next month</strong> (we have to get back to building). Here is a [35% discount code](https://bit.ly/evals-ai) for readers of this post.</em> ðŸ‘ˆ
</div>
<hr>

## Annotated Presentation

### The Premise: Search Hasn't Caught Up to LLMs

![](p3-images/slide_2.png)

*([Timestamp: 00:00:07](https://youtu.be/YB3b-wPbSH8?t=7s){target="_blank"})*

Modern LLMs like ChatGPT are powerful because of two key capabilities: **instruction following** (executing complex commands) and **reasoning** (generating a step-by-step "chain of thought"). While now taken for granted, these were recent breakthroughs. The central question is how to bring these features from text generation into Information Retrieval (IR).

![](p3-images/slide_5.png)

*([Timestamp: 00:01:41](https://youtu.be/YB3b-wPbSH8?t=101s){target="_blank"})*

While user interfaces have improved, the core retrieval process has remained surprisingly static.

![](p3-images/slide_8.png)

*([Timestamp: 00:01:58](https://youtu.be/YB3b-wPbSH8?t=118s){target="_blank"})*

Despite decades of development, the fundamental user interaction for search hasn't changed much: you type keywords and get a list of links. Even in advanced "SearchGPT" style systems, the LLM often acts as a sophisticated wrapper. These systems typically send the user's query to a traditional search engine, retrieve a list of documents, and then use the LLM to generate a summary. The retrieval step itself hasn't gained the instruction-following or reasoning capabilities of the LLM.

![](p3-images/slide_11.png)

*([Timestamp: 00:02:46](https://youtu.be/YB3b-wPbSH8?t=166s){target="_blank"})*

### The Path Forward: From Keywords to Instructions

The limitations of current systems are evident in the evolution of search.

![](p3-images/slide_10.png)

*([Timestamp: 00:03:19](https://youtu.be/YB3b-wPbSH8?t=199s){target="_blank"})*

**Keyword Search**, the dominant paradigm for years, relies on exact lexical matching and is brittle. A search for "data privacy" would miss a relevant document titled "Digital Protection" because it can't connect the synonyms.

![](p3-images/slide_12.png)

*([Timestamp: 00:04:11](https://youtu.be/YB3b-wPbSH8?t=251s){target="_blank"})*

**Semantic Search** improves on this by matching based on meaning, correctly retrieving all topically relevant documents. While this is the state-of-the-art for most systems, it still falls short of true instruction following.

![](p3-images/slide_13.png)

*([Timestamp: 00:04:38](https://youtu.be/YB3b-wPbSH8?t=278s){target="_blank"})*

The next frontier is **Instruction-based Search**. Here, the query is a nuanced command. For example, a user wants documents about data privacy that also "use extended metaphors." This requires reasoning about the *style* of a document, not just its topic.

This problem cannot be solved by reranking top results from a standard semantic search. The metaphorical document might have a low semantic score (because its language is abstract) and therefore might not even appear in the initial set of documents to be reranked. The instruction must influence the **initial retrieval** to change which documents are considered relevant in the first place.

![](p3-images/slide_15.png)

*([Timestamp: 00:06:02](https://youtu.be/YB3b-wPbSH8?t=362s){target="_blank"})*

Taking the idea further, **Prompt and Reasoning-based Search** allows for instructions about the retriever's *behavior*. A query like "Have really high recall" should cause the model to adjust its retrieval strategy (e.g., lower its relevance threshold), not search for the word "recall." The core idea is simple: since our retrievers are built on LLMs, we should be able to prompt them just as we prompt language models.

### The Solution: Promptriever and Rank1

![](p3-images/slide_16.png)

*([Timestamp: 00:06:42](https://youtu.be/YB3b-wPbSH8?t=402s){target="_blank"})*

An "instruction" in IR can include document attributes (date, length), NLU aspects (sentiment, style), or logical conditions.

### The Core Goal: Use Retrievers Like LLMs

![](p3-images/slide_17.png)

*([Timestamp: 00:07:31](https://youtu.be/YB3b-wPbSH8?t=451s){target="_blank"})*

The core idea is to treat retrievers like LLMs. Since they are built on the same underlying technology, we should be able to prompt them to unlock their full instruction-following and reasoning capabilities.

### Introducing Promptriever and Rank1

![](p3-images/slide_18.png)

*([Timestamp: 00:07:45](https://youtu.be/YB3b-wPbSH8?t=465s){target="_blank"})*

To explore this, Orion introduces two models:

1.  **Promptriever:** A fast embedder for quick, instruction-aware retrieval.
2.  **Rank1:** A slower but more powerful reranker that uses reasoning.

### Promptriever: Instruction-Trained Retrievers

![](p3-images/slide_20.png)

*([Timestamp: 00:08:17](https://youtu.be/YB3b-wPbSH8?t=497s){target="_blank"})*

Promptriever is a **bi-encoder**, which is fast and scalable because it processes the query and documents separately to create embeddings for comparison. The key challenge was teaching this efficient architecture to follow complex instructions.

![](p3-images/slide_22.png)

*([Timestamp: 00:09:10](https://youtu.be/YB3b-wPbSH8?t=550s){target="_blank"})*

The solution was **training data**. Existing retrieval datasets lack instructions because users don't type commands into search engines. To solve this, a new dataset was synthetically generated.

![](p3-images/slide_25.png)

*([Timestamp: 00:10:07](https://youtu.be/YB3b-wPbSH8?t=607s){target="_blank"})*

The team used a powerful LLM to **synthetically generate an instruction** that logically connects an existing query and a relevant document. This process creates the necessary data to teach a retriever to follow instructions.

### Promptriever: Experimental Settings

![](p3-images/slide_26.png)

*([Timestamp: 00:10:34](https://youtu.be/YB3b-wPbSH8?t=634s){target="_blank"})*

To ensure a fair comparison, experiments started with the **RepLLaMA** training recipe, evaluating on in-domain data, out-of-domain data, and new instruction-based datasets.

### Evaluation Data

![](p3-images/slide_28.png)

*([Timestamp: 00:11:22](https://youtu.be/YB3b-wPbSH8?t=682s){target="_blank"})*

The **FollowIR** dataset tests if a model can update its search based on a revised instruction. Before this work, no embedding model had scored above zero on this benchmark.

![](p3-images/slide_29.png)

*([Timestamp: 00:12:13](https://youtu.be/YB3b-wPbSH8?t=733s){target="_blank"})*

The **InstructIR** dataset simulates varied user needs by applying different "personas" (e.g., a professional vs. a student) to each query.

### Instruction Following Results

![](p3-images/slide_32.png)

*([Timestamp: 00:12:35](https://youtu.be/YB3b-wPbSH8?t=755s){target="_blank"})*

On the FollowIR benchmark, the baseline RepLLaMA scores negatively. In contrast, **Promptriever achieves a positive score**, demonstrating for the first time that an embedding model can follow instructions.

![](p3-images/slide_33.png)

*([Timestamp: 00:12:51](https://youtu.be/YB3b-wPbSH8?t=771s){target="_blank"})*

The results on InstructIR are even more dramatic, with Promptriever significantly outperforming the baseline.

### Results on Data without Instructions

![](p3-images/slide_35.png)

*([Timestamp: 00:13:02](https://youtu.be/YB3b-wPbSH8?t=782s){target="_blank"})*

What happens when you evaluate on standard data with no set instruction? You can either use no prompt (the standard way) or try to use a generic, "one-size-fits-all" prompt. They came up with 10 generic prompts (e.g., "Be careful when assigning relevance...") and used the best-performing one from the development set.

![](p3-images/slide_36.png)

*([Timestamp: 00:13:58](https://youtu.be/YB3b-wPbSH8?t=838s){target="_blank"})*

On the BEIR out-of-domain benchmark with **no prompt**, Promptriever and RepLLaMA perform about the same, showing that instruction-following capabilities don't hurt standard performance.

![](p3-images/slide_37.png)

*([Timestamp: 00:14:11](https://youtu.be/YB3b-wPbSH8?t=851s){target="_blank"})*

Furthermore, when a generic instruction (e.g., "Find the most relevant document") is added to a query on a standard benchmark, Promptriever's performance significantly improves, while the baseline model's performance degrades. As the paper explains, this enables a form of **zero-shot hyperparameter optimization via prompting**, where you can tune the retriever's behavior with natural language instead of numerical settings.

### Prompt Robustness and Understanding Semantic Meaning

![](p3-images/slide_39.png)

*([Timestamp: 00:14:52](https://youtu.be/YB3b-wPbSH8?t=892s){target="_blank"})*

Promptriever shows much smaller variance in performance across paraphrased prompts than keyword-based models. This indicates it understands the core semantic meaning of an instruction, not just the keywords.

### Promptriever Summary

![](p3-images/slide_40.png)

*([Timestamp: 00:15:16](https://youtu.be/YB3b-wPbSH8?t=916s){target="_blank"})*

In summary, Promptriever shows that with the right training data, retrievers can be made promptable like LLMs, unlocking new types of queries and allowing users to state what they want in natural language.

### Introducing Rank1: Test-Time Compute for IR

![](p3-images/slide_42.png)

*([Timestamp: 00:16:04](https://youtu.be/YB3b-wPbSH8?t=964s){target="_blank"})*

The talk now shifts to the second model, **Rank1**, a strong but slower reranker that uses reasoning.

![](p3-images/slide_43.png)

The title slide for the Rank1 paper, highlighting its focus on Test-Time Compute for Information Retrieval.

![](p3-images/slide_44.png)

This slide frames Rank1 as a Cross-Encoder, which processes the query and document jointly.

### Test-Time Compute Explained

![](p3-images/slide_45.png)

*([Timestamp: 00:16:21](https://youtu.be/YB3b-wPbSH8?t=981s){target="_blank"})*

**Test-time compute** is the idea that spending more computational resources at inference time can improve performance. The chart on the right shows that as the amount of "thinking" increases, model accuracy on a difficult math benchmark rises dramatically.

### What Test-Time Compute Looks Like in IR

![](p3-images/slide_46.png)

This slide poses the question of how this "thinking" process applies to IR.

![](p3-images/slide_47.png)

Here is a basic IR task: a query and a document.

![](p3-images/slide_48.png)

*([Timestamp: 00:17:17](https://youtu.be/YB3b-wPbSH8?t=1037s){target="_blank"})*

This slide shows Rank1's "thinking" process. Instead of just outputting "true" or "false," the model generates a detailed reasoning chain, analyzing the query, the passage, and even questioning its own interpretations to arrive at a more nuanced judgment.

### Evaluation Data for Rank1

![](p3-images/slide_49.png)

This slide transitions to the evaluation data for Rank1.

![](p3-images/slide_50.png)

*([Timestamp: 00:18:06](https://youtu.be/YB3b-wPbSH8?t=1086s){target="_blank"})*

The **BRIGHT dataset** evaluates complex reasoning, with unique relevance definitions that go beyond topic matching (e.g., finding a math problem that **uses the same theorem**).

![](p3-images/slide_51.png)

*([Timestamp: 00:18:52](https://youtu.be/YB3b-wPbSH8?t=1132s){target="_blank"})*

This example shows the model's reasoning on a LeetCode problem. It correctly identifies that two different problems use the same underlying **two-pointer approach**, demonstrating a deep, algorithmic understanding.

### Rank1 Results

![](p3-images/slide_52.png)

This slide marks the beginning of the Rank1 results.

![](p3-images/slide_53.png)

The experiments evaluate Rank1 against a strong baseline (RankLLaMA) on a broad range of tasks: reasoning (BRIGHT), negation understanding (NevIR), and instruction following (mFollowIR). Orion notes that the baseline model was trained on **10x more data**.

![](p3-images/slide_54.png)

*([Timestamp: 00:19:55](https://youtu.be/YB3b-wPbSH8?t=1195s){target="_blank"})*

On the BRIGHT reasoning task, Rank1 nearly doubles the performance of the baseline, achieving a score of 27.5 compared to 13.9.

![](p3-images/slide_55.png)

The trend continues. On the NevIR negation benchmark, Rank1 more than doubles the baseline's accuracy.

![](p3-images/slide_56.png)

*([Timestamp: 00:20:05](https://youtu.be/YB3b-wPbSH8?t=1205s){target="_blank"})*

Across all three difficult tasksâ€”reasoning, negation, and instruction followingâ€”Rank1 consistently and dramatically outperforms the baseline, even with significantly less training data.

### The Impact of the Reasoning Chain

![](p3-images/slide_58.png)

*([Timestamp: 00:20:24](https://youtu.be/YB3b-wPbSH8?t=1224s){target="_blank"})*

A key takeaway: an ablation study shows that training the model to generate the explicit reasoning chain achieves a **10-point gain** in performance. The act of generating the reasoning chain is itself a powerful mechanism for improving the model's capabilities.

### The Impact on Old Evaluation Data

![](p3-images/slide_59.png)

This slide introduces a story about evaluating on older, established benchmarks.

![](p3-images/slide_60.png)

*([Timestamp: 00:20:44](https://youtu.be/YB3b-wPbSH8?t=1244s){target="_blank"})*

When initially evaluated on the DL19 dataset (created in 2019, before BERT), Rank1 scored surprisingly low compared to other models. The team discovered this was because the metric only considered documents that had been previously judged by humans.

![](p3-images/slide_61.png)

*([Timestamp: 00:21:38](https://youtu.be/YB3b-wPbSH8?t=1298s){target="_blank"})*

When evaluated on the old DL19 dataset, Rank1 initially scored low because the metric only considered previously judged documents. After re-judging the new documents retrieved by Rank1, its score surpassed other models. **Rank1 was finding new, relevant documents that older systems had completely missed,** offering a "fresh perspective."

### Rank1 Summary

![](p3-images/slide_62.png)

*([Timestamp: 00:22:06](https://youtu.be/YB3b-wPbSH8?t=1326s){target="_blank"})*

In summary, Rank1 demonstrates that test-time compute (reasoning chains) creates powerful rerankers without needing complex Reinforcement Learning. While slower, these models are much more powerful, and fine-tuning on in-domain data could lead to huge gains.

### Conclusion: The Future of Retrieval Models

![](p3-images/slide_64.png)

*([Timestamp: 00:22:37](https://youtu.be/YB3b-wPbSH8?t=1357s){target="_blank"})*

The ultimate goal is to create Information Retrieval systems that work just like LLMs, understanding complex queries that include instructions about style, constraints, and user intent.

### What This Means for Developers

![](p3-images/slide_65.png)

*([Timestamp: 00:22:56](https://youtu.be/YB3b-wPbSH8?t=1376s){target="_blank"})*

For practitioners, this means new retrievers can directly benefit from LLM advances, and instruction-based search unlocks new, more powerful query types. All models discussed are open-data and open-source.

<hr>

## Q&A Session

*   **How is Promptriever operationalized for queries vs. documents?**
    *   *([Timestamp: 23:45](https://youtu.be/YB3b-wPbSH8?t=1425s))* The instruction is only applied to the query at inference time. The documents are pre-processed into embeddings without any instruction. This way, you can batch-process your entire corpus once, and then at query time, you append the user's instruction to their query to generate a single, instruction-aware query embedding for the search.

*   **Can this instruction-based approach be used for cross-encoders (rerankers) too?**
    *   *([Timestamp: 26:04](https://youtu.be/YB3b-wPbSH8?t=1564s))* Yes, absolutely. Orion mentions they have other work that explores this, and the concepts are applicable to rerankers as well. The [paper for the FollowIR benchmark](https://arxiv.org/abs/2403.15246), for example, includes work on instruction-based rerankers.

*   **Who provides the meta-instructions for search? Humans or LLMs?**
    *   *([Timestamp: 26:32](https://youtu.be/YB3b-wPbSH8?t=1592s))* Both are possible and interesting. For a "deep research" system, an LLM agent could generate precise, detailed instructions to guide the retrieval process. For end-user applications, a "power user" could type in these complex instructions directly to get more fine-grained control over their search results.

*   **How does Rank1 compare to frontier reasoning models like OpenAI's?**
    *   *([Timestamp: 28:04](https://youtu.be/YB3b-wPbSH8?t=1684s))* There is still a performance gap. On some benchmarks, a model like OpenAI's `o3` might score around 75, while the 7B parameter Rank1 model scores around 69. However, Rank1 is significantly smaller (7B vs. a much larger frontier model), faster, and fully open-source, making it ideal for applications with private data or where cost and latency are concerns.

*   **How easy is it to train Rank1 on a custom dataset?**
    *   *([Timestamp: 30:30](https://youtu.be/YB3b-wPbSH8?t=1830s))* It's surprisingly easy. The training process uses a standard supervised fine-tuning approach (predict-the-next-token loss) on reasoning traces. The [Rank1 paper](https://arxiv.org/abs/2502.18418) notes that the model generalizes remarkably well even without in-domain training, but fine-tuning on a specific dataset is straightforward and would likely lead to large performance gains.

*   **Why does supervised fine-tuning (SFT) work for a reasoning model instead of reinforcement learning (RL)?**
    *   *([Timestamp: 31:32](https://youtu.be/YB3b-wPbSH8?t=1892s))* The model learns to reason effectively through **distillation**, a process where it is trained on the reasoning chains generated by a more powerful model (in this case, Deepseek's R1). By learning to mimic the step-by-step "thought process" of the stronger model, it acquires reasoning abilities using a simple and stable supervised fine-tuning objective. This is so effective that it removes the need for more complex RL techniques. Orion speculates this is why major companies have stopped exposing the full reasoning chains of their models, since they are incredibly valuable as training data.

<hr>
<div class="cta" style="text-align: center;"> 
<strong>ðŸ‘‰ <em>We are teaching our last and final cohort of our [AI Evals course](https://bit.ly/evals-ai) next month</strong> (we have to get back to building). Here is a [35% discount code](https://bit.ly/evals-ai) for readers of this post.</em> ðŸ‘ˆ
</div>
<hr>

## Video

Here is the full video:

{{< video https://youtu.be/YB3b-wPbSH8 >}}