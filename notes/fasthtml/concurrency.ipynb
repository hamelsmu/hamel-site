{
 "cells": [
  {
   "cell_type": "raw",
   "id": "159e9fe6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Concurrency Foundations For FastHTML\n",
    "description: Concurreny fundamentals for FastHTML\n",
    "date: last-modified\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759004d",
   "metadata": {},
   "source": [
    "**Motivation**: we often want to call LLMs in [fastHTML](https://fastht.ml/) apps, and we don't want to block the server on network calls to APIs.\n",
    "\n",
    "This notebook documents of explorations I went on to better understand various types of concurrency that could be useful with FastHTML.  Specifically, I wanted to investigate how to run certain kinds of tasks in the background without blocking the main process.\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "You can [see this notebook here](https://github.com/hamelsmu/hamel-site/blob/master/notes/fasthtml/01_concurrency_fasthtml.ipynb). \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a628e18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d447115",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b62c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: false\n",
    "import ContextKit as ck\n",
    "fc_docs = ck.ctx_fastcore()['fc_llms_ctx']\n",
    "fl_docs = ck.ctx_fastlite_sqlutils()['fastlite_core']\n",
    "p_docs = ck.read_url('https://raw.githubusercontent.com/fastai/fastcore/refs/heads/master/fastcore/parallel.py')\n",
    "md_docs = ck.read_url('https://docs.fastht.ml/explains/minidataapi.html.md')\n",
    "oai_docs = ck.read_url('https://raw.githubusercontent.com/openai/openai-python/af8e0ad8b7988e72e1d02478022d80eff4f55b91/README.md')\n",
    "\n",
    "# %%ai 0 -c\n",
    "\n",
    "# Look atLook at this source code $`p_docs`, particularly `parallel`.  Also look at $`fl_docs` and $`md_docs`.  Let me know if you understand.  Finally look at $`oai_docs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b783ab",
   "metadata": {},
   "source": [
    "## Using A SQL Database As A Queue\n",
    "\n",
    " We will use [fastlite](https://github.com/AnswerDotAI/fastlite) as the interface to our SQL database.\n",
    "\n",
    "### Why\n",
    "\n",
    "You are often already using a database for your web application, and if you need to process items in that database with some kind queue, its convenient to use the database itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11696ab8",
   "metadata": {},
   "source": [
    "![Screenshot is from this [HN Comment](https://news.ycombinator.com/item?id=27482402)](hn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed4037",
   "metadata": {},
   "source": [
    "### First, let's define our queue table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15087c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastlite import *\n",
    "Path('queue.db').delete()\n",
    "\n",
    "db = Database('queue.db')\n",
    "\n",
    "class QueueItem:\n",
    "    id: int\n",
    "    data: str\n",
    "    expire: int  # Unix timestamp\n",
    "\n",
    "queue = db.create(QueueItem, pk='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18182591",
   "metadata": {},
   "source": [
    "### Now, let's implement the enqueue operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6449b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enqueue(data): return queue.insert(data=data, expire=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95e8f6",
   "metadata": {},
   "source": [
    "### For the dequeue operation, we'll implement the logic described in the comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff185083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def dequeue():\n",
    "    available_items = queue(where=\"expire = 0\", limit=1)\n",
    "    \n",
    "    if not available_items: return None  # Queue is empty\n",
    "    \n",
    "    item = available_items[0]\n",
    "    future_time = int(time.time()) + 300  # 5 minutes from now\n",
    "    \n",
    "    # Step 2: UPDATE SET expire = future_time WHERE id = item.id AND expire = 0\n",
    "    updated_item = queue.update(id=item.id, expire=future_time)\n",
    "    \n",
    "    if updated_item.expire == future_time: return updated_item\n",
    "    else: return dequeue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b34833",
   "metadata": {},
   "source": [
    "### Let's See It In Action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c077ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Task 1\n",
      "Processing Task 2\n",
      "Processing Task 3\n",
      "Queue is empty\n"
     ]
    }
   ],
   "source": [
    "# Enqueue some items\n",
    "enqueue(\"Task 1\")\n",
    "enqueue(\"Task 2\")\n",
    "enqueue(\"Task 3\")\n",
    "\n",
    "# Dequeue and process items\n",
    "while True:\n",
    "    item = dequeue()\n",
    "    if item is None:\n",
    "        print(\"Queue is empty\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Processing {item.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c6023",
   "metadata": {},
   "source": [
    "## Using Threads To Run Tasks In Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09266d7d",
   "metadata": {},
   "source": [
    "Next, we want perform proceessing on items from the queue, but do so in the background.  We can use the `ThreadPoolExecutor` from Python's `concurrent.futures` module to process items in a thread pool without blocking the main process. Here's how we can modify our implementation to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdce3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e61d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(item):\n",
    "    # Simulate some work\n",
    "    print(f\"Processing {item.data}\")\n",
    "    time.sleep(2)  # Simulating work that takes 2 seconds\n",
    "    print(f\"Finished processing {item.data}\")\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        item = dequeue()\n",
    "        if item is None: break\n",
    "        yield item\n",
    "\n",
    "def run_queue_processor_background(max_workers=3):\n",
    "    def background_task():\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for item in worker():\n",
    "                future = executor.submit(process_item, item)\n",
    "                futures.append(future)\n",
    "            # Wait for all tasks to complete\n",
    "            for future in futures: future.result()\n",
    "        print(\"Queue processing completed\")\n",
    "\n",
    "    # Start the background thread\n",
    "    thread = threading.Thread(target=background_task)\n",
    "    thread.start()\n",
    "    \n",
    "    return thread  # Return the thread object in case we want to join it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d18aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enqueue some items\n",
    "for i in range(10): enqueue(f\"Task {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33144308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main process continues...\n",
      "Processing Task 1\n",
      "Processing Task 2\n",
      "Processing Task 3\n",
      "Finished processing Task 1\n",
      "Processing Task 4\n",
      "Finished processing Task 2\n",
      "Processing Task 5\n",
      "Finished processing Task 3\n",
      "Processing Task 6\n",
      "Finished processing Task 4\n",
      "Processing Task 7\n",
      "Finished processing Task 5\n",
      "Processing Task 8\n",
      "Finished processing Task 6\n",
      "Processing Task 9\n",
      "Finished processing Task 7\n",
      "Processing Task 10\n",
      "Finished processing Task 8\n",
      "Finished processing Task 9\n",
      "Finished processing Task 10\n",
      "Queue processing completed\n"
     ]
    }
   ],
   "source": [
    "processor_thread = run_queue_processor_background()\n",
    "\n",
    "# Main process can continue immediately\n",
    "print(\"Main process continues...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83428c",
   "metadata": {},
   "source": [
    "## Async Processing\n",
    "\n",
    "On a completely separate note, we can use async processing, which is very similar to threads.  The main benefit of async over threads is that async is easier to debug (stacktrace, breakpoints, etc).  \n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "In a jupyter notebook, you need to run the following code to enable async processing.\n",
    "\n",
    "```python\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df21e39",
   "metadata": {},
   "source": [
    "In the code below, we are calling openai library with asyncio.  You will see that async is faster than sync in this case, because the majority of the work involves waiting for the response, which is perfect for async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bf27e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync processing time: 9.67 seconds\n",
      "Async processing time: 4.14 seconds\n",
      "\n",
      "Synchronous execution time: 9.67 seconds\n",
      "Asynchronous execution time: 4.14 seconds\n",
      "Time saved with async: 5.53 seconds\n",
      "Speedup factor: 2.34x\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio in Jupyter\n",
    "nest_asyncio.apply()\n",
    "prompts = [\"Tell me a joke\", \"What's the capital of France?\", \"Explain quantum computing\", \"How many planets are in the solar system?\", \"What is the meaning of life?\", \"How many bytes are in a kilobyte?\", \"When was the first iPhone released?\", \"What is the capital of Canada?\", \"What is the capital of Australia?\", \"What is the capital of the United Kingdom?\"]\n",
    "\n",
    "async def async_process_prompt(client, prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def sync_process_prompt(client, prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def async_main():\n",
    "    client = AsyncOpenAI()\n",
    "    start_time = time.time()\n",
    "    tasks = [asyncio.create_task(async_process_prompt(client, prompt)) for prompt in prompts]\n",
    "    # you can modify this code (see below) if you wish to just run this completely in the background.\n",
    "    await asyncio.gather(*tasks) \n",
    "    end_time = time.time()\n",
    "    async_time = end_time - start_time\n",
    "    print(f\"Async processing time: {async_time:.2f} seconds\")\n",
    "    return async_time\n",
    "\n",
    "def sync_main():\n",
    "    client = OpenAI()\n",
    "    start_time = time.time()\n",
    "    results = [sync_process_prompt(client, prompt) for prompt in prompts]\n",
    "    end_time = time.time()\n",
    "    sync_time = end_time - start_time\n",
    "    print(f\"Sync processing time: {sync_time:.2f} seconds\")\n",
    "    return sync_time\n",
    "\n",
    "# Run synchronous version\n",
    "sync_time = sync_main()\n",
    "\n",
    "# Run asynchronous version\n",
    "async_time = await async_main()\n",
    "\n",
    "# Compare execution times\n",
    "print(f\"\\nSynchronous execution time: {sync_time:.2f} seconds\")\n",
    "print(f\"Asynchronous execution time: {async_time:.2f} seconds\")\n",
    "print(f\"Time saved with async: {sync_time - async_time:.2f} seconds\")\n",
    "print(f\"Speedup factor: {sync_time / async_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594ba3b",
   "metadata": {},
   "source": [
    "In the code above, async is only as slow as the slowest single task.  calling `await asyncio.gather(*tasks)` waits until all tasks are finished.  **However, if you just want to run tasks in the background, you can make the following change:**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba47e7",
   "metadata": {},
   "source": [
    "```diff\n",
    "- tasks = [async_process_prompt(client, prompt) for prompt in prompts]\n",
    "- await asyncio.gather(*tasks)\n",
    "+ tasks = [asyncio.create_task(async_process_prompt(client, prompt)) for prompt in prompts]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9731fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: false\n",
    "# fh_docs = ck.ctx_fasthtml()['fasthtml_llms_ctx']\n",
    "\n",
    "# %%ai 0\n",
    "\n",
    "# Have a look at $`fh_docs`, let me know if you have any questions\n",
    "\n",
    "# %%ai 0\n",
    "\n",
    "# How would you build a minimal fasthtml app that incorporates the async approach described earlier, specifically I want a handler that can kick off a long running task in the background.  The handler should return a htmx component, but before that kick off the async task.  When the async task is done, it should print \"I'm done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcf554",
   "metadata": {},
   "source": [
    "### Limiting Async Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28efcb",
   "metadata": {},
   "source": [
    "To limit the number of tasks that can be running concurrently, we can use a `asyncio.Semaphore`. A semaphore allows us to control access to a shared resource, in this case, the number of concurrent tasks. [^1] Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c09596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task 0\n",
      "Starting task 1\n",
      "Starting task 2\n",
      "Starting task 3\n",
      "Starting task 4\n",
      "Finished task 0\n",
      "Finished task 1\n",
      "Finished task 2\n",
      "Finished task 3\n",
      "Finished task 4\n",
      "Starting task 5\n",
      "Starting task 6\n",
      "Starting task 7\n",
      "Starting task 8\n",
      "Starting task 9\n",
      "Finished task 5\n",
      "Finished task 6\n",
      "Finished task 7\n",
      "Finished task 8\n",
      "Finished task 9\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Allows you to run asyncio in Jupyter\n",
    "\n",
    "# Create a semaphore with the maximum number of concurrent tasks\n",
    "max_concurrent_tasks = 5\n",
    "semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
    "\n",
    "async def limited_task(task_id):\n",
    "    async with semaphore:\n",
    "        print(f\"Starting task {task_id}\")\n",
    "        await asyncio.sleep(2)  # Simulate some work\n",
    "        print(f\"Finished task {task_id}\")\n",
    "\n",
    "\n",
    "tasks = [limited_task(i) for i in range(10)]\n",
    "_ = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31915a8",
   "metadata": {},
   "source": [
    "[^1]: Thanks to Krisztian for the [suggestion](https://x.com/kk1694/status/1844336706058846496)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ab669",
   "metadata": {},
   "source": [
    "## FastHTML App With Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018641ec",
   "metadata": {},
   "source": [
    "Here's a minimal FastHTML app that incorporates async.  You have to run this in a notebook to try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fbd4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasthtml.common import *\n",
    "from fasthtml.jupyter import *\n",
    "from fastcore.utils import *\n",
    "import asyncio, nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d9eac",
   "metadata": {},
   "source": [
    "### Define the server\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "The \"I'm done\" messages will be printed after this cell, because all of the console output is printed where the server is defined in a Jupyter notebook.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47e2b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done\n",
      "I'm done\n"
     ]
    }
   ],
   "source": [
    "#The \"I'm done\" messages will be printed after this cell\n",
    "if IN_JUPYTER:\n",
    "    from fasthtml.jupyter import JupyUvi, jupy_app, HTMX\n",
    "    app, rt = jupy_app()\n",
    "    server = JupyUvi(app) \n",
    "    nest_asyncio.apply()\n",
    "else:\n",
    "    app,rt = fast_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32cc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def long_running_task():\n",
    "    await asyncio.sleep(5)  # Simulate a long-running task\n",
    "    print(\"I'm done\")\n",
    "\n",
    "@rt(\"/\")\n",
    "def get():\n",
    "    return P(\"Async Task Demo\",\n",
    "        Div(\n",
    "            Button(\"Start Task\", hx_post=\"/start-task\", hx_swap=\"outerHTML\"),\n",
    "            id=\"task-button\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@rt(\"/start-task\")\n",
    "async def post():\n",
    "    # These will run in the background since we aren't calling await.\n",
    "    # There are multiple tasks, and asyncio.gather is one way of kicking them off\n",
    "    asyncio.gather(long_running_task(), long_running_task())\n",
    "    return Div(\n",
    "        P(\"Task started! Check your console in 5 seconds.\"),\n",
    "        id=\"task-button\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d1fe8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output:false\n",
    "HTMX()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab8cc516",
   "metadata": {},
   "source": [
    "![](ex_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f365184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c4e82",
   "metadata": {},
   "source": [
    "## Async OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a595b",
   "metadata": {},
   "source": [
    "Let's show a more realistic example by using OpenAI instead of the sleep. We'll use the OpenAI API to generate a response, and then print it when it's done. Here's the updated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad60f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasthtml.common import *\n",
    "from fasthtml.jupyter import *\n",
    "import asyncio, nest_asyncio\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e6454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI response: Sure, here's a long joke for you:\n",
      "\n",
      "Once upon a time, in a small village nestled at the foot of a great mountain, there lived a farmer named Joe. Joe was known throughout the village for his impeccable ability to grow the juiciest, most succulent tomatoes. People would travel from far and wide just to taste Joe's legendary tomatoes. In fact, they were so famous that he entered them into the annual state fair every year, and every year, without fail, he won the blue ribbon.\n",
      "\n",
      "Now, there was one thing about Joe's tomatoes that puzzled everyone in the village: nobody really knew how he managed to grow them to be so perfect. Joe, of course, had a secret: every night, he'd go out into the fields and serenade his tomatoes with his accordion. It was a strange sight, to be sure, but Joe swore that the music helped his tomatoes grow bigger and better.\n",
      "\n",
      "One day, a traveling salesman named Bob came to the village. Bob was always looking for the next big thing to make a quick profit, and he was instantly intrigued by the legend of Joe's tomatoes. Determined to uncover Joe's secret, Bob approached him with a proposition. \"Joe,\" he said, \"how about selling me the secret to your amazing tomatoes? I'll make it worth your while!\"\n",
      "\n",
      "Joe smiled and replied, \"Well, Bob, I don't know if I can do that. You see, my tomatoes are like my children. But I'll tell you what, I'll let you in on a trade secret if you promise to never tell another soul.\"\n",
      "\n",
      "Bob eagerly agreed, and that night, under the cover of darkness, Joe took Bob out to the tomato fields. He handed Bob an accordion and instructed him to play it for the tomatoes. Bob, although skeptical, began to play. And much to his surprise, the tomatoes began to sway gently to the music. It was as if the entire field came alive with the sound of the accordion.\n",
      "\n",
      "After a few hours, Joe said, \"Well, there you have it, Bob. Now you see how I grow my tomatoes.\"\n",
      "\n",
      "Bob was ecstatic. He rushed back to his car and immediately placed an order for hundreds of accordions, planning to replicate Joe's technique to grow his own amazing tomatoes and sell them all over the state.\n",
      "\n",
      "However, as luck would have it, Bob wasn't exactly musically gifted. The next night, with fields full of accordions, he attempted to play for his own crop of tomatoes, but the cacophony was unbearable. The tomatoes, rather than growing, seemed to shrivel in horror. The neighbors complained, pets howled, and the town's birds flew away, never to return.\n",
      "\n",
      "Distraught and out of money, Bob went back to Joe the next day. \"Joe,\" he said, \"I did exactly as you showed me, but I must have done something wrong. My tomatoes are worse than ever!\"\n",
      "\n",
      "Joe chuckled heartily and put a friendly hand on Bob's shoulder. \"Well, Bob, I guess there's another secret I forgot to tell you,\" Joe said, trying to suppress his laughter. \"It's not just about playing an accordion; it's about playing it well. It's not the tomatoes that enjoy the music, but how much you love playing it.\"\n",
      "\n",
      "And from that day on, Bob decided to leave the tomato-growing business to Joe and found a perfect career selling earplugs to the village folks. Meanwhile, Joe continued to serenade his tomatoes every night, playing sweet melodies and enjoying every minute of it. And his tomatoesâ€¦? Well, they just kept on winning those blue ribbons.\n"
     ]
    }
   ],
   "source": [
    "## The console output (from the background task) will be printed after this cell\n",
    "if IN_JUPYTER:\n",
    "    from fasthtml.jupyter import JupyUvi, jupy_app, HTMX\n",
    "    app, rt = jupy_app()\n",
    "    server = JupyUvi(app) \n",
    "    nest_asyncio.apply()\n",
    "else:\n",
    "    app,rt = fast_app()\n",
    "\n",
    "# Initialize the AsyncOpenAI client\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99c1eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def openai_task():\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a long joke\"}]\n",
    "    )\n",
    "    joke = response.choices[0].message.content\n",
    "    print(f\"OpenAI response: {joke}\")\n",
    "\n",
    "@rt(\"/\")\n",
    "def get():\n",
    "    return P(\"Async OpenAI Demo\",\n",
    "        Div(\n",
    "            Button(\"Get a Joke\", hx_post=\"/get-joke\", hx_swap=\"outerHTML\"),\n",
    "            id=\"joke-button\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@rt(\"/get-joke\")\n",
    "async def post():\n",
    "    asyncio.create_task(openai_task())\n",
    "    return Div(\n",
    "        P(\"Joke request sent! Check your console in a minute.\"),\n",
    "        id=\"joke-button\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8972e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output:false\n",
    "HTMX()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c13118",
   "metadata": {},
   "source": [
    "![](ex_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f447aea",
   "metadata": {},
   "source": [
    "## Threads & Processes\n",
    "\n",
    "Note: Async tasks can be started in the background with threads or processes.  You can also spawn threads or processes from other threads or processes as well.\n",
    "\n",
    "Let's see the basic functionality of threads and processes in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996f9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fastcore.parallel import parallel\n",
    "\n",
    "def f(x): time.sleep(1); print(x)\n",
    "a = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# parallel starts a new thread when threadpool=True.\n",
    "def g(): parallel(f, a, threadpool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffb0d1",
   "metadata": {},
   "source": [
    "### Run with a process\n",
    "\n",
    "We are starting a thread inside a new process so it runs in the background. Remember, `parallel` will execute `f` in a new thread.\n",
    "\n",
    "It will print kinda wierd because of the threading and things completing at the same time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95e9c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.04 ms, sys: 6.68 ms, total: 12.7 ms\n",
      "Wall time: 14.4 ms\n",
      "48\n",
      "6\n",
      "2315\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocess import Process\n",
    "p = Process(target=g)\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941232d3",
   "metadata": {},
   "source": [
    "### Run with a thread\n",
    "\n",
    "Instaed of starting a thread in the background with a process, we can also start it with another thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdc9e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "\n",
      "4\n",
      "7\n",
      "2\n",
      "1\n",
      "6\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "t = Thread(target=g)\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984399be",
   "metadata": {},
   "source": [
    "### How to choose Threads vs. Processes\n",
    "\n",
    "See [my blog post](https://python.hamel.dev/concurrency/).\n",
    "\n",
    "If your tasks involves network calls, consider using threads.  For CPU intensive tasks, use processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d192b",
   "metadata": {},
   "source": [
    "## Fastcore\n",
    "\n",
    "Fastcore has goodies for threads and processes\n",
    "\n",
    "### `@threaded` decorator\n",
    "\n",
    "This will make functions run in the background in a new thread or process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34c071a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import threaded\n",
    "\n",
    "@threaded # casuses g to be threaded\n",
    "def g(): parallel(f, a, threadpool=True)\n",
    "    \n",
    "    \n",
    "@threaded(process=True) # casuses h to be run in a process\n",
    "def h(): parallel(f, a, threadpool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cec8579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-17 (g), started 13053145088)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "6\n",
      "7\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "g()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "865e155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Process name='Process-7' pid=65921 parent=59763 started>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284637\n",
      "\n",
      "\n",
      "5\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "h()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb235b0",
   "metadata": {},
   "source": [
    "### `startthread`\n",
    "\n",
    "We can also start a thread by calling the `startthread` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6ec36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "12\n",
      "\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "8\n",
      "6\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from fastcore.parallel import startthread\n",
    "\n",
    "def g(): parallel(f, a, threadpool=True)\n",
    "startthread(g)\n",
    "\n",
    "# this will run right away in the main process, since the other code is running in the background\n",
    "print('hello') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b8adc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/hamelsmu/92fd223178522b256776ea3d8787d549"
  },
  "gist": {
   "data": {
    "description": "dataroom/nbs/db_queue.ipynb",
    "public": true
   },
   "id": "92fd223178522b256776ea3d8787d549"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
