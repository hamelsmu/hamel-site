{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6aef57f2-0ebc-4667-a847-8f424cb5e6e3",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"GPUs & Batching\"\n",
    "description: Dynamic batching and using a GPU\n",
    "order: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf06d11-c74a-4e92-9320-dfc71c7c8f7e",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "I was not able to simulate a situation where dynamic batching is better than not batching.  Apparently it can take time and lots of experiments to get right.  Follow [this guide](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning) for more information.  This is a topic I may revisit in the future.\n",
    "\n",
    ":::\n",
    "\n",
    "According to [the docs](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration):\n",
    "\n",
    "> Model Server has the ability to batch requests in a variety of settings in order to realize better throughput. The scheduling for this batching is done globally for all models and model versions on the server to ensure the best possible utilization of the underlying resources no matter how many models or model versions are currently being served by the server.  You can enable this by using the `--enable_batching` flag and control it with the `--batching_parameters_file`.\n",
    "\n",
    "This is an example batching parameters file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "027a6526-a252-43a9-bf56-4875605ab022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch-config.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch-config.cfg\n",
    "max_batch_size { value: 1000 }\n",
    "batch_timeout_micros { value: 1000 }\n",
    "max_enqueued_batches { value: 16 }\n",
    "num_batch_threads { value: 16 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f81137-b636-42bd-b393-e0e2a503d672",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "### Guidance on batch configuration\n",
    "\n",
    "Guidance for these config files are [here](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md) there is no \"right answer\".  For GPUs, the guidance is this:\n",
    "\n",
    "GPU: One Approach\n",
    "\n",
    "If your model uses a GPU device for part or all of your its inference work, consider the following approach:\n",
    "\n",
    "1. Set `num_batch_threads` to the number of CPU cores.\n",
    "\n",
    "2. Temporarily set `batch_timeout_micros` to a really high value while you tune `max_batch_size` to achieve the desired balance between throughput and average latency. Consider values in the hundreds or thousands.\n",
    "\n",
    "3. For online serving, tune `batch_timeout_micros` to rein in tail latency. The idea is that batches normally get filled to max_batch_size, but occasionally when there is a lapse in incoming requests, to avoid introducing a latency spike it makes sense to process whatever's in the queue even if it represents an underfull batch. The best value for `batch_timeout_micros` is typically a few milliseconds, and depends on your context and goals. Zero is a value to consider; it works well for some workloads. (For bulk processing jobs, choose a large value, perhaps a few seconds, to ensure good throughput but not wait too long for the final (and likely underfull) batch.)\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd26e6-dd57-40a0-9f25-2d8b9b9266ee",
   "metadata": {},
   "source": [
    "## Test the server\n",
    "\n",
    "The model we are going to serve is generated [in this note](./tf-serving-basics.ipynb).\n",
    "\n",
    "I'm going to start two TF Serving instances, one thats regular CPU and one that does batching on GPU.  I'm running both commands from the `/home/hamel/tf-serving/` directory.\n",
    "\n",
    "\n",
    "### CPU Version\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model/,target=/models/model \\\n",
    "--net=host -t tensorflow/serving --grpc_max_threads=1000\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "`--net=host` binds all ports to the host, which is convenient for testing\n",
    "\n",
    ":::\n",
    "\n",
    "Test the CPU version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4768f89-2d94-4dc2-9a0f-765567c4728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8501/v1/models/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c953c8-f7d1-4ef8-94c5-12ce1f0a9b5f",
   "metadata": {},
   "source": [
    "### GPU Version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "119f1245-fd0f-4be3-b426-1d6dacfa5140",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "You must [install nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) first\n",
    "\n",
    "### Docker Command\n",
    "\n",
    "You can pass additional arguments like `--enable_batching` to the `docker run ...` command just like you would if you were [running tfserving locally](https://www.tensorflow.org/tfx/serving/docker#passing_additional_arguments).\n",
    "\n",
    "Note that we need the `--gpus all` flag to enable GPUs with nvidia-Docker.  Furthermore, use the `latest-gpu` tag to enable GPUs as well as the `--port` and `--rest_api_port` so that it doesn't conflict with the other tf serving instance I have running:\n",
    "\n",
    "```bash\n",
    "docker run --gpus all \\\n",
    "--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models \\\n",
    "--net=host -t tensorflow/serving:latest-gpu --enable_batching \\\n",
    "--batching_parameters_file=/models/batch-config.cfg --port=8505 \\\n",
    "--rest_api_port=8506 --grpc_max_threads=1000\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "\n",
    "#### `--grpc_max_threads` flag\n",
    "\n",
    "I found that in non-batch mode I can easily overwhelm the server with gRPC requests.  I wasn't able to overwhelm the server over REST.  Setting `--grpc_max_threads=1000` takes care of this.  \n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "### Other flags\n",
    "\n",
    "There are [lots of flags](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc#L59).  Hannes uses these additional ones, and they seem to make things a bit faster.\n",
    "\n",
    "```\n",
    "--enable_model_warmup  \\\n",
    "--tensorflow_intra_op_parallelism=4 \\\n",
    "--tensorflow_inter_op_parallelism=4\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "### Understanding the volume mount\n",
    "\n",
    "On the host, the config file is located at `/home/hamel/hamel/notes/serving/tfserving/batch-config.cfg` and the model is located at `/home/hamel/hamel/notes/serving/tfserving/model/`\n",
    "\n",
    "The [Docker file](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile.gpu) will try to import the model like this:\n",
    "\n",
    "```dockerfile\n",
    "# Set where models should be stored in the container\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}\n",
    "\n",
    "# The only required piece is the model name in order to differentiate endpoints\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "# Create a script that runs the model server so we can use environment variables\n",
    "# while also passing in arguments from the docker command line\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n",
    "&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n",
    "```\n",
    "\n",
    "By default it will try to get models from `${MODEL_BASE_PATH}/${MODEL_NAME}` which is `/models/model`. So when we mount `/home/hamel/hamel/notes/serving/tfserving` from the host to `/models` in the container.\n",
    "\n",
    "In the container:\n",
    "\n",
    "- The model files will be available at `models/model` as expected\n",
    "- The config file will be available at `models/batch-config.cfg`\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310ea94-5af8-4b46-897c-88207fbb8236",
   "metadata": {},
   "source": [
    "Test the TF-Serving GPU api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef71c40-0793-4d8f-b98f-f8e48328d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8506/v1/models/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7fb905-8d68-4c37-a54a-12511315bacd",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "\"All benchmarks are wrong, some are useful\"\n",
    "\n",
    "We are going to send 5 instances to score 10,000 times and measure the total inference time.  We will parallelize the 10,000 requests (each with 5 instances to score) with threads.  As a reminder, The model we are going to serve is generated [in this note](./tf-serving-basics.ipynb).\n",
    "\n",
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae546006-469d-420f-9eb4-57ca70974f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "_, (x_val, _) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "sample_data = x_val[:5, :]\n",
    "data = [sample_data] * 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9244-72cf-4127-a1b9-94b99b227157",
   "metadata": {},
   "source": [
    "## The prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bb6ffb-793a-44da-8951-00fed516c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import numpy as np\n",
    "\n",
    "from fastcore.parallel import parallel\n",
    "from functools import partial\n",
    "parallel_pred = partial(parallel, threadpool=True, n_workers=500)\n",
    "\n",
    "\n",
    "def predict_rest(data, port):\n",
    "    json_data = json.dumps(\n",
    "    {\"signature_name\": \"serving_default\", \"instances\": data.tolist()}\n",
    "    )\n",
    "    url = f\"http://localhost:{port}/v1/models/model:predict\"\n",
    "\n",
    "    json_response = requests.post(url, data=json_data)\n",
    "    response = json.loads(json_response.text)\n",
    "    rest_outputs = np.array(response[\"predictions\"])\n",
    "    return rest_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b53cd-3eec-44d0-82eb-99c2aa97a8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89650154, 0.10349847],\n",
       "       [0.00330466, 0.9966954 ],\n",
       "       [0.13089457, 0.8691054 ],\n",
       "       [0.49083445, 0.50916553],\n",
       "       [0.0377177 , 0.96228224]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_outputs = predict_rest(sample_data, '8501')\n",
    "rest_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df740f-df22-4754-bc02-0dafbe23c54b",
   "metadata": {},
   "source": [
    "### gRPC\n",
    "\n",
    "This is the code that will be used to make gRPC prediction requests. For more discussion about gRPC, see [this note](./tf-serving-basics.ipynb#grpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edaf2aa5-af8a-4db5-ba3e-ebd999ed57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n",
    "# Create a channel that will be connected to the gRPC port of the container\n",
    "\n",
    "\n",
    "\n",
    "def predict_grpc(data, input_name='input_1', port='8505'):\n",
    "    \n",
    "    options = [('grpc.max_receive_message_length', 100 * 1024 * 1024)]\n",
    "    channel = grpc.insecure_channel(f\"localhost:{port}\", options=options) # the gRPC port for the GPU server was set at 8505\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    # Create a gRPC request made for prediction\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    # Set the name of the model, for this use case it is \"model\"\n",
    "    request.model_spec.name = \"model\"\n",
    "\n",
    "    # Set which signature is used to format the gRPC query\n",
    "    # here the default one \"serving_default\"\n",
    "    request.model_spec.signature_name = \"serving_default\"\n",
    "\n",
    "    # Set the input as the data\n",
    "    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n",
    "    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n",
    "\n",
    "    # Send the gRPC request to the TF Server\n",
    "    result = stub.Predict(request)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab844eea-8aa3-4e57-ac4d-e353510706e2",
   "metadata": {},
   "source": [
    "## CPU Server\n",
    "\n",
    "The CPU server is running on port `8501`.\n",
    "\n",
    "### REST CPU\n",
    "\n",
    "The REST API endpoint on the CPU-bound server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94cd3923-ba0a-4345-8384-dea312a7907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_pred = partial(predict_rest, port = '8501')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1372e0a-10f9-4a6e-a6b9-4fc4e59d9a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.7 s, sys: 5.56 s, total: 33.3 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(cpu_pred, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d502f-64bc-4c70-822a-4434d34792bf",
   "metadata": {},
   "source": [
    "### grpc CPU\n",
    "\n",
    "This is using the same CPU-bound TF Serving server, but is hitting the gRPC endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc8722f-0d40-4c65-974a-406f373ee8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grpc_cpu = partial(predict_grpc, port='8500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbae9f7b-52a7-4b03-a809-1f1b1253312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.5 s, sys: 2.33 s, total: 9.84 s\n",
      "Wall time: 7.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(predict_grpc_cpu, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06a572-3a9c-40ab-8a6a-1b2a5e59b526",
   "metadata": {},
   "source": [
    "## GPU Server with batching\n",
    "\n",
    "The GPU server is running on port `8506` (we already started it above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090e152-8726-4383-a422-bb89379e3534",
   "metadata": {},
   "source": [
    "### REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e93266-d86b-4f64-ba8d-4fbe085bd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_pred = partial(predict_rest, port = '8506')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b7f35c-4918-4308-b4d8-b13fdef5dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.1 s, sys: 3.44 s, total: 30.6 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(gpu_pred, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340d743-3c6d-43ce-a212-64501e00e503",
   "metadata": {},
   "source": [
    "### gRPC with batch\n",
    "\n",
    "This is **much** faster than the REST endpoint!  This is also much faster than the CPU version on this specific example.  However, the batching part doesn't appear to be providing any speedup at all, because the non-batch gRPC version is almost the same speed (if not a little bit faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a37cd459-a9bc-42c6-a6ce-eca12f5c300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.71 s, sys: 551 ms, total: 3.26 s\n",
      "Wall time: 6.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = parallel(predict_grpc, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0cb4c-9a0f-4187-ae12-bce50f42319b",
   "metadata": {},
   "source": [
    "## GPU server without batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68742ea2-ee98-4945-8168-fb693f7441bc",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run --gpus all --mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models --net=host -t tensorflow/serving:latest-gpu --port=8507 --rest_api_port=8508\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4993ca1-786d-4fb4-b726-ed449bc2ba6a",
   "metadata": {},
   "source": [
    "### REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde84bc1-6be7-4f43-9e6b-33673a629044",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_pred_no_batch = partial(predict_rest, port = '8508')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5defa54-52e9-4e54-83b9-f42cc65378c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 s, sys: 3.61 s, total: 30.5 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(gpu_pred_no_batch, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03767d5-5678-4160-aae8-30c228784a8f",
   "metadata": {},
   "source": [
    "### gRPC without batching\n",
    "\n",
    "When I initially did this I got an error that said \"Resources Exhausted\".  I was able to solve this by increasing the threads with the flag `--grpc_max_threads=1000` when running the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74cd073a-6f84-4491-b406-38e046aec63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_grpc_no_batch = partial(predict_grpc, port='8507')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "044123bc-83fb-4aaa-b8d7-97e421f7a0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.06 s, sys: 1.42 s, total: 6.48 s\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = parallel_pred(predict_grpc_no_batch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477e5c8-4570-40f2-abe5-b640b6b9a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
